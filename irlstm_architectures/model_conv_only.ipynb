{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 10:33:19.855259: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import keras\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Flatten, BatchNormalization, TimeDistributed, Input, Add, Concatenate\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, LSTM, TimeDistributed, Reshape\n",
    "import keras.backend as K\n",
    "import keras.callbacks as callbacks\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_path = \"/Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/\"\n",
    "model_name = \"conv_only\"\n",
    "kf = KFold(n_splits = 10, shuffle =True)\n",
    "num_epochs = 10\n",
    "\n",
    "#### define functions ####\n",
    "\n",
    "def model_cycle():\n",
    "    inputs = Input(shape=(50, 4, 1))\n",
    "        \n",
    "    x = Conv2D(48, kernel_size=(3,4),\n",
    "                   activation='relu',\n",
    "                   padding='valid')(inputs)\n",
    "    x = MaxPooling2D((2,1),padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    # print(f\"After first convolutional layer: x.shape = {x.shape}\")\n",
    "\n",
    "    # x = Reshape((K.int_shape(x)[1], K.int_shape(x)[3]))(x)\n",
    "    # x = Conv1D(48, kernel_size=(11),\n",
    "    #                activation='relu',\n",
    "    #                padding='same')(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    # x = Dropout(0.2)(x)\n",
    "\n",
    "    # print(f\"After second convolutional layer: x.shape = {x.shape}\")\n",
    "\n",
    "    # x = Conv2D(48, kernel_size=(21,1),\n",
    "    #                activation='relu',\n",
    "    #                padding='same')(x)\n",
    "    # x = MaxPooling2D((24,1),padding='same')(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    # x = Dropout(0.2)(x)\n",
    "\n",
    "    # print(f\"After third convolutional layer, x.shape = {x.shape}\")\n",
    "\n",
    "    # x = Conv2D(48, kernel_size=(3,1),\n",
    "    #                activation='relu',\n",
    "    #                padding='same')(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    # x = Dropout(0.2)(x)\n",
    "    # x = Conv2D(48, kernel_size=(3,1),\n",
    "    #                activation='relu',\n",
    "    #                padding='same')(x)\n",
    "    # x = MaxPooling2D((12,1),padding='same')(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    # x = Dropout(0.2)(x)\n",
    "\n",
    "    # # parallel line 1\n",
    "    # fx1 = Conv2D(48, kernel_size=(3,1),\n",
    "    #                activation='relu',\n",
    "    #                padding='same')(x)\n",
    "    # fx1 = BatchNormalization()(fx1)\n",
    "    # fx1 = Dropout(0.2)(fx1)\n",
    "    # fx1 = Conv2D(48, kernel_size=(3,1),\n",
    "    #                activation='relu',\n",
    "    #                padding='same')(fx1)\n",
    "    # fx1 = MaxPooling2D((2,1),padding='same')(fx1)\n",
    "    # fx1 = BatchNormalization()(fx1)\n",
    "    # fx1 = Dropout(0.2)(fx1)\n",
    "    \n",
    "    # # parallel line 2\n",
    "    # fx2 = Conv2D(48, kernel_size=(11,1),\n",
    "    #                activation='relu',\n",
    "    #                padding='same')(x)\n",
    "    # fx2 = BatchNormalization()(fx2)\n",
    "    # fx2 = Dropout(0.2)(fx2)\n",
    "    # fx2 = Conv2D(48, kernel_size=(21,1),\n",
    "    #                activation='relu',\n",
    "    #                padding='same')(fx2)\n",
    "    # fx2 = MaxPooling2D((2,1),padding='same')(fx2)\n",
    "    # fx2 = BatchNormalization()(fx2)\n",
    "    # fx2 = Dropout(0.2)(fx2)\n",
    "    \n",
    "    # # # Add\n",
    "    # x1 = Concatenate(axis=-3)([fx1, fx2])\n",
    "    # x = Add()([x, x1])\n",
    "    # x = MaxPooling2D((2,1),padding='same')(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    # x = Dropout(0.2)(x)\n",
    "    \n",
    "    # x = Reshape((K.int_shape(x)[1], K.int_shape(x)[3]))(x)\n",
    "    # x = LSTM(20, return_sequences=False)(x)\n",
    "    # x = Dropout(0.2)(x)\n",
    "\n",
    "    # x = Reshape((1, 24*48))(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    outputs = Dense(1, activation='linear')(x)\n",
    "    # print(outputs.shape)\n",
    "    network = Model(inputs, outputs)\n",
    "    network.compile(optimizer='rmsprop',\n",
    "                    loss='mean_squared_error')\n",
    "    return network\n",
    "    \n",
    "def dnaOneHot(sequence):\n",
    "    seq_array = array(list(sequence))\n",
    "    code = {\"A\": [0], \"C\": [1], \"G\": [2], \"T\": [3], \"N\": [4],\n",
    "            \"a\": [0], \"c\": [1], \"g\": [2], \"t\": [3], \"n\": [4]}\n",
    "    onehot_encoded_seq = []\n",
    "    for char in seq_array:\n",
    "        onehot_encoded = np.zeros(5)\n",
    "        onehot_encoded[code[char]] = 1\n",
    "        onehot_encoded_seq.append(onehot_encoded[0:4])\n",
    "    return onehot_encoded_seq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_fits(fits):\n",
    "    print(f\"Average correlation on tiling: {np.mean([fits[0][i] for i in range(fits[0].size) if (i % 8) == 5])}\",\n",
    "          f\"\\nAverage MSE on tiling: {np.mean([fits[1][i] for i in range(fits[1].size) if (i % 8) == 5])}\",\n",
    "          f\"\\nAverage correlation on random: {np.mean([fits[0][i] for i in range(fits[0].size) if (i % 8) == 3])}\",\n",
    "          f\"\\nAverage MSE on random: {np.mean([fits[1][i] for i in range(fits[1].size) if (i % 8) == 3])}\",\n",
    "          f\"\\nAverage correlation on ChrV: {np.mean([fits[0][i] for i in range(fits[0].size) if (i % 8) == 7])}\",\n",
    "          f\"\\nAverage MSE on ChrV: {np.mean([fits[1][i] for i in range(fits[1].size) if (i % 8) == 7])}\",\n",
    "          f\"\\nAverage correlation on CN: {np.mean([fits[0][i] for i in range(fits[0].size) if (i % 8) == 0])}\",\n",
    "          f\"\\nAverage MSE on CN: {np.mean([fits[1][i] for i in range(fits[1].size) if (i % 8) == 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_c0new(dat):\n",
    "  mat = np.empty((3,3), float)\n",
    "  k = 2*np.pi/10.4\n",
    "  n = array([26, 29, 31])\n",
    "  mat[0:3,0] = 1\n",
    "  mat[0:3, 1] = np.sin(n*k)\n",
    "  mat[0:3, 2] = np.cos(n*k)\n",
    "  inv_mat = np.linalg.inv(mat)\n",
    "  c0A1A2 = array(np.matmul(dat[[\"n=26\", \"n=29\", \"n=31\"]], np.transpose(inv_mat)))\n",
    "  c0Aphi = c0A1A2\n",
    "  c0Aphi[:,0] = c0A1A2[:,0]\n",
    "  c0Aphi[:,1] = np.sqrt(c0A1A2[:,1]**2 + c0A1A2[:,2]**2)\n",
    "  c0Aphi[:,2] <- np.sign(c0A1A2[:,2]) * np.arccos(c0A1A2[:,1]/c0Aphi[:,1])\n",
    "  return c0Aphi[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.process_time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.process_time() - self.epoch_time_start)\n",
    "        \n",
    "#### preparing data ####\n",
    "\n",
    "data_cerevisiae_nucle = pd.read_csv(\"/Users/Brody1/Documents/Northwestern/DNA_Cyclizability/cycle1.txt\",delimiter = \",\")\n",
    "X1 = []\n",
    "for sequence_nt in data_cerevisiae_nucle[\"Sequence\"]:\n",
    "    X1.append(dnaOneHot(sequence_nt))\n",
    "X1 = array(X1)\n",
    "X1 = X1.reshape((X1.shape[0],50,4,1))\n",
    "X1_reverse = np.flip(X1,[1,2])\n",
    "# Y1 = data_cerevisiae_nucle[\"C0\"].values.astype(float)\n",
    "Y1 = find_c0new(data_cerevisiae_nucle).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_random_library = pd.read_csv(\"/Users/Brody1/Documents/Northwestern/DNA_Cyclizability/cycle3.txt\",delimiter = \",\")\n",
    "X3 = []\n",
    "for sequence_nt in data_random_library[\"Sequence\"]:\n",
    "    X3.append(dnaOneHot(sequence_nt))\n",
    "X3 = array(X3)\n",
    "X3 = X3.reshape((X3.shape[0],50,4,1))\n",
    "X3_reverse = np.flip(X3,[1,2])\n",
    "# Y3 = data_random_library[\"C0\"].values.astype(float)\n",
    "Y3 = find_c0new(data_random_library).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tiling = pd.read_csv(\"/Users/Brody1/Documents/Northwestern/DNA_Cyclizability/cycle5.txt\",delimiter = \",\")\n",
    "X5 = []\n",
    "for sequence_nt in data_tiling[\"Sequence\"]:\n",
    "    X5.append(dnaOneHot(sequence_nt))\n",
    "X5 = array(X5)\n",
    "X5 = X5.reshape((X5.shape[0],50,4,1))\n",
    "X5_reverse = np.flip(X5,[1,2])\n",
    "# Y5 = data_tiling[\"C0\"].values.astype(float)\n",
    "Y5 = find_c0new(data_tiling).astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chr5 = pd.read_csv(\"/Users/Brody1/Documents/Northwestern/DNA_Cyclizability/cycle6.txt\",delimiter = \",\")\n",
    "X6 = []\n",
    "for sequence_nt in data_chr5[\"Sequence\"]:\n",
    "    X6.append(dnaOneHot(sequence_nt))\n",
    "X6 = array(X6)\n",
    "X6 = X6.reshape((X6.shape[0],50,4,1))\n",
    "X6_reverse = np.flip(X6,[1,2])\n",
    "# Y6 = data_chr5[\"C0\"].values.astype(float)\n",
    "Y6 = find_c0new(data_chr5).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 17:29:32.542413: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2295/2317 [============================>.] - ETA: 0s - loss: 0.2819\n",
      "Epoch 1: val_loss improved from inf to 0.21694, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_1.h5\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2814 - val_loss: 0.2169\n",
      "Epoch 2/10\n",
      "2310/2317 [============================>.] - ETA: 0s - loss: 0.2188\n",
      "Epoch 2: val_loss did not improve from 0.21694\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2187 - val_loss: 0.2192\n",
      "Epoch 3/10\n",
      "2312/2317 [============================>.] - ETA: 0s - loss: 0.2175\n",
      "Epoch 3: val_loss improved from 0.21694 to 0.21272, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_1.h5\n",
      "2317/2317 [==============================] - 6s 3ms/step - loss: 0.2176 - val_loss: 0.2127\n",
      "Epoch 4/10\n",
      "2303/2317 [============================>.] - ETA: 0s - loss: 0.2171\n",
      "Epoch 4: val_loss did not improve from 0.21272\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2171 - val_loss: 0.2172\n",
      "Epoch 5/10\n",
      "2294/2317 [============================>.] - ETA: 0s - loss: 0.2171\n",
      "Epoch 5: val_loss did not improve from 0.21272\n",
      "2317/2317 [==============================] - 6s 2ms/step - loss: 0.2168 - val_loss: 0.2158\n",
      "Epoch 6/10\n",
      "2294/2317 [============================>.] - ETA: 0s - loss: 0.2161\n",
      "Epoch 6: val_loss improved from 0.21272 to 0.21091, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_1.h5\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2162 - val_loss: 0.2109\n",
      "Epoch 7/10\n",
      "2297/2317 [============================>.] - ETA: 0s - loss: 0.2156\n",
      "Epoch 7: val_loss did not improve from 0.21091\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2157 - val_loss: 0.2132\n",
      "Epoch 8/10\n",
      "2301/2317 [============================>.] - ETA: 0s - loss: 0.2160\n",
      "Epoch 8: val_loss did not improve from 0.21091\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2159 - val_loss: 0.2120\n",
      "Epoch 9/10\n",
      "2301/2317 [============================>.] - ETA: 0s - loss: 0.2153\n",
      "Epoch 9: val_loss improved from 0.21091 to 0.20975, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_1.h5\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2154 - val_loss: 0.2098\n",
      "Epoch 10/10\n",
      "2298/2317 [============================>.] - ETA: 0s - loss: 0.2151\n",
      "Epoch 10: val_loss did not improve from 0.20975\n",
      "2317/2317 [==============================] - 6s 2ms/step - loss: 0.2151 - val_loss: 0.2107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2317/2317 [==============================] - 3s 1ms/step\n",
      "2317/2317 [==============================] - 3s 1ms/step\n",
      "623/623 [==============================] - 1s 1ms/step\n",
      "623/623 [==============================] - 1s 1ms/step\n",
      "390/390 [==============================] - 0s 1ms/step\n",
      "390/390 [==============================] - 0s 1ms/step\n",
      "258/258 [==============================] - 0s 1ms/step\n",
      "258/258 [==============================] - 0s 1ms/step\n",
      "2576/2576 [==============================] - 4s 2ms/step\n",
      "2576/2576 [==============================] - 4s 1ms/step\n",
      "Epoch 1/10\n",
      "2304/2317 [============================>.] - ETA: 0s - loss: 0.2806\n",
      "Epoch 1: val_loss improved from inf to 0.21825, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_2.h5\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2802 - val_loss: 0.2182\n",
      "Epoch 2/10\n",
      "2309/2317 [============================>.] - ETA: 0s - loss: 0.2175\n",
      "Epoch 2: val_loss improved from 0.21825 to 0.21474, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_2.h5\n",
      "2317/2317 [==============================] - 9s 4ms/step - loss: 0.2175 - val_loss: 0.2147\n",
      "Epoch 3/10\n",
      "2309/2317 [============================>.] - ETA: 0s - loss: 0.2162\n",
      "Epoch 3: val_loss improved from 0.21474 to 0.21463, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_2.h5\n",
      "2317/2317 [==============================] - 10s 4ms/step - loss: 0.2163 - val_loss: 0.2146\n",
      "Epoch 4/10\n",
      "2297/2317 [============================>.] - ETA: 0s - loss: 0.2161\n",
      "Epoch 4: val_loss improved from 0.21463 to 0.21301, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_2.h5\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2163 - val_loss: 0.2130\n",
      "Epoch 5/10\n",
      "2295/2317 [============================>.] - ETA: 0s - loss: 0.2151\n",
      "Epoch 5: val_loss did not improve from 0.21301\n",
      "2317/2317 [==============================] - 6s 3ms/step - loss: 0.2152 - val_loss: 0.2145\n",
      "Epoch 6/10\n",
      "2310/2317 [============================>.] - ETA: 0s - loss: 0.2145\n",
      "Epoch 6: val_loss improved from 0.21301 to 0.21039, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_2.h5\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2144 - val_loss: 0.2104\n",
      "Epoch 7/10\n",
      "2297/2317 [============================>.] - ETA: 0s - loss: 0.2147\n",
      "Epoch 7: val_loss did not improve from 0.21039\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2148 - val_loss: 0.2132\n",
      "Epoch 8/10\n",
      "2309/2317 [============================>.] - ETA: 0s - loss: 0.2139\n",
      "Epoch 8: val_loss did not improve from 0.21039\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2141 - val_loss: 0.2152\n",
      "Epoch 9/10\n",
      "2311/2317 [============================>.] - ETA: 0s - loss: 0.2145\n",
      "Epoch 9: val_loss did not improve from 0.21039\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2144 - val_loss: 0.2143\n",
      "Epoch 10/10\n",
      "2301/2317 [============================>.] - ETA: 0s - loss: 0.2143\n",
      "Epoch 10: val_loss improved from 0.21039 to 0.20932, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_2.h5\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2142 - val_loss: 0.2093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2317/2317 [==============================] - 2s 876us/step\n",
      "2317/2317 [==============================] - 2s 958us/step\n",
      "623/623 [==============================] - 1s 966us/step\n",
      "623/623 [==============================] - 1s 860us/step\n",
      "390/390 [==============================] - 0s 867us/step\n",
      "390/390 [==============================] - 0s 1ms/step\n",
      "258/258 [==============================] - 0s 1ms/step\n",
      "258/258 [==============================] - 0s 1ms/step\n",
      "2576/2576 [==============================] - 2s 960us/step\n",
      "2576/2576 [==============================] - 3s 978us/step\n",
      "Epoch 1/10\n",
      "2301/2317 [============================>.] - ETA: 0s - loss: 0.2783\n",
      "Epoch 1: val_loss improved from inf to 0.21427, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_3.h5\n",
      "2317/2317 [==============================] - 6s 2ms/step - loss: 0.2780 - val_loss: 0.2143\n",
      "Epoch 2/10\n",
      "2306/2317 [============================>.] - ETA: 0s - loss: 0.2183\n",
      "Epoch 2: val_loss improved from 0.21427 to 0.21203, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_3.h5\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2184 - val_loss: 0.2120\n",
      "Epoch 3/10\n",
      "2311/2317 [============================>.] - ETA: 0s - loss: 0.2163\n",
      "Epoch 3: val_loss improved from 0.21203 to 0.21149, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_3.h5\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2163 - val_loss: 0.2115\n",
      "Epoch 4/10\n",
      "2306/2317 [============================>.] - ETA: 0s - loss: 0.2169\n",
      "Epoch 4: val_loss did not improve from 0.21149\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2167 - val_loss: 0.2116\n",
      "Epoch 5/10\n",
      "2312/2317 [============================>.] - ETA: 0s - loss: 0.2166\n",
      "Epoch 5: val_loss did not improve from 0.21149\n",
      "2317/2317 [==============================] - 6s 2ms/step - loss: 0.2166 - val_loss: 0.2155\n",
      "Epoch 6/10\n",
      "2301/2317 [============================>.] - ETA: 0s - loss: 0.2157\n",
      "Epoch 6: val_loss improved from 0.21149 to 0.21092, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_3.h5\n",
      "2317/2317 [==============================] - 6s 3ms/step - loss: 0.2158 - val_loss: 0.2109\n",
      "Epoch 7/10\n",
      "2300/2317 [============================>.] - ETA: 0s - loss: 0.2160\n",
      "Epoch 7: val_loss did not improve from 0.21092\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2160 - val_loss: 0.2192\n",
      "Epoch 8/10\n",
      "2309/2317 [============================>.] - ETA: 0s - loss: 0.2155\n",
      "Epoch 8: val_loss did not improve from 0.21092\n",
      "2317/2317 [==============================] - 6s 3ms/step - loss: 0.2156 - val_loss: 0.2148\n",
      "Epoch 9/10\n",
      "2300/2317 [============================>.] - ETA: 0s - loss: 0.2157\n",
      "Epoch 9: val_loss improved from 0.21092 to 0.21092, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_3.h5\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2158 - val_loss: 0.2109\n",
      "Epoch 10/10\n",
      "2304/2317 [============================>.] - ETA: 0s - loss: 0.2163\n",
      "Epoch 10: val_loss improved from 0.21092 to 0.21028, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_3.h5\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2162 - val_loss: 0.2103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2317/2317 [==============================] - 3s 1ms/step\n",
      "2317/2317 [==============================] - 3s 1ms/step\n",
      "623/623 [==============================] - 1s 1ms/step\n",
      "623/623 [==============================] - 1s 1ms/step\n",
      "390/390 [==============================] - 0s 1ms/step\n",
      "390/390 [==============================] - 0s 1ms/step\n",
      "258/258 [==============================] - 0s 1ms/step\n",
      "258/258 [==============================] - 0s 982us/step\n",
      "2576/2576 [==============================] - 3s 1ms/step\n",
      "2576/2576 [==============================] - 3s 1ms/step\n",
      "Epoch 1/10\n",
      "2300/2317 [============================>.] - ETA: 0s - loss: 0.2735\n",
      "Epoch 1: val_loss improved from inf to 0.22190, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_4.h5\n",
      "2317/2317 [==============================] - 8s 3ms/step - loss: 0.2729 - val_loss: 0.2219\n",
      "Epoch 2/10\n",
      "2306/2317 [============================>.] - ETA: 0s - loss: 0.2181\n",
      "Epoch 2: val_loss did not improve from 0.22190\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2179 - val_loss: 0.2225\n",
      "Epoch 3/10\n",
      "2312/2317 [============================>.] - ETA: 0s - loss: 0.2171\n",
      "Epoch 3: val_loss improved from 0.22190 to 0.21867, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_4.h5\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2171 - val_loss: 0.2187\n",
      "Epoch 4/10\n",
      "2306/2317 [============================>.] - ETA: 0s - loss: 0.2162\n",
      "Epoch 4: val_loss did not improve from 0.21867\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2162 - val_loss: 0.2196\n",
      "Epoch 5/10\n",
      "2309/2317 [============================>.] - ETA: 0s - loss: 0.2154\n",
      "Epoch 5: val_loss did not improve from 0.21867\n",
      "2317/2317 [==============================] - 8s 4ms/step - loss: 0.2153 - val_loss: 0.2223\n",
      "Epoch 6/10\n",
      "2312/2317 [============================>.] - ETA: 0s - loss: 0.2154\n",
      "Epoch 6: val_loss improved from 0.21867 to 0.21590, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_4.h5\n",
      "2317/2317 [==============================] - 8s 3ms/step - loss: 0.2155 - val_loss: 0.2159\n",
      "Epoch 7/10\n",
      "2303/2317 [============================>.] - ETA: 0s - loss: 0.2146\n",
      "Epoch 7: val_loss did not improve from 0.21590\n",
      "2317/2317 [==============================] - 8s 3ms/step - loss: 0.2145 - val_loss: 0.2192\n",
      "Epoch 8/10\n",
      "2305/2317 [============================>.] - ETA: 0s - loss: 0.2146\n",
      "Epoch 8: val_loss improved from 0.21590 to 0.21554, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_4.h5\n",
      "2317/2317 [==============================] - 8s 3ms/step - loss: 0.2146 - val_loss: 0.2155\n",
      "Epoch 9/10\n",
      "2311/2317 [============================>.] - ETA: 0s - loss: 0.2142\n",
      "Epoch 9: val_loss did not improve from 0.21554\n",
      "2317/2317 [==============================] - 8s 3ms/step - loss: 0.2142 - val_loss: 0.2157\n",
      "Epoch 10/10\n",
      "2316/2317 [============================>.] - ETA: 0s - loss: 0.2140\n",
      "Epoch 10: val_loss did not improve from 0.21554\n",
      "2317/2317 [==============================] - 8s 3ms/step - loss: 0.2140 - val_loss: 0.2179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2317/2317 [==============================] - 3s 1ms/step\n",
      "2317/2317 [==============================] - 3s 1ms/step\n",
      "623/623 [==============================] - 1s 1ms/step\n",
      "623/623 [==============================] - 1s 1ms/step\n",
      "390/390 [==============================] - 1s 1ms/step\n",
      "390/390 [==============================] - 1s 1ms/step\n",
      "258/258 [==============================] - 0s 1ms/step\n",
      "258/258 [==============================] - 0s 1ms/step\n",
      "2576/2576 [==============================] - 4s 2ms/step\n",
      "2576/2576 [==============================] - 3s 1ms/step\n",
      "Epoch 1/10\n",
      "2317/2317 [==============================] - ETA: 0s - loss: 0.2719\n",
      "Epoch 1: val_loss improved from inf to 0.22959, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_5.h5\n",
      "2317/2317 [==============================] - 8s 3ms/step - loss: 0.2719 - val_loss: 0.2296\n",
      "Epoch 2/10\n",
      "2303/2317 [============================>.] - ETA: 0s - loss: 0.2196\n",
      "Epoch 2: val_loss improved from 0.22959 to 0.21230, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_5.h5\n",
      "2317/2317 [==============================] - 8s 3ms/step - loss: 0.2194 - val_loss: 0.2123\n",
      "Epoch 3/10\n",
      "2316/2317 [============================>.] - ETA: 0s - loss: 0.2183\n",
      "Epoch 3: val_loss improved from 0.21230 to 0.20745, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_5.h5\n",
      "2317/2317 [==============================] - 8s 3ms/step - loss: 0.2183 - val_loss: 0.2075\n",
      "Epoch 4/10\n",
      "2308/2317 [============================>.] - ETA: 0s - loss: 0.2176\n",
      "Epoch 4: val_loss did not improve from 0.20745\n",
      "2317/2317 [==============================] - 8s 3ms/step - loss: 0.2176 - val_loss: 0.2085\n",
      "Epoch 5/10\n",
      "2310/2317 [============================>.] - ETA: 0s - loss: 0.2166\n",
      "Epoch 5: val_loss did not improve from 0.20745\n",
      "2317/2317 [==============================] - 8s 3ms/step - loss: 0.2166 - val_loss: 0.2104\n",
      "Epoch 6/10\n",
      "2310/2317 [============================>.] - ETA: 0s - loss: 0.2167\n",
      "Epoch 6: val_loss did not improve from 0.20745\n",
      "2317/2317 [==============================] - 8s 3ms/step - loss: 0.2168 - val_loss: 0.2097\n",
      "Epoch 7/10\n",
      "2306/2317 [============================>.] - ETA: 0s - loss: 0.2163\n",
      "Epoch 7: val_loss did not improve from 0.20745\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2164 - val_loss: 0.2157\n",
      "Epoch 8/10\n",
      "2303/2317 [============================>.] - ETA: 0s - loss: 0.2165\n",
      "Epoch 8: val_loss did not improve from 0.20745\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2164 - val_loss: 0.2152\n",
      "Epoch 9/10\n",
      "2304/2317 [============================>.] - ETA: 0s - loss: 0.2163\n",
      "Epoch 9: val_loss did not improve from 0.20745\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2162 - val_loss: 0.2115\n",
      "Epoch 10/10\n",
      "2306/2317 [============================>.] - ETA: 0s - loss: 0.2157\n",
      "Epoch 10: val_loss did not improve from 0.20745\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2157 - val_loss: 0.2104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2317/2317 [==============================] - 3s 1ms/step\n",
      "2317/2317 [==============================] - 3s 1ms/step\n",
      "623/623 [==============================] - 1s 1ms/step\n",
      "623/623 [==============================] - 1s 1ms/step\n",
      "390/390 [==============================] - 0s 1ms/step\n",
      "390/390 [==============================] - 0s 1ms/step\n",
      "258/258 [==============================] - 0s 1ms/step\n",
      "258/258 [==============================] - 0s 1ms/step\n",
      "2576/2576 [==============================] - 3s 1ms/step\n",
      "2576/2576 [==============================] - 3s 1ms/step\n",
      "Epoch 1/10\n",
      "2313/2317 [============================>.] - ETA: 0s - loss: 0.2654\n",
      "Epoch 1: val_loss improved from inf to 0.22633, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_6.h5\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2653 - val_loss: 0.2263\n",
      "Epoch 2/10\n",
      "2315/2317 [============================>.] - ETA: 0s - loss: 0.2182\n",
      "Epoch 2: val_loss improved from 0.22633 to 0.21965, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_6.h5\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2183 - val_loss: 0.2196\n",
      "Epoch 3/10\n",
      "2312/2317 [============================>.] - ETA: 0s - loss: 0.2172\n",
      "Epoch 3: val_loss improved from 0.21965 to 0.21659, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_6.h5\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2172 - val_loss: 0.2166\n",
      "Epoch 4/10\n",
      "2316/2317 [============================>.] - ETA: 0s - loss: 0.2159\n",
      "Epoch 4: val_loss improved from 0.21659 to 0.21554, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_6.h5\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2159 - val_loss: 0.2155\n",
      "Epoch 5/10\n",
      "2306/2317 [============================>.] - ETA: 0s - loss: 0.2160\n",
      "Epoch 5: val_loss did not improve from 0.21554\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2159 - val_loss: 0.2164\n",
      "Epoch 6/10\n",
      "2303/2317 [============================>.] - ETA: 0s - loss: 0.2151\n",
      "Epoch 6: val_loss improved from 0.21554 to 0.21540, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_6.h5\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2150 - val_loss: 0.2154\n",
      "Epoch 7/10\n",
      "2309/2317 [============================>.] - ETA: 0s - loss: 0.2153\n",
      "Epoch 7: val_loss did not improve from 0.21540\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2152 - val_loss: 0.2169\n",
      "Epoch 8/10\n",
      "2303/2317 [============================>.] - ETA: 0s - loss: 0.2145\n",
      "Epoch 8: val_loss did not improve from 0.21540\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2145 - val_loss: 0.2158\n",
      "Epoch 9/10\n",
      "2304/2317 [============================>.] - ETA: 0s - loss: 0.2142\n",
      "Epoch 9: val_loss did not improve from 0.21540\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2143 - val_loss: 0.2157\n",
      "Epoch 10/10\n",
      "2313/2317 [============================>.] - ETA: 0s - loss: 0.2138\n",
      "Epoch 10: val_loss improved from 0.21540 to 0.21519, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_6.h5\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2138 - val_loss: 0.2152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2317/2317 [==============================] - 3s 1ms/step\n",
      "2317/2317 [==============================] - 3s 1ms/step\n",
      "623/623 [==============================] - 1s 1ms/step\n",
      "623/623 [==============================] - 1s 1ms/step\n",
      "390/390 [==============================] - 0s 1ms/step\n",
      "390/390 [==============================] - 0s 1ms/step\n",
      "258/258 [==============================] - 0s 1ms/step\n",
      "258/258 [==============================] - 0s 1ms/step\n",
      "2576/2576 [==============================] - 3s 1ms/step\n",
      "2576/2576 [==============================] - 3s 1ms/step\n",
      "Epoch 1/10\n",
      "2299/2317 [============================>.] - ETA: 0s - loss: 0.2736\n",
      "Epoch 1: val_loss improved from inf to 0.21090, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_7.h5\n",
      "2317/2317 [==============================] - 8s 3ms/step - loss: 0.2731 - val_loss: 0.2109\n",
      "Epoch 2/10\n",
      "2305/2317 [============================>.] - ETA: 0s - loss: 0.2185\n",
      "Epoch 2: val_loss improved from 0.21090 to 0.20669, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_7.h5\n",
      "2317/2317 [==============================] - 7s 3ms/step - loss: 0.2186 - val_loss: 0.2067\n",
      "Epoch 3/10\n",
      "2312/2317 [============================>.] - ETA: 0s - loss: 0.2187\n",
      "Epoch 3: val_loss did not improve from 0.20669\n",
      "2317/2317 [==============================] - 6s 3ms/step - loss: 0.2188 - val_loss: 0.2087\n",
      "Epoch 4/10\n",
      "2307/2317 [============================>.] - ETA: 0s - loss: 0.2182\n",
      "Epoch 4: val_loss improved from 0.20669 to 0.20559, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_7.h5\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2182 - val_loss: 0.2056\n",
      "Epoch 5/10\n",
      "2297/2317 [============================>.] - ETA: 0s - loss: 0.2173\n",
      "Epoch 5: val_loss did not improve from 0.20559\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2174 - val_loss: 0.2086\n",
      "Epoch 6/10\n",
      "2303/2317 [============================>.] - ETA: 0s - loss: 0.2173\n",
      "Epoch 6: val_loss improved from 0.20559 to 0.20491, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_7.h5\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2172 - val_loss: 0.2049\n",
      "Epoch 7/10\n",
      "2313/2317 [============================>.] - ETA: 0s - loss: 0.2172\n",
      "Epoch 7: val_loss did not improve from 0.20491\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2172 - val_loss: 0.2065\n",
      "Epoch 8/10\n",
      "2311/2317 [============================>.] - ETA: 0s - loss: 0.2166\n",
      "Epoch 8: val_loss did not improve from 0.20491\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2166 - val_loss: 0.2050\n",
      "Epoch 9/10\n",
      "2307/2317 [============================>.] - ETA: 0s - loss: 0.2163\n",
      "Epoch 9: val_loss did not improve from 0.20491\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2162 - val_loss: 0.2051\n",
      "Epoch 10/10\n",
      "2295/2317 [============================>.] - ETA: 0s - loss: 0.2163\n",
      "Epoch 10: val_loss did not improve from 0.20491\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2164 - val_loss: 0.2065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2317/2317 [==============================] - 2s 777us/step\n",
      "2317/2317 [==============================] - 2s 781us/step\n",
      "623/623 [==============================] - 1s 824us/step\n",
      "623/623 [==============================] - 0s 768us/step\n",
      "390/390 [==============================] - 0s 792us/step\n",
      "390/390 [==============================] - 0s 792us/step\n",
      "258/258 [==============================] - 0s 771us/step\n",
      "258/258 [==============================] - 0s 793us/step\n",
      "2576/2576 [==============================] - 2s 782us/step\n",
      "2576/2576 [==============================] - 2s 794us/step\n",
      "Epoch 1/10\n",
      "2303/2317 [============================>.] - ETA: 0s - loss: 0.2669\n",
      "Epoch 1: val_loss improved from inf to 0.21453, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_8.h5\n",
      "2317/2317 [==============================] - 6s 2ms/step - loss: 0.2667 - val_loss: 0.2145\n",
      "Epoch 2/10\n",
      "2294/2317 [============================>.] - ETA: 0s - loss: 0.2179\n",
      "Epoch 2: val_loss did not improve from 0.21453\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2181 - val_loss: 0.2165\n",
      "Epoch 3/10\n",
      "2304/2317 [============================>.] - ETA: 0s - loss: 0.2160\n",
      "Epoch 3: val_loss did not improve from 0.21453\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2159 - val_loss: 0.2157\n",
      "Epoch 4/10\n",
      "2304/2317 [============================>.] - ETA: 0s - loss: 0.2159\n",
      "Epoch 4: val_loss improved from 0.21453 to 0.21447, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_8.h5\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2160 - val_loss: 0.2145\n",
      "Epoch 5/10\n",
      "2298/2317 [============================>.] - ETA: 0s - loss: 0.2151\n",
      "Epoch 5: val_loss improved from 0.21447 to 0.21290, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_8.h5\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2154 - val_loss: 0.2129\n",
      "Epoch 6/10\n",
      "2298/2317 [============================>.] - ETA: 0s - loss: 0.2154\n",
      "Epoch 6: val_loss improved from 0.21290 to 0.21164, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_8.h5\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2155 - val_loss: 0.2116\n",
      "Epoch 7/10\n",
      "2289/2317 [============================>.] - ETA: 0s - loss: 0.2151\n",
      "Epoch 7: val_loss did not improve from 0.21164\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2151 - val_loss: 0.2118\n",
      "Epoch 8/10\n",
      "2299/2317 [============================>.] - ETA: 0s - loss: 0.2152\n",
      "Epoch 8: val_loss did not improve from 0.21164\n",
      "2317/2317 [==============================] - 5s 2ms/step - loss: 0.2150 - val_loss: 0.2117\n",
      "Epoch 9/10\n",
      "2315/2317 [============================>.] - ETA: 0s - loss: 0.2154\n",
      "Epoch 9: val_loss improved from 0.21164 to 0.21105, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_8.h5\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2154 - val_loss: 0.2110\n",
      "Epoch 10/10\n",
      "2308/2317 [============================>.] - ETA: 0s - loss: 0.2145\n",
      "Epoch 10: val_loss improved from 0.21105 to 0.21068, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_8.h5\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2146 - val_loss: 0.2107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2317/2317 [==============================] - 1s 615us/step\n",
      "2317/2317 [==============================] - 1s 611us/step\n",
      "623/623 [==============================] - 0s 634us/step\n",
      "623/623 [==============================] - 0s 614us/step\n",
      "390/390 [==============================] - 0s 621us/step\n",
      "390/390 [==============================] - 0s 621us/step\n",
      "258/258 [==============================] - 0s 622us/step\n",
      "258/258 [==============================] - 0s 624us/step\n",
      "2576/2576 [==============================] - 2s 615us/step\n",
      "2576/2576 [==============================] - 2s 614us/step\n",
      "Epoch 1/10\n",
      "2299/2317 [============================>.] - ETA: 0s - loss: 0.2696\n",
      "Epoch 1: val_loss improved from inf to 0.21777, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_9.h5\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2694 - val_loss: 0.2178\n",
      "Epoch 2/10\n",
      "2308/2317 [============================>.] - ETA: 0s - loss: 0.2195\n",
      "Epoch 2: val_loss improved from 0.21777 to 0.21144, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_9.h5\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2194 - val_loss: 0.2114\n",
      "Epoch 3/10\n",
      "2291/2317 [============================>.] - ETA: 0s - loss: 0.2180\n",
      "Epoch 3: val_loss improved from 0.21144 to 0.21017, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_9.h5\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2178 - val_loss: 0.2102\n",
      "Epoch 4/10\n",
      "2309/2317 [============================>.] - ETA: 0s - loss: 0.2163\n",
      "Epoch 4: val_loss improved from 0.21017 to 0.20852, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_9.h5\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2161 - val_loss: 0.2085\n",
      "Epoch 5/10\n",
      "2314/2317 [============================>.] - ETA: 0s - loss: 0.2159\n",
      "Epoch 5: val_loss did not improve from 0.20852\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2158 - val_loss: 0.2104\n",
      "Epoch 6/10\n",
      "2312/2317 [============================>.] - ETA: 0s - loss: 0.2156\n",
      "Epoch 6: val_loss did not improve from 0.20852\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2156 - val_loss: 0.2100\n",
      "Epoch 7/10\n",
      "2288/2317 [============================>.] - ETA: 0s - loss: 0.2154\n",
      "Epoch 7: val_loss did not improve from 0.20852\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2157 - val_loss: 0.2095\n",
      "Epoch 8/10\n",
      "2317/2317 [==============================] - ETA: 0s - loss: 0.2152\n",
      "Epoch 8: val_loss did not improve from 0.20852\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2152 - val_loss: 0.2099\n",
      "Epoch 9/10\n",
      "2293/2317 [============================>.] - ETA: 0s - loss: 0.2148\n",
      "Epoch 9: val_loss did not improve from 0.20852\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2148 - val_loss: 0.2087\n",
      "Epoch 10/10\n",
      "2316/2317 [============================>.] - ETA: 0s - loss: 0.2146\n",
      "Epoch 10: val_loss did not improve from 0.20852\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2146 - val_loss: 0.2089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2317/2317 [==============================] - 1s 611us/step\n",
      "2317/2317 [==============================] - 1s 611us/step\n",
      "623/623 [==============================] - 0s 631us/step\n",
      "623/623 [==============================] - 0s 618us/step\n",
      "390/390 [==============================] - 0s 625us/step\n",
      "390/390 [==============================] - 0s 617us/step\n",
      "258/258 [==============================] - 0s 621us/step\n",
      "258/258 [==============================] - 0s 619us/step\n",
      "2576/2576 [==============================] - 2s 614us/step\n",
      "2576/2576 [==============================] - 2s 618us/step\n",
      "Epoch 1/10\n",
      "2287/2317 [============================>.] - ETA: 0s - loss: 0.2808\n",
      "Epoch 1: val_loss improved from inf to 0.21682, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_10.h5\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2799 - val_loss: 0.2168\n",
      "Epoch 2/10\n",
      "2298/2317 [============================>.] - ETA: 0s - loss: 0.2173\n",
      "Epoch 2: val_loss did not improve from 0.21682\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2173 - val_loss: 0.2210\n",
      "Epoch 3/10\n",
      "2311/2317 [============================>.] - ETA: 0s - loss: 0.2159\n",
      "Epoch 3: val_loss did not improve from 0.21682\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2159 - val_loss: 0.2188\n",
      "Epoch 4/10\n",
      "2287/2317 [============================>.] - ETA: 0s - loss: 0.2149\n",
      "Epoch 4: val_loss improved from 0.21682 to 0.21339, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_10.h5\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2154 - val_loss: 0.2134\n",
      "Epoch 5/10\n",
      "2292/2317 [============================>.] - ETA: 0s - loss: 0.2148\n",
      "Epoch 5: val_loss did not improve from 0.21339\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2148 - val_loss: 0.2148\n",
      "Epoch 6/10\n",
      "2306/2317 [============================>.] - ETA: 0s - loss: 0.2148\n",
      "Epoch 6: val_loss improved from 0.21339 to 0.21319, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_10.h5\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2148 - val_loss: 0.2132\n",
      "Epoch 7/10\n",
      "2301/2317 [============================>.] - ETA: 0s - loss: 0.2140\n",
      "Epoch 7: val_loss did not improve from 0.21319\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2139 - val_loss: 0.2138\n",
      "Epoch 8/10\n",
      "2294/2317 [============================>.] - ETA: 0s - loss: 0.2147\n",
      "Epoch 8: val_loss improved from 0.21319 to 0.21215, saving model to /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_10.h5\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2143 - val_loss: 0.2122\n",
      "Epoch 9/10\n",
      "2296/2317 [============================>.] - ETA: 0s - loss: 0.2141\n",
      "Epoch 9: val_loss did not improve from 0.21215\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2142 - val_loss: 0.2166\n",
      "Epoch 10/10\n",
      "2298/2317 [============================>.] - ETA: 0s - loss: 0.2139\n",
      "Epoch 10: val_loss did not improve from 0.21215\n",
      "2317/2317 [==============================] - 4s 2ms/step - loss: 0.2140 - val_loss: 0.2131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Brody1/Documents/Northwestern/DNA_Cyclizability/benchmarks/deep-learning/conv_only_tiling_10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2317/2317 [==============================] - 1s 625us/step\n",
      "2317/2317 [==============================] - 2s 705us/step\n",
      "623/623 [==============================] - 0s 639us/step\n",
      "623/623 [==============================] - 0s 616us/step\n",
      "390/390 [==============================] - 0s 650us/step\n",
      "390/390 [==============================] - 0s 676us/step\n",
      "258/258 [==============================] - 0s 633us/step\n",
      "258/258 [==============================] - 0s 633us/step\n",
      "2576/2576 [==============================] - 2s 683us/step\n",
      "2576/2576 [==============================] - 2s 715us/step\n"
     ]
    }
   ],
   "source": [
    "#### tiling\n",
    "\n",
    "VALIDATION_LOSS = []\n",
    "fold_var = 1\n",
    "n = Y5.shape[0]\n",
    "\n",
    "fits = []\n",
    "detrend = []\n",
    "times = []\n",
    "times2 = []\n",
    "\n",
    "for train_index, val_index in kf.split(Y5):\n",
    "    training_X = X5[train_index]\n",
    "    training_X_reverse = X5_reverse[train_index]\n",
    "    validation_X = X5[val_index]\n",
    "    validation_X_reverse = X5_reverse[val_index]\n",
    "    training_Y = Y5[train_index]\n",
    "    validation_Y = Y5[val_index]\n",
    "    # CREATE NEW MODEL\n",
    "    model = model_cycle()\n",
    "    # CREATE CALLBACKS\n",
    "    checkpoint = callbacks.ModelCheckpoint(save_path + model_name+\"_tiling_\"+str(fold_var)+\".h5\",\n",
    "                                                    monitor='val_loss', verbose=1,\n",
    "                                                    save_best_only=True, mode='min')\n",
    "    time_callback = TimeHistory()\n",
    "\n",
    "    history = model.fit(training_X, training_Y,\n",
    "                        epochs=num_epochs,\n",
    "                        callbacks= [checkpoint, time_callback],\n",
    "                        validation_data=(validation_X, validation_Y))\n",
    "    model.load_weights(save_path + model_name+\"_tiling_\"+str(fold_var)+\".h5\")\n",
    "    model.save(save_path+model_name+\"_tiling_\"+str(fold_var),save_traces=False)\n",
    "    times.append(time_callback.times)\n",
    "\n",
    "    pred_Y = model.predict(training_X)\n",
    "    pred_Y = pred_Y.reshape(pred_Y.shape[0])\n",
    "    pred_Y_reverse = model.predict(training_X_reverse)\n",
    "    pred_Y_reverse = pred_Y_reverse.reshape(pred_Y_reverse.shape[0])\n",
    "    pred_Y = (pred_Y+pred_Y_reverse)/2\n",
    "    reg =  LinearRegression().fit(array(pred_Y).reshape(-1, 1), array(training_Y).reshape(-1, 1))\n",
    "    \n",
    "    detrend_int = reg.intercept_\n",
    "    detrend_slope = reg.coef_\n",
    "    detrend.append([float(detrend_int), float(detrend_slope)])\n",
    "\n",
    "    start_time = time.process_time()\n",
    "    fit = model.predict(X1)\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_reverse = model.predict(X1_reverse)\n",
    "    fit_reverse = fit_reverse.reshape(fit_reverse.shape[0])\n",
    "    reverse_corr = np.corrcoef(fit, fit_reverse)[0,1]\n",
    "    fit = (fit + fit_reverse)/2\n",
    "    fit = fit.flatten()\n",
    "    fit_tmp =[np.corrcoef(fit, Y1)[0,1],np.mean(np.square(fit-Y1)),np.mean(fit),np.std(fit),reverse_corr]\n",
    "    fits.append(fit_tmp)\n",
    "    fit = detrend_int + fit * detrend_slope\n",
    "    fit = fit.flatten()\n",
    "    fit_tmp =[np.corrcoef(fit, Y1)[0,1],np.mean(np.square(fit-Y1)),np.mean(fit),np.std(fit),reverse_corr]\n",
    "    time0 = time.process_time() - start_time\n",
    "    times2.append([time0])\n",
    "    fits.append(fit_tmp)\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model.predict(X3)\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_reverse = model.predict(X3_reverse)\n",
    "    fit_reverse = fit_reverse.reshape(fit_reverse.shape[0])\n",
    "    reverse_corr = np.corrcoef(fit, fit_reverse)[0,1]\n",
    "    fit = (fit + fit_reverse)/2\n",
    "    fit = fit.flatten()\n",
    "    fit_tmp =[np.corrcoef(fit, Y3)[0,1],np.mean(np.square(fit-Y3)),np.mean(fit),np.std(fit),reverse_corr]\n",
    "    fits.append(fit_tmp)\n",
    "    fit = detrend_int + fit * detrend_slope\n",
    "    fit = fit.flatten()\n",
    "    fit_tmp =[np.corrcoef(fit, Y3)[0,1],np.mean(np.square(fit-Y3)),np.mean(fit),np.std(fit),reverse_corr]\n",
    "    time0 = time.process_time() - start_time\n",
    "    times2.append([time0])\n",
    "    fits.append(fit_tmp)\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model.predict(validation_X)\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_reverse = model.predict(validation_X_reverse)\n",
    "    fit_reverse = fit_reverse.reshape(fit_reverse.shape[0])\n",
    "    reverse_corr = np.corrcoef(fit, fit_reverse)[0,1]\n",
    "    fit = (fit + fit_reverse)/2\n",
    "    fit = fit.flatten()\n",
    "    fit_tmp =[np.corrcoef(fit, validation_Y)[0,1],np.mean(np.square(fit-validation_Y)),np.mean(fit),np.std(fit),reverse_corr]\n",
    "    fits.append(fit_tmp)\n",
    "    fit = detrend_int + fit * detrend_slope\n",
    "    fit = fit.flatten()\n",
    "    fit_tmp =[np.corrcoef(fit, validation_Y)[0,1],np.mean(np.square(fit-validation_Y)),np.mean(fit),np.std(fit),reverse_corr]\n",
    "    time0 = time.process_time() - start_time\n",
    "    times2.append([time0])\n",
    "    fits.append(fit_tmp)\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model.predict(X6)\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_reverse = model.predict(X6_reverse)\n",
    "    fit_reverse = fit_reverse.reshape(fit_reverse.shape[0])\n",
    "    reverse_corr = np.corrcoef(fit, fit_reverse)[0,1]\n",
    "    fit = (fit + fit_reverse)/2\n",
    "    fit = fit.flatten()\n",
    "    fit_tmp =[np.corrcoef(fit, Y6)[0,1],np.mean(np.square(fit-Y6)),np.mean(fit),np.std(fit),reverse_corr]\n",
    "    fits.append(fit_tmp)\n",
    "    fit = detrend_int + fit * detrend_slope\n",
    "    fit = fit.flatten()\n",
    "    fit_tmp =[np.corrcoef(fit, Y6)[0,1],np.mean(np.square(fit-Y6)),np.mean(fit),np.std(fit),reverse_corr]\n",
    "    time0 = time.process_time() - start_time\n",
    "    times2.append([time0])\n",
    "    fits.append(fit_tmp)\n",
    "    \n",
    "    K.clear_session()\n",
    "    fold_var += 1\n",
    "    \n",
    "detrend = array(detrend)\n",
    "detrend = pd.DataFrame(detrend)\n",
    "detrend.to_csv(save_path +model_name+\"_detrend_tiling.txt\", index = False)\n",
    "\n",
    "fits = array(fits)\n",
    "fits = pd.DataFrame((fits))\n",
    "fits.to_csv(save_path +model_name+ \"_fits_tiling.txt\", index = False)\n",
    "\n",
    "with open(save_path +model_name+\"_time_tiling.txt\", \"w\") as file:\n",
    "    for row in times:\n",
    "        s = \" \".join(map(str, row))\n",
    "        file.write(s+'\\n')\n",
    "\n",
    "with open(save_path +model_name+\"_pred_time_tiling.txt\", \"w\") as file:\n",
    "    for row in times2:\n",
    "        s = \" \".join(map(str, row))\n",
    "        file.write(s+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits = pd.read_csv(save_path +model_name+ \"_fits_tiling.txt\",delimiter = \",\")\n",
    "fits=array(fits.values.tolist())\n",
    "fits = np.transpose(fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average correlation on tiling: 0.3552491751584278 \n",
      "Average MSE on tiling: 0.2116396255226801 \n",
      "Average correlation on random: 0.39398630477831575 \n",
      "Average MSE on random: 0.12187173926202433 \n",
      "Average correlation on ChrV: 0.2831865412110821 \n",
      "Average MSE on ChrV: 0.2676530816022441 \n",
      "Average correlation on CN: 0.2967467853894225 \n",
      "Average MSE on CN: 0.1980794400475368\n"
     ]
    }
   ],
   "source": [
    "display_fits(fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(save_path + model_name + \"_tiling_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2574/2574 [==============================] - 2s 754us/step\n",
      "390/390 [==============================] - 0s 600us/step\n"
     ]
    }
   ],
   "source": [
    "first_conv_model = Model(inputs = model.input, outputs = model.layers[1].output)\n",
    "first_conv_output = first_conv_model.predict(X5)\n",
    "pd.DataFrame(first_conv_output.reshape(first_conv_output.shape[0], -1)).to_csv(save_path + model_name+\"_tiling_tiling_first_conv_output\")\n",
    "first_conv_output_random = first_conv_model.predict(X3)\n",
    "pd.DataFrame(first_conv_output_random.reshape(first_conv_output_random.shape[0], -1)).to_csv(save_path + model_name+\"_tiling_random_first_conv_output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.37548894, 0.        , 0.        , 0.        ,\n",
       "        0.63033015, 0.17333601, 0.        , 0.        , 0.4276547 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.35167992,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.5217654 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.16024733,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.3988306 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_conv_output[0][47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_conv_weights = model.layers[1].weights[0]\n",
    "first_conv_biases = model.layers[1].weights[1]\n",
    "# Format: [Position 0: A, C, G, T, Position 1: A, C, G, T, Position 2: A, C, G, T]\n",
    "pd.DataFrame(array(first_conv_weights).transpose((3,2,0,1)).reshape(first_conv_weights.shape[-1], -1)).to_csv(save_path + model_name+\"_tiling_first_conv_kernels\")\n",
    "pd.DataFrame(first_conv_biases).to_csv(save_path + model_name+\"_tiling_first_conv_biases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'conv2d/kernel:0' shape=(3, 4, 1, 48) dtype=float32, numpy=\n",
      "array([[[[-8.15694332e-02,  1.06328567e-02, -4.22265008e-02,\n",
      "          -9.87932738e-03, -8.39487538e-02,  4.30427603e-02,\n",
      "          -1.67828426e-01,  2.96451226e-02, -5.20521179e-02,\n",
      "           3.25803980e-02, -7.31743947e-02, -5.79910539e-02,\n",
      "           3.58043052e-02, -7.67797530e-02, -9.07838494e-02,\n",
      "          -7.02487975e-02,  6.50768578e-02,  1.58567369e-01,\n",
      "           4.29326203e-03, -3.68713252e-02, -3.29190157e-02,\n",
      "          -2.40225643e-01,  2.68410370e-02, -3.01064327e-02,\n",
      "          -7.29377717e-02, -2.09992863e-02, -2.22886577e-02,\n",
      "          -2.20489055e-02, -2.45698895e-02, -3.05987913e-02,\n",
      "           2.00851075e-02, -5.59120020e-03,  6.01138920e-02,\n",
      "          -4.43689078e-02, -6.31678775e-02, -4.54354808e-02,\n",
      "          -1.32699031e-02, -8.74557421e-02, -1.09437108e-01,\n",
      "           2.10127910e-03, -9.16371942e-02, -2.67568454e-02,\n",
      "           2.74200849e-02, -1.03594493e-02,  2.00665519e-02,\n",
      "          -7.62655661e-02, -1.08363386e-03, -1.77751742e-02]],\n",
      "\n",
      "        [[-1.48135787e-02,  1.59049630e-01, -1.48712425e-02,\n",
      "           6.45468235e-02,  1.09931901e-01,  7.77474642e-02,\n",
      "           6.18786812e-02,  4.31310944e-02, -2.36688536e-02,\n",
      "           8.44759718e-02, -9.51435268e-02, -4.15554876e-03,\n",
      "          -6.41828552e-02,  7.38873100e-03,  1.13408528e-01,\n",
      "           3.72832045e-02,  3.61695252e-02, -2.28054136e-01,\n",
      "           3.24840918e-02,  1.72828441e-04,  2.20689494e-02,\n",
      "           2.27965266e-01,  3.88385542e-02,  6.77024666e-03,\n",
      "          -7.59158358e-02, -4.87320498e-02, -3.63271013e-02,\n",
      "          -5.89338914e-02, -6.91509694e-02,  1.37510613e-01,\n",
      "          -1.22718781e-01,  4.85927053e-02, -8.80320221e-02,\n",
      "          -1.58820357e-02, -6.20269477e-02,  1.67539320e-03,\n",
      "           1.24981161e-02,  2.68540122e-02, -9.93878245e-02,\n",
      "          -2.29091533e-02, -1.79798866e-04,  6.65758029e-02,\n",
      "           1.06729142e-01, -7.86772370e-02, -2.09747814e-02,\n",
      "           9.28902533e-03, -5.64091690e-02, -4.97909822e-02]],\n",
      "\n",
      "        [[-2.39290632e-02,  1.67475596e-01, -1.97374858e-02,\n",
      "          -6.76847324e-02, -6.58369809e-02,  9.81846303e-02,\n",
      "           6.94877505e-02,  1.55125195e-02, -9.73729119e-02,\n",
      "           1.21495053e-01, -9.79699418e-02,  5.09188771e-02,\n",
      "           2.34640725e-02,  1.43204201e-02,  1.74607381e-01,\n",
      "          -5.08055352e-02,  9.44472179e-02,  1.07395537e-01,\n",
      "           2.10070293e-02,  1.47628291e-02, -7.61949643e-03,\n",
      "           1.56314343e-01,  2.11198404e-02, -7.46833012e-02,\n",
      "           2.32109893e-02, -7.82490522e-02, -3.42598483e-02,\n",
      "          -1.56535730e-02,  1.22825811e-02,  4.64945585e-02,\n",
      "           5.02010211e-02, -1.00610964e-01,  4.96963114e-02,\n",
      "          -2.45000347e-02, -1.16729751e-01, -1.49522219e-02,\n",
      "          -1.08660674e-02, -6.61451370e-02, -1.10445634e-01,\n",
      "           2.13458668e-02,  2.12054588e-02,  3.49780358e-02,\n",
      "           1.72989547e-01,  1.75133273e-02,  2.72132736e-02,\n",
      "           3.51938941e-02, -6.00857250e-02,  2.51356163e-03]],\n",
      "\n",
      "        [[-8.20983015e-03, -3.07123065e-01, -4.65271249e-02,\n",
      "           8.20704829e-03, -8.99714231e-02, -1.36048079e-01,\n",
      "           1.18556388e-01,  2.43674163e-02,  1.91322137e-02,\n",
      "           1.36472449e-01, -7.45865628e-02, -8.33904296e-02,\n",
      "          -7.21281096e-02, -2.66504791e-02,  2.83854157e-02,\n",
      "           2.77020168e-02,  6.88332096e-02,  8.51658508e-02,\n",
      "           2.48735324e-02,  4.60208766e-02, -3.91841009e-02,\n",
      "           4.40144800e-02,  2.05849074e-02, -4.86874878e-02,\n",
      "          -4.55755517e-02, -1.33442748e-02, -2.51958966e-02,\n",
      "           3.94821353e-03,  6.20983820e-03,  5.19794002e-02,\n",
      "          -1.24078974e-01,  3.74970213e-02,  6.38532499e-03,\n",
      "          -5.14121354e-02, -4.20755222e-02, -3.50166932e-02,\n",
      "           5.10803871e-02, -6.50144070e-02,  8.63325521e-02,\n",
      "           2.85832281e-03, -2.32793782e-02, -7.10128695e-02,\n",
      "          -1.85932338e-01, -2.55014189e-02,  5.39315343e-02,\n",
      "           1.66955274e-02, -3.38270096e-03,  6.26540452e-04]]],\n",
      "\n",
      "\n",
      "       [[[-5.77393770e-02,  9.84482560e-03,  1.87141337e-02,\n",
      "          -4.37122621e-02, -7.88094848e-02, -1.29983723e-01,\n",
      "           1.38337612e-01, -7.96231702e-02,  1.80073008e-02,\n",
      "          -3.12447906e-01,  5.83495758e-03, -7.46757016e-02,\n",
      "           4.14842516e-02, -4.73283082e-02,  2.00137906e-02,\n",
      "          -9.12639946e-02,  6.08644169e-03,  6.72561154e-02,\n",
      "          -8.62643421e-02, -7.23101897e-03,  3.87061313e-02,\n",
      "           3.01657282e-02, -1.00171007e-01, -2.54116803e-02,\n",
      "           8.96178186e-03,  6.78905938e-03,  3.26898284e-02,\n",
      "          -6.51479289e-02, -4.38119173e-02, -6.19389229e-02,\n",
      "          -5.76071963e-02, -6.16799705e-02, -5.14349639e-02,\n",
      "          -3.38441059e-02, -7.68567398e-02,  3.57909240e-02,\n",
      "          -9.21406597e-02, -7.50620514e-02,  1.22821532e-01,\n",
      "          -5.13096452e-02,  3.28787565e-02, -1.10689774e-02,\n",
      "          -9.43722576e-02, -8.23711157e-02, -1.83748472e-02,\n",
      "          -1.06694423e-01,  9.18158505e-04, -7.95329884e-02]],\n",
      "\n",
      "        [[-1.74440965e-02,  1.01113953e-01, -8.55389144e-03,\n",
      "          -1.96251571e-02,  1.11079112e-01,  1.57198668e-01,\n",
      "           7.68748969e-02, -4.39544953e-03,  9.86179244e-03,\n",
      "           2.11257920e-01, -5.54157607e-02, -7.02622235e-02,\n",
      "           4.59962599e-02,  5.64041398e-02,  1.88948736e-01,\n",
      "           7.04086125e-02, -9.76043269e-02,  6.23934343e-02,\n",
      "          -1.76147930e-02, -5.66723235e-02,  1.02776266e-03,\n",
      "          -1.01669624e-01, -5.40289935e-03,  5.54747693e-03,\n",
      "           9.28200502e-03, -3.77133489e-03, -1.78553872e-02,\n",
      "          -5.35341818e-03,  3.54676843e-02, -1.19508609e-01,\n",
      "          -9.40143466e-02,  1.67896673e-02, -5.58396839e-02,\n",
      "           4.42879573e-02, -7.25568756e-02, -8.54162872e-02,\n",
      "           2.25343816e-02,  1.15877949e-02, -5.02431020e-02,\n",
      "          -3.66509147e-02, -7.10151494e-02,  2.83780079e-02,\n",
      "           1.48168117e-01, -1.55563634e-02,  1.48051474e-02,\n",
      "           1.94891617e-02, -2.79324558e-02,  4.83995639e-02]],\n",
      "\n",
      "        [[-2.52614208e-02, -5.65465763e-02,  1.00147994e-02,\n",
      "          -5.34295142e-02, -8.00538361e-02,  1.76034108e-01,\n",
      "           5.17846234e-02, -1.84412394e-02, -4.30276431e-02,\n",
      "           1.03087343e-01, -3.83167565e-02, -6.83526024e-02,\n",
      "          -2.79809032e-02,  6.34859875e-02,  1.22943148e-01,\n",
      "           7.39730820e-02,  2.47053020e-02, -2.21167341e-01,\n",
      "          -1.28571820e-02, -1.75245143e-02,  7.71499723e-02,\n",
      "           3.67006771e-02,  3.67832258e-02,  5.93739236e-03,\n",
      "           6.17555678e-02, -3.78876925e-03, -1.70497708e-02,\n",
      "          -4.43469137e-02, -1.34612259e-03,  6.08999170e-02,\n",
      "           1.06467605e-01,  2.45907865e-02, -5.63997105e-02,\n",
      "          -2.56222878e-02, -7.21326023e-02,  5.67688160e-02,\n",
      "          -9.48527083e-02, -7.54129887e-02, -7.71869123e-02,\n",
      "          -2.37819720e-02,  1.27895996e-02, -1.83107145e-02,\n",
      "           1.94477905e-02, -1.18188141e-03,  6.97510410e-03,\n",
      "          -2.52107228e-03, -1.06286898e-01, -1.15350425e-01]],\n",
      "\n",
      "        [[-1.32511370e-02,  6.22294880e-02,  1.77087244e-02,\n",
      "          -3.25363912e-02, -8.13018307e-02,  1.08742982e-01,\n",
      "          -1.55692056e-01, -9.47779268e-02, -9.91739631e-02,\n",
      "           4.76240069e-02, -8.74432921e-02, -5.84049411e-02,\n",
      "          -4.68532406e-02,  5.87779060e-02, -1.99534416e-01,\n",
      "          -1.30100409e-02,  2.63443496e-02,  1.55259311e-01,\n",
      "           1.12622175e-02, -6.72922358e-02, -4.93215546e-02,\n",
      "           7.33045116e-03,  2.94118356e-02,  1.35414954e-02,\n",
      "          -3.34006771e-02, -3.66780125e-02,  1.28498282e-02,\n",
      "          -6.12874748e-03, -1.12652980e-01, -1.28706411e-01,\n",
      "          -7.09818080e-02, -1.40020680e-02, -8.21455121e-02,\n",
      "           5.32732857e-03,  7.90501982e-02,  7.15835579e-03,\n",
      "          -1.22030620e-02, -9.62963551e-02, -7.84587115e-02,\n",
      "          -1.50263123e-02, -4.57397364e-02,  3.84932943e-02,\n",
      "           8.45373720e-02, -1.54745653e-02, -4.82922457e-02,\n",
      "           1.67439599e-02, -6.76355660e-02, -3.29925865e-02]]],\n",
      "\n",
      "\n",
      "       [[[ 5.77822365e-02,  9.94275287e-02,  5.81165776e-02,\n",
      "          -9.23685730e-03, -1.08813889e-01,  1.43392265e-01,\n",
      "          -2.07632601e-01, -7.23541006e-02, -2.10082997e-02,\n",
      "           1.16582267e-01, -2.80266404e-02,  2.83403564e-02,\n",
      "           2.05718596e-02, -8.60655308e-02, -3.33034061e-02,\n",
      "          -6.80990294e-02, -3.81975388e-03, -1.41383316e-02,\n",
      "           1.00065060e-02, -3.17063555e-02,  6.56473916e-03,\n",
      "           4.87288684e-02, -3.97419557e-02, -1.32216997e-02,\n",
      "          -9.17784870e-02,  2.28900537e-02,  3.97186168e-02,\n",
      "           4.88430746e-02, -4.84179296e-02,  1.64098516e-01,\n",
      "          -1.14037521e-01,  3.01652160e-02, -8.67914781e-02,\n",
      "          -1.10740401e-02,  1.27247348e-01,  1.58919394e-02,\n",
      "          -7.93808550e-02, -6.66786060e-02, -1.35828238e-02,\n",
      "           3.47237997e-02, -4.98442166e-03, -6.29845634e-02,\n",
      "           5.77492714e-02, -7.07957447e-02, -3.64465751e-02,\n",
      "          -1.59333535e-02,  2.95612235e-02,  3.38397175e-02]],\n",
      "\n",
      "        [[-6.03526272e-03,  1.33655772e-01,  5.16057648e-02,\n",
      "          -6.36565983e-02,  5.55829331e-02,  7.16302171e-02,\n",
      "           6.33977875e-02,  1.12500601e-02,  1.73682440e-02,\n",
      "           5.94494268e-02,  3.75905633e-02, -1.96466558e-02,\n",
      "           5.65637741e-03, -6.23966753e-02,  1.27227724e-01,\n",
      "          -7.59080127e-02, -2.89275823e-03,  1.82037994e-01,\n",
      "          -6.34010658e-02, -2.17155870e-02, -1.15836347e-02,\n",
      "           5.64556755e-02, -1.47567354e-02,  2.43120715e-02,\n",
      "          -5.15890382e-02,  6.67095482e-02, -9.44965333e-02,\n",
      "          -8.99492055e-02, -5.89076988e-02,  6.14957102e-02,\n",
      "           2.81115510e-02, -1.74714066e-02,  5.60518280e-02,\n",
      "           1.46442866e-02, -9.63102579e-02,  4.91296640e-03,\n",
      "           2.16529928e-02,  5.52174635e-03, -8.41892660e-02,\n",
      "          -1.48929805e-02, -2.50465292e-02, -4.79933582e-02,\n",
      "          -1.57687113e-01, -3.57331857e-02, -6.17318042e-02,\n",
      "          -5.24225608e-02,  7.04685412e-03, -8.19279347e-03]],\n",
      "\n",
      "        [[-6.30251691e-02,  1.11525729e-01, -3.95000912e-02,\n",
      "           6.11063605e-03, -1.21969350e-01,  4.98823524e-02,\n",
      "           6.94818422e-02,  4.27260483e-03,  7.02155754e-03,\n",
      "           4.10640053e-02,  1.93905607e-02,  2.93058679e-02,\n",
      "           2.79935207e-02, -7.02728108e-02,  7.19543323e-02,\n",
      "          -8.86719748e-02, -4.56523336e-03,  1.12430610e-01,\n",
      "          -8.06436501e-03,  3.39260437e-02, -8.91760588e-02,\n",
      "           1.24274567e-01, -1.90564934e-02, -8.19904655e-02,\n",
      "          -3.06265205e-02, -9.63877439e-02,  4.28006314e-02,\n",
      "          -9.07915607e-02, -9.14140567e-02,  1.07785195e-01,\n",
      "           6.14300482e-02,  2.80508008e-02,  1.74854696e-02,\n",
      "          -8.49545076e-02, -2.15734169e-02, -4.91417982e-02,\n",
      "           5.43275587e-02, -1.03672713e-01, -4.71282080e-02,\n",
      "           3.68282460e-02, -5.90593293e-02, -1.13956377e-01,\n",
      "           1.45656630e-01,  1.00185703e-02, -5.35606779e-02,\n",
      "          -7.66057521e-02, -6.65757507e-02, -1.18805155e-01]],\n",
      "\n",
      "        [[-9.94604677e-02,  5.74178286e-02, -4.77893092e-02,\n",
      "           1.37311192e-02,  1.95492636e-02, -2.25334108e-01,\n",
      "           1.33391649e-01, -8.86695236e-02,  4.68980148e-02,\n",
      "           1.18547417e-01,  8.03042725e-02,  3.73063758e-02,\n",
      "          -3.50420699e-02, -7.99586475e-02,  2.26463955e-02,\n",
      "          -6.90495819e-02, -3.17942090e-02,  2.72310968e-03,\n",
      "          -5.91320507e-02,  3.95267643e-02, -5.73237166e-02,\n",
      "           4.20620404e-02, -4.19352204e-03, -8.15406814e-02,\n",
      "          -5.22375479e-02,  1.53693892e-02,  3.36354636e-02,\n",
      "           3.72483805e-02,  1.84492227e-02, -1.88445784e-02,\n",
      "          -9.96677577e-02, -2.79859751e-02,  5.15053310e-02,\n",
      "          -2.29320433e-02, -5.30095398e-02, -1.78345479e-02,\n",
      "          -6.64154487e-03, -4.38938960e-02, -1.67494814e-03,\n",
      "          -9.06189978e-02, -5.52403294e-02, -1.10943206e-01,\n",
      "          -2.22204365e-02, -1.98806971e-02, -9.20732319e-02,\n",
      "          -2.00910913e-03,  1.94963142e-02,  1.03710620e-02]]]],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "print(first_conv_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_conv_output[0][0][0][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82368, 50, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (48,3,4,1) (3,4,1,48) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_4/m5twwlnn26b0q41jgwd5jnz80000gq/T/ipykernel_55261/3430388511.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mX5\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_conv_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (48,3,4,1) (3,4,1,48) "
     ]
    }
   ],
   "source": [
    "[X5[0][i:i+3] for i in range(48)]*array(first_conv_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-8.15694332e-02  1.06328567e-02 -4.22265008e-02 -9.87932738e-03\n",
      "    -8.39487538e-02  4.30427603e-02 -1.67828426e-01  2.96451226e-02\n",
      "    -5.20521179e-02  3.25803980e-02 -7.31743947e-02 -5.79910539e-02\n",
      "     3.58043052e-02 -7.67797530e-02 -9.07838494e-02 -7.02487975e-02\n",
      "     6.50768578e-02  1.58567369e-01  4.29326203e-03 -3.68713252e-02\n",
      "    -3.29190157e-02 -2.40225643e-01  2.68410370e-02 -3.01064327e-02\n",
      "    -7.29377717e-02 -2.09992863e-02 -2.22886577e-02 -2.20489055e-02\n",
      "    -2.45698895e-02 -3.05987913e-02  2.00851075e-02 -5.59120020e-03\n",
      "     6.01138920e-02 -4.43689078e-02 -6.31678775e-02 -4.54354808e-02\n",
      "    -1.32699031e-02 -8.74557421e-02 -1.09437108e-01  2.10127910e-03\n",
      "    -9.16371942e-02 -2.67568454e-02  2.74200849e-02 -1.03594493e-02\n",
      "     2.00665519e-02 -7.62655661e-02 -1.08363386e-03 -1.77751742e-02]]\n",
      "\n",
      "  [[-1.48135787e-02  1.59049630e-01 -1.48712425e-02  6.45468235e-02\n",
      "     1.09931901e-01  7.77474642e-02  6.18786812e-02  4.31310944e-02\n",
      "    -2.36688536e-02  8.44759718e-02 -9.51435268e-02 -4.15554876e-03\n",
      "    -6.41828552e-02  7.38873100e-03  1.13408528e-01  3.72832045e-02\n",
      "     3.61695252e-02 -2.28054136e-01  3.24840918e-02  1.72828441e-04\n",
      "     2.20689494e-02  2.27965266e-01  3.88385542e-02  6.77024666e-03\n",
      "    -7.59158358e-02 -4.87320498e-02 -3.63271013e-02 -5.89338914e-02\n",
      "    -6.91509694e-02  1.37510613e-01 -1.22718781e-01  4.85927053e-02\n",
      "    -8.80320221e-02 -1.58820357e-02 -6.20269477e-02  1.67539320e-03\n",
      "     1.24981161e-02  2.68540122e-02 -9.93878245e-02 -2.29091533e-02\n",
      "    -1.79798866e-04  6.65758029e-02  1.06729142e-01 -7.86772370e-02\n",
      "    -2.09747814e-02  9.28902533e-03 -5.64091690e-02 -4.97909822e-02]]\n",
      "\n",
      "  [[-2.39290632e-02  1.67475596e-01 -1.97374858e-02 -6.76847324e-02\n",
      "    -6.58369809e-02  9.81846303e-02  6.94877505e-02  1.55125195e-02\n",
      "    -9.73729119e-02  1.21495053e-01 -9.79699418e-02  5.09188771e-02\n",
      "     2.34640725e-02  1.43204201e-02  1.74607381e-01 -5.08055352e-02\n",
      "     9.44472179e-02  1.07395537e-01  2.10070293e-02  1.47628291e-02\n",
      "    -7.61949643e-03  1.56314343e-01  2.11198404e-02 -7.46833012e-02\n",
      "     2.32109893e-02 -7.82490522e-02 -3.42598483e-02 -1.56535730e-02\n",
      "     1.22825811e-02  4.64945585e-02  5.02010211e-02 -1.00610964e-01\n",
      "     4.96963114e-02 -2.45000347e-02 -1.16729751e-01 -1.49522219e-02\n",
      "    -1.08660674e-02 -6.61451370e-02 -1.10445634e-01  2.13458668e-02\n",
      "     2.12054588e-02  3.49780358e-02  1.72989547e-01  1.75133273e-02\n",
      "     2.72132736e-02  3.51938941e-02 -6.00857250e-02  2.51356163e-03]]\n",
      "\n",
      "  [[-8.20983015e-03 -3.07123065e-01 -4.65271249e-02  8.20704829e-03\n",
      "    -8.99714231e-02 -1.36048079e-01  1.18556388e-01  2.43674163e-02\n",
      "     1.91322137e-02  1.36472449e-01 -7.45865628e-02 -8.33904296e-02\n",
      "    -7.21281096e-02 -2.66504791e-02  2.83854157e-02  2.77020168e-02\n",
      "     6.88332096e-02  8.51658508e-02  2.48735324e-02  4.60208766e-02\n",
      "    -3.91841009e-02  4.40144800e-02  2.05849074e-02 -4.86874878e-02\n",
      "    -4.55755517e-02 -1.33442748e-02 -2.51958966e-02  3.94821353e-03\n",
      "     6.20983820e-03  5.19794002e-02 -1.24078974e-01  3.74970213e-02\n",
      "     6.38532499e-03 -5.14121354e-02 -4.20755222e-02 -3.50166932e-02\n",
      "     5.10803871e-02 -6.50144070e-02  8.63325521e-02  2.85832281e-03\n",
      "    -2.32793782e-02 -7.10128695e-02 -1.85932338e-01 -2.55014189e-02\n",
      "     5.39315343e-02  1.66955274e-02 -3.38270096e-03  6.26540452e-04]]]\n",
      "\n",
      "\n",
      " [[[-5.77393770e-02  9.84482560e-03  1.87141337e-02 -4.37122621e-02\n",
      "    -7.88094848e-02 -1.29983723e-01  1.38337612e-01 -7.96231702e-02\n",
      "     1.80073008e-02 -3.12447906e-01  5.83495758e-03 -7.46757016e-02\n",
      "     4.14842516e-02 -4.73283082e-02  2.00137906e-02 -9.12639946e-02\n",
      "     6.08644169e-03  6.72561154e-02 -8.62643421e-02 -7.23101897e-03\n",
      "     3.87061313e-02  3.01657282e-02 -1.00171007e-01 -2.54116803e-02\n",
      "     8.96178186e-03  6.78905938e-03  3.26898284e-02 -6.51479289e-02\n",
      "    -4.38119173e-02 -6.19389229e-02 -5.76071963e-02 -6.16799705e-02\n",
      "    -5.14349639e-02 -3.38441059e-02 -7.68567398e-02  3.57909240e-02\n",
      "    -9.21406597e-02 -7.50620514e-02  1.22821532e-01 -5.13096452e-02\n",
      "     3.28787565e-02 -1.10689774e-02 -9.43722576e-02 -8.23711157e-02\n",
      "    -1.83748472e-02 -1.06694423e-01  9.18158505e-04 -7.95329884e-02]]\n",
      "\n",
      "  [[-1.74440965e-02  1.01113953e-01 -8.55389144e-03 -1.96251571e-02\n",
      "     1.11079112e-01  1.57198668e-01  7.68748969e-02 -4.39544953e-03\n",
      "     9.86179244e-03  2.11257920e-01 -5.54157607e-02 -7.02622235e-02\n",
      "     4.59962599e-02  5.64041398e-02  1.88948736e-01  7.04086125e-02\n",
      "    -9.76043269e-02  6.23934343e-02 -1.76147930e-02 -5.66723235e-02\n",
      "     1.02776266e-03 -1.01669624e-01 -5.40289935e-03  5.54747693e-03\n",
      "     9.28200502e-03 -3.77133489e-03 -1.78553872e-02 -5.35341818e-03\n",
      "     3.54676843e-02 -1.19508609e-01 -9.40143466e-02  1.67896673e-02\n",
      "    -5.58396839e-02  4.42879573e-02 -7.25568756e-02 -8.54162872e-02\n",
      "     2.25343816e-02  1.15877949e-02 -5.02431020e-02 -3.66509147e-02\n",
      "    -7.10151494e-02  2.83780079e-02  1.48168117e-01 -1.55563634e-02\n",
      "     1.48051474e-02  1.94891617e-02 -2.79324558e-02  4.83995639e-02]]\n",
      "\n",
      "  [[-2.52614208e-02 -5.65465763e-02  1.00147994e-02 -5.34295142e-02\n",
      "    -8.00538361e-02  1.76034108e-01  5.17846234e-02 -1.84412394e-02\n",
      "    -4.30276431e-02  1.03087343e-01 -3.83167565e-02 -6.83526024e-02\n",
      "    -2.79809032e-02  6.34859875e-02  1.22943148e-01  7.39730820e-02\n",
      "     2.47053020e-02 -2.21167341e-01 -1.28571820e-02 -1.75245143e-02\n",
      "     7.71499723e-02  3.67006771e-02  3.67832258e-02  5.93739236e-03\n",
      "     6.17555678e-02 -3.78876925e-03 -1.70497708e-02 -4.43469137e-02\n",
      "    -1.34612259e-03  6.08999170e-02  1.06467605e-01  2.45907865e-02\n",
      "    -5.63997105e-02 -2.56222878e-02 -7.21326023e-02  5.67688160e-02\n",
      "    -9.48527083e-02 -7.54129887e-02 -7.71869123e-02 -2.37819720e-02\n",
      "     1.27895996e-02 -1.83107145e-02  1.94477905e-02 -1.18188141e-03\n",
      "     6.97510410e-03 -2.52107228e-03 -1.06286898e-01 -1.15350425e-01]]\n",
      "\n",
      "  [[-1.32511370e-02  6.22294880e-02  1.77087244e-02 -3.25363912e-02\n",
      "    -8.13018307e-02  1.08742982e-01 -1.55692056e-01 -9.47779268e-02\n",
      "    -9.91739631e-02  4.76240069e-02 -8.74432921e-02 -5.84049411e-02\n",
      "    -4.68532406e-02  5.87779060e-02 -1.99534416e-01 -1.30100409e-02\n",
      "     2.63443496e-02  1.55259311e-01  1.12622175e-02 -6.72922358e-02\n",
      "    -4.93215546e-02  7.33045116e-03  2.94118356e-02  1.35414954e-02\n",
      "    -3.34006771e-02 -3.66780125e-02  1.28498282e-02 -6.12874748e-03\n",
      "    -1.12652980e-01 -1.28706411e-01 -7.09818080e-02 -1.40020680e-02\n",
      "    -8.21455121e-02  5.32732857e-03  7.90501982e-02  7.15835579e-03\n",
      "    -1.22030620e-02 -9.62963551e-02 -7.84587115e-02 -1.50263123e-02\n",
      "    -4.57397364e-02  3.84932943e-02  8.45373720e-02 -1.54745653e-02\n",
      "    -4.82922457e-02  1.67439599e-02 -6.76355660e-02 -3.29925865e-02]]]\n",
      "\n",
      "\n",
      " [[[ 5.77822365e-02  9.94275287e-02  5.81165776e-02 -9.23685730e-03\n",
      "    -1.08813889e-01  1.43392265e-01 -2.07632601e-01 -7.23541006e-02\n",
      "    -2.10082997e-02  1.16582267e-01 -2.80266404e-02  2.83403564e-02\n",
      "     2.05718596e-02 -8.60655308e-02 -3.33034061e-02 -6.80990294e-02\n",
      "    -3.81975388e-03 -1.41383316e-02  1.00065060e-02 -3.17063555e-02\n",
      "     6.56473916e-03  4.87288684e-02 -3.97419557e-02 -1.32216997e-02\n",
      "    -9.17784870e-02  2.28900537e-02  3.97186168e-02  4.88430746e-02\n",
      "    -4.84179296e-02  1.64098516e-01 -1.14037521e-01  3.01652160e-02\n",
      "    -8.67914781e-02 -1.10740401e-02  1.27247348e-01  1.58919394e-02\n",
      "    -7.93808550e-02 -6.66786060e-02 -1.35828238e-02  3.47237997e-02\n",
      "    -4.98442166e-03 -6.29845634e-02  5.77492714e-02 -7.07957447e-02\n",
      "    -3.64465751e-02 -1.59333535e-02  2.95612235e-02  3.38397175e-02]]\n",
      "\n",
      "  [[-6.03526272e-03  1.33655772e-01  5.16057648e-02 -6.36565983e-02\n",
      "     5.55829331e-02  7.16302171e-02  6.33977875e-02  1.12500601e-02\n",
      "     1.73682440e-02  5.94494268e-02  3.75905633e-02 -1.96466558e-02\n",
      "     5.65637741e-03 -6.23966753e-02  1.27227724e-01 -7.59080127e-02\n",
      "    -2.89275823e-03  1.82037994e-01 -6.34010658e-02 -2.17155870e-02\n",
      "    -1.15836347e-02  5.64556755e-02 -1.47567354e-02  2.43120715e-02\n",
      "    -5.15890382e-02  6.67095482e-02 -9.44965333e-02 -8.99492055e-02\n",
      "    -5.89076988e-02  6.14957102e-02  2.81115510e-02 -1.74714066e-02\n",
      "     5.60518280e-02  1.46442866e-02 -9.63102579e-02  4.91296640e-03\n",
      "     2.16529928e-02  5.52174635e-03 -8.41892660e-02 -1.48929805e-02\n",
      "    -2.50465292e-02 -4.79933582e-02 -1.57687113e-01 -3.57331857e-02\n",
      "    -6.17318042e-02 -5.24225608e-02  7.04685412e-03 -8.19279347e-03]]\n",
      "\n",
      "  [[-6.30251691e-02  1.11525729e-01 -3.95000912e-02  6.11063605e-03\n",
      "    -1.21969350e-01  4.98823524e-02  6.94818422e-02  4.27260483e-03\n",
      "     7.02155754e-03  4.10640053e-02  1.93905607e-02  2.93058679e-02\n",
      "     2.79935207e-02 -7.02728108e-02  7.19543323e-02 -8.86719748e-02\n",
      "    -4.56523336e-03  1.12430610e-01 -8.06436501e-03  3.39260437e-02\n",
      "    -8.91760588e-02  1.24274567e-01 -1.90564934e-02 -8.19904655e-02\n",
      "    -3.06265205e-02 -9.63877439e-02  4.28006314e-02 -9.07915607e-02\n",
      "    -9.14140567e-02  1.07785195e-01  6.14300482e-02  2.80508008e-02\n",
      "     1.74854696e-02 -8.49545076e-02 -2.15734169e-02 -4.91417982e-02\n",
      "     5.43275587e-02 -1.03672713e-01 -4.71282080e-02  3.68282460e-02\n",
      "    -5.90593293e-02 -1.13956377e-01  1.45656630e-01  1.00185703e-02\n",
      "    -5.35606779e-02 -7.66057521e-02 -6.65757507e-02 -1.18805155e-01]]\n",
      "\n",
      "  [[-9.94604677e-02  5.74178286e-02 -4.77893092e-02  1.37311192e-02\n",
      "     1.95492636e-02 -2.25334108e-01  1.33391649e-01 -8.86695236e-02\n",
      "     4.68980148e-02  1.18547417e-01  8.03042725e-02  3.73063758e-02\n",
      "    -3.50420699e-02 -7.99586475e-02  2.26463955e-02 -6.90495819e-02\n",
      "    -3.17942090e-02  2.72310968e-03 -5.91320507e-02  3.95267643e-02\n",
      "    -5.73237166e-02  4.20620404e-02 -4.19352204e-03 -8.15406814e-02\n",
      "    -5.22375479e-02  1.53693892e-02  3.36354636e-02  3.72483805e-02\n",
      "     1.84492227e-02 -1.88445784e-02 -9.96677577e-02 -2.79859751e-02\n",
      "     5.15053310e-02 -2.29320433e-02 -5.30095398e-02 -1.78345479e-02\n",
      "    -6.64154487e-03 -4.38938960e-02 -1.67494814e-03 -9.06189978e-02\n",
      "    -5.52403294e-02 -1.10943206e-01 -2.22204365e-02 -1.98806971e-02\n",
      "    -9.20732319e-02 -2.00910913e-03  1.94963142e-02  1.03710620e-02]]]]\n"
     ]
    }
   ],
   "source": [
    "print(array(first_conv_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.51478916 0.         0.         0.         0.48996627\n",
      " 0.25058284 0.         0.         0.3336922  0.         0.\n",
      " 0.         0.         0.19565895 0.         0.         0.35314748\n",
      " 0.         0.         0.         0.49628997 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.61808795 0.         0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# first_conv_output = first_conv_model.predict(X5)\n",
    "print(first_conv_output[0][0][0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.08156943\n",
      "tf.Tensor(-0.014813579, shape=(), dtype=float32)\n",
      "[array(-0.08156943, dtype=float32), array(-0.01481358, dtype=float32), array(-0.02392906, dtype=float32), array(-0.00820983, dtype=float32)]\n",
      "[array(-0.05773938, dtype=float32), array(-0.0174441, dtype=float32), array(-0.02526142, dtype=float32), array(-0.01325114, dtype=float32)]\n",
      "[-0.08156943321228027, -0.014813578687608242, -0.023929063230752945, -0.008209830150008202, -0.05773937702178955, -0.017444096505641937, -0.025261420756578445, -0.013251136988401413, 0.057782236486673355, -0.00603526271879673, -0.06302516907453537, -0.09946046769618988]\n",
      "[0.010632856748998165, 0.1590496301651001, 0.1674755960702896, -0.307123064994812, 0.009844825603067875, 0.1011139526963234, -0.05654657632112503, 0.06222948804497719, 0.09942752867937088, 0.13365577161312103, 0.11152572929859161, 0.05741782858967781]\n",
      "[[-0.08156943321228027, -0.014813578687608242, -0.023929063230752945, -0.008209830150008202, -0.05773937702178955, -0.017444096505641937, -0.025261420756578445, -0.013251136988401413, 0.057782236486673355, -0.00603526271879673, -0.06302516907453537, -0.09946046769618988], [0.010632856748998165, 0.1590496301651001, 0.1674755960702896, -0.307123064994812, 0.009844825603067875, 0.1011139526963234, -0.05654657632112503, 0.06222948804497719, 0.09942752867937088, 0.13365577161312103, 0.11152572929859161, 0.05741782858967781], [-0.04222650080919266, -0.014871242456138134, -0.01973748579621315, -0.04652712494134903, 0.01871413365006447, -0.008553891442716122, 0.010014799423515797, 0.01770872436463833, 0.05811657756567001, 0.05160576477646828, -0.03950009122490883, -0.0477893091738224], [-0.00987932737916708, 0.06454682350158691, -0.06768473237752914, 0.008207048289477825, -0.04371226206421852, -0.019625157117843628, -0.05342951416969299, -0.03253639116883278, -0.009236857295036316, -0.06365659832954407, 0.006110636051744223, 0.013731119222939014], [-0.08394875377416611, 0.10993190109729767, -0.06583698093891144, -0.08997142314910889, -0.07880948483943939, 0.11107911169528961, -0.08005383610725403, -0.08130183070898056, -0.10881388932466507, 0.055582933127880096, -0.12196934968233109, 0.019549263641238213], [0.04304276034235954, 0.07774746417999268, 0.09818463027477264, -0.1360480785369873, -0.12998372316360474, 0.15719866752624512, 0.1760341078042984, 0.10874298214912415, 0.14339226484298706, 0.07163021713495255, 0.049882352352142334, -0.22533410787582397], [-0.16782842576503754, 0.06187868118286133, 0.06948775053024292, 0.11855638772249222, 0.1383376121520996, 0.07687489688396454, 0.05178462341427803, -0.15569205582141876, -0.20763260126113892, 0.06339778751134872, 0.06948184221982956, 0.13339164853096008], [0.029645122587680817, 0.04313109442591667, 0.015512519516050816, 0.02436741627752781, -0.07962317019701004, -0.004395449534058571, -0.01844123937189579, -0.09477792680263519, -0.07235410064458847, 0.011250060051679611, 0.004272604826837778, -0.08866952359676361], [-0.05205211788415909, -0.02366885356605053, -0.09737291187047958, 0.019132213667035103, 0.018007300794124603, 0.00986179243773222, -0.043027643114328384, -0.09917396306991577, -0.02100829966366291, 0.017368244007229805, 0.007021557539701462, 0.0468980148434639], [0.03258039802312851, 0.08447597175836563, 0.12149505317211151, 0.1364724487066269, -0.3124479055404663, 0.2112579196691513, 0.10308734327554703, 0.04762400686740875, 0.11658226698637009, 0.05944942682981491, 0.041064005345106125, 0.11854741722345352], [-0.07317439466714859, -0.09514352679252625, -0.09796994179487228, -0.07458656281232834, 0.005834957584738731, -0.05541576072573662, -0.0383167564868927, -0.0874432921409607, -0.02802664041519165, 0.03759056329727173, 0.019390560686588287, 0.08030427247285843], [-0.05799105390906334, -0.004155548755079508, 0.05091887712478638, -0.08339042961597443, -0.07467570155858994, -0.07026222348213196, -0.0683526024222374, -0.058404941111803055, 0.028340356424450874, -0.01964665576815605, 0.029305867850780487, 0.03730637580156326], [0.03580430522561073, -0.06418285518884659, 0.02346407249569893, -0.07212810963392258, 0.041484251618385315, 0.04599625989794731, -0.027980903163552284, -0.046853240579366684, 0.020571859553456306, 0.005656377412378788, 0.027993520721793175, -0.03504206985235214], [-0.07677975296974182, 0.007388730999082327, 0.01432042010128498, -0.026650479063391685, -0.04732830822467804, 0.05640413984656334, 0.06348598748445511, 0.058777906000614166, -0.08606553077697754, -0.06239667534828186, -0.07027281075716019, -0.07995864748954773], [-0.09078384935855865, 0.11340852826833725, 0.17460738122463226, 0.02838541567325592, 0.02001379057765007, 0.18894873559474945, 0.12294314801692963, -0.19953441619873047, -0.033303406089544296, 0.12722772359848022, 0.07195433229207993, 0.02264639548957348], [-0.0702487975358963, 0.03728320449590683, -0.05080553516745567, 0.02770201675593853, -0.09126399457454681, 0.07040861248970032, 0.07397308200597763, -0.013010040856897831, -0.06809902936220169, -0.07590801268815994, -0.08867197483778, -0.06904958188533783], [0.06507685780525208, 0.03616952523589134, 0.09444721788167953, 0.06883320957422256, 0.006086441688239574, -0.09760432690382004, 0.024705301970243454, 0.02634434960782528, -0.0038197538815438747, -0.0028927582316100597, -0.004565233364701271, -0.03179420903325081], [0.1585673689842224, -0.22805413603782654, 0.10739553719758987, 0.0851658508181572, 0.06725611537694931, 0.0623934343457222, -0.22116734087467194, 0.15525931119918823, -0.014138331636786461, 0.18203799426555634, 0.11243060976266861, 0.0027231096755713224], [0.004293262027204037, 0.03248409181833267, 0.021007029339671135, 0.024873532354831696, -0.08626434206962585, -0.01761479303240776, -0.012857181951403618, 0.011262217536568642, 0.010006505995988846, -0.06340106576681137, -0.00806436501443386, -0.05913205072283745], [-0.036871325224637985, 0.00017282844055444002, 0.014762829057872295, 0.04602087661623955, -0.0072310189716517925, -0.05667232349514961, -0.017524514347314835, -0.0672922357916832, -0.03170635551214218, -0.021715587005019188, 0.033926043659448624, 0.039526764303445816], [-0.0329190157353878, 0.022068949416279793, -0.007619496434926987, -0.039184100925922394, 0.038706131279468536, 0.001027762657031417, 0.07714997231960297, -0.049321554601192474, 0.0065647391602396965, -0.011583634652197361, -0.08917605876922607, -0.05732371658086777], [-0.2402256429195404, 0.22796526551246643, 0.15631434321403503, 0.04401447996497154, 0.03016572818160057, -0.10166962444782257, 0.03670067712664604, 0.007330451160669327, 0.04872886836528778, 0.05645567551255226, 0.12427456676959991, 0.04206204041838646], [0.02684103697538376, 0.03883855417370796, 0.021119840443134308, 0.02058490738272667, -0.10017100721597672, -0.005402899347245693, 0.03678322583436966, 0.029411835595965385, -0.03974195569753647, -0.014756735414266586, -0.019056493416428566, -0.004193522036075592], [-0.030106432735919952, 0.006770246662199497, -0.0746833011507988, -0.048687487840652466, -0.025411680340766907, 0.0055474769324064255, 0.005937392357736826, 0.01354149542748928, -0.013221699744462967, 0.02431207150220871, -0.08199046552181244, -0.08154068142175674], [-0.07293777167797089, -0.07591583579778671, 0.02321098931133747, -0.04557555168867111, 0.008961781859397888, 0.00928200501948595, 0.06175556778907776, -0.033400677144527435, -0.09177848696708679, -0.05158903822302818, -0.03062652051448822, -0.05223754793405533], [-0.020999286323785782, -0.04873204976320267, -0.07824905216693878, -0.013344274833798409, 0.00678905937820673, -0.0037713348865509033, -0.003788769245147705, -0.0366780124604702, 0.022890053689479828, 0.06670954823493958, -0.09638774394989014, 0.015369389206171036], [-0.02228865772485733, -0.03632710129022598, -0.0342598482966423, -0.0251958966255188, 0.03268982842564583, -0.017855387181043625, -0.017049770802259445, 0.012849828228354454, 0.039718616753816605, -0.09449653327465057, 0.04280063137412071, 0.033635463565588], [-0.02204890549182892, -0.058933891355991364, -0.015653572976589203, 0.003948213532567024, -0.06514792889356613, -0.005353418178856373, -0.04434691369533539, -0.00612874748185277, 0.04884307458996773, -0.08994920551776886, -0.09079156070947647, 0.03724838048219681], [-0.024569889530539513, -0.06915096938610077, 0.012282581068575382, 0.006209838204085827, -0.043811917304992676, 0.035467684268951416, -0.001346122589893639, -0.1126529797911644, -0.04841792955994606, -0.05890769883990288, -0.09141405671834946, 0.018449222669005394], [-0.03059879131615162, 0.13751061260700226, 0.046494558453559875, 0.05197940021753311, -0.061938922852277756, -0.11950860917568207, 0.06089991703629494, -0.1287064105272293, 0.16409851610660553, 0.06149571016430855, 0.1077851951122284, -0.01884457841515541], [0.020085107535123825, -0.12271878123283386, 0.05020102113485336, -0.12407897412776947, -0.057607196271419525, -0.09401434659957886, 0.106467604637146, -0.07098180800676346, -0.11403752118349075, 0.028111550956964493, 0.06143004819750786, -0.0996677577495575], [-0.005591200198978186, 0.0485927052795887, -0.10061096400022507, 0.03749702125787735, -0.06167997047305107, 0.016789667308330536, 0.024590786546468735, -0.014002067968249321, 0.030165215954184532, -0.01747140660881996, 0.028050800785422325, -0.02798597514629364], [0.06011389195919037, -0.08803202211856842, 0.049696311354637146, 0.006385324988514185, -0.0514349639415741, -0.05583968386054039, -0.05639971047639847, -0.08214551210403442, -0.08679147809743881, 0.0560518279671669, 0.017485469579696655, 0.051505330950021744], [-0.04436890780925751, -0.015882035717368126, -0.024500034749507904, -0.05141213536262512, -0.033844105899333954, 0.04428795725107193, -0.025622287765145302, 0.005327328573912382, -0.011074040085077286, 0.014644286595284939, -0.08495450764894485, -0.02293204329907894], [-0.06316787749528885, -0.06202694773674011, -0.1167297512292862, -0.04207552224397659, -0.07685673981904984, -0.07255687564611435, -0.07213260233402252, 0.07905019819736481, 0.12724734842777252, -0.09631025791168213, -0.021573416888713837, -0.05300953984260559], [-0.045435480773448944, 0.0016753931995481253, -0.014952221885323524, -0.03501669317483902, 0.03579092398285866, -0.0854162871837616, 0.05676881596446037, 0.007158355787396431, 0.015891939401626587, 0.004912966396659613, -0.04914179816842079, -0.01783454790711403], [-0.013269903138279915, 0.012498116120696068, -0.010866067372262478, 0.05108038708567619, -0.09214065968990326, 0.022534381598234177, -0.09485270828008652, -0.012203061953186989, -0.07938085496425629, 0.02165299281477928, 0.05432755872607231, -0.006641544867306948], [-0.08745574206113815, 0.0268540121614933, -0.06614513695240021, -0.06501440703868866, -0.07506205141544342, 0.011587794870138168, -0.07541298866271973, -0.09629635512828827, -0.06667860597372055, 0.005521746352314949, -0.10367271304130554, -0.04389389604330063], [-0.10943710803985596, -0.09938782453536987, -0.11044563353061676, 0.08633255213499069, 0.12282153218984604, -0.05024310201406479, -0.07718691229820251, -0.07845871150493622, -0.013582823798060417, -0.0841892659664154, -0.047128207981586456, -0.001674948143772781], [0.0021012790966778994, -0.022909153252840042, 0.021345866844058037, 0.002858322812244296, -0.05130964517593384, -0.03665091469883919, -0.023781972005963326, -0.015026312321424484, 0.03472379967570305, -0.014892980456352234, 0.03682824596762657, -0.09061899781227112], [-0.09163719415664673, -0.00017979886615648866, 0.021205458790063858, -0.023279378190636635, 0.032878756523132324, -0.07101514935493469, 0.012789599597454071, -0.045739736407995224, -0.004984421655535698, -0.02504652924835682, -0.05905932933092117, -0.05524032935500145], [-0.02675684541463852, 0.0665758028626442, 0.03497803583741188, -0.07101286947727203, -0.011068977415561676, 0.02837800793349743, -0.01831071451306343, 0.03849329426884651, -0.06298456341028214, -0.047993358224630356, -0.11395637691020966, -0.11094320565462112], [0.027420084923505783, 0.10672914236783981, 0.17298954725265503, -0.18593233823776245, -0.09437225759029388, 0.14816811680793762, 0.01944779045879841, 0.08453737199306488, 0.057749271392822266, -0.15768711268901825, 0.14565663039684296, -0.022220436483621597], [-0.010359449312090874, -0.078677237033844, 0.017513327300548553, -0.025501418858766556, -0.08237111568450928, -0.015556363388895988, -0.001181881409138441, -0.015474565327167511, -0.07079574465751648, -0.0357331857085228, 0.010018570348620415, -0.01988069713115692], [0.020066551864147186, -0.0209747813642025, 0.02721327356994152, 0.05393153429031372, -0.018374847248196602, 0.014805147424340248, 0.006975104101002216, -0.04829224571585655, -0.036446575075387955, -0.06173180416226387, -0.05356067791581154, -0.0920732319355011], [-0.07626556605100632, 0.009289025329053402, 0.03519389405846596, 0.016695527359843254, -0.10669442266225815, 0.019489161670207977, -0.002521072281524539, 0.016743959859013557, -0.015933353453874588, -0.05242256075143814, -0.07660575211048126, -0.002009109128266573], [-0.0010836338624358177, -0.05640916898846626, -0.0600857250392437, -0.0033827009610831738, 0.0009181585046462715, -0.027932455763220787, -0.1062868982553482, -0.06763556599617004, 0.029561223462224007, 0.007046854123473167, -0.06657575070858002, 0.019496314227581024], [-0.017775174230337143, -0.04979098215699196, 0.0025135616306215525, 0.0006265404517762363, -0.07953298836946487, 0.04839956387877464, -0.11535042524337769, -0.032992586493492126, 0.033839717507362366, -0.00819279346615076, -0.11880515515804291, 0.010371061973273754]]\n"
     ]
    }
   ],
   "source": [
    "print(array(model.layers[1].weights[0][0][0][0][0]))\n",
    "print(model.layers[1].weights[0][0][1][0][0])\n",
    "\n",
    "print([array(weights[0][0]) for weights in model.layers[1].weights[0][0]])\n",
    "print([array(weights[0][0]) for weights in model.layers[1].weights[0][1]])\n",
    "\n",
    "\n",
    "print([array(weights[0][0]).item() for subweights in model.layers[1].weights[0] for weights in subweights])\n",
    "print([array(weights[0][1]).item() for subweights in model.layers[1].weights[0] for weights in subweights])\n",
    "\n",
    "\n",
    "print([[array(weights[0][i]).item()for subweights in model.layers[1].weights[0] for weights in subweights] for i in range(48)])\n",
    "\n",
    "\n",
    "\n",
    "# print(pd.DataFrame(model.layers[1].weights.reshape(3,4,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.05773938  0.00984483  0.01871413 -0.04371226 -0.07880948\n",
      "   -0.12998372  0.13833761 -0.07962317  0.0180073  -0.3124479\n",
      "    0.00583496 -0.0746757   0.04148425 -0.04732831  0.02001379\n",
      "   -0.09126399  0.00608644  0.06725612 -0.08626434 -0.00723102\n",
      "    0.03870613  0.03016573 -0.10017101 -0.02541168  0.00896178\n",
      "    0.00678906  0.03268983 -0.06514793 -0.04381192 -0.06193892\n",
      "   -0.0576072  -0.06167997 -0.05143496 -0.03384411 -0.07685674\n",
      "    0.03579092 -0.09214066 -0.07506205  0.12282153 -0.05130965\n",
      "    0.03287876 -0.01106898 -0.09437226 -0.08237112 -0.01837485\n",
      "   -0.10669442  0.00091816 -0.07953299]]\n",
      "\n",
      " [[-0.0174441   0.10111395 -0.00855389 -0.01962516  0.11107911\n",
      "    0.15719867  0.0768749  -0.00439545  0.00986179  0.21125792\n",
      "   -0.05541576 -0.07026222  0.04599626  0.05640414  0.18894874\n",
      "    0.07040861 -0.09760433  0.06239343 -0.01761479 -0.05667232\n",
      "    0.00102776 -0.10166962 -0.0054029   0.00554748  0.00928201\n",
      "   -0.00377133 -0.01785539 -0.00535342  0.03546768 -0.11950861\n",
      "   -0.09401435  0.01678967 -0.05583968  0.04428796 -0.07255688\n",
      "   -0.08541629  0.02253438  0.01158779 -0.0502431  -0.03665091\n",
      "   -0.07101515  0.02837801  0.14816812 -0.01555636  0.01480515\n",
      "    0.01948916 -0.02793246  0.04839956]]\n",
      "\n",
      " [[-0.02526142 -0.05654658  0.0100148  -0.05342951 -0.08005384\n",
      "    0.17603411  0.05178462 -0.01844124 -0.04302764  0.10308734\n",
      "   -0.03831676 -0.0683526  -0.0279809   0.06348599  0.12294315\n",
      "    0.07397308  0.0247053  -0.22116734 -0.01285718 -0.01752451\n",
      "    0.07714997  0.03670068  0.03678323  0.00593739  0.06175557\n",
      "   -0.00378877 -0.01704977 -0.04434691 -0.00134612  0.06089992\n",
      "    0.1064676   0.02459079 -0.05639971 -0.02562229 -0.0721326\n",
      "    0.05676882 -0.09485271 -0.07541299 -0.07718691 -0.02378197\n",
      "    0.0127896  -0.01831071  0.01944779 -0.00118188  0.0069751\n",
      "   -0.00252107 -0.1062869  -0.11535043]]\n",
      "\n",
      " [[-0.01325114  0.06222949  0.01770872 -0.03253639 -0.08130183\n",
      "    0.10874298 -0.15569206 -0.09477793 -0.09917396  0.04762401\n",
      "   -0.08744329 -0.05840494 -0.04685324  0.05877791 -0.19953442\n",
      "   -0.01301004  0.02634435  0.15525931  0.01126222 -0.06729224\n",
      "   -0.04932155  0.00733045  0.02941184  0.0135415  -0.03340068\n",
      "   -0.03667801  0.01284983 -0.00612875 -0.11265298 -0.12870641\n",
      "   -0.07098181 -0.01400207 -0.08214551  0.00532733  0.0790502\n",
      "    0.00715836 -0.01220306 -0.09629636 -0.07845871 -0.01502631\n",
      "   -0.04573974  0.03849329  0.08453737 -0.01547457 -0.04829225\n",
      "    0.01674396 -0.06763557 -0.03299259]]], shape=(4, 1, 48), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[1].weights[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(X5[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_conv_weights = model.layers[1].weights[0]\n",
    "first_conv_biases = model.layers[1].weights[1]\n",
    "# Format: [Position 0: A, C, G, T, Position 1: A, C, G, T, Position 2: A, C, G, T]\n",
    "pd.DataFrame(array(first_conv_weights).transpose((3,2,0,1)).reshape(first_conv_weights.shape[-1], -1)).to_csv(save_path + model_name+\"_tiling_first_conv_kernels\")\n",
    "pd.DataFrame(first_conv_biases).to_csv(save_path + model_name+\"_tiling_first_conv_biases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2cd633bf9703d9b8d2b7bb6e04b82983774c32d5f891ed1890ee26b779f7466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
