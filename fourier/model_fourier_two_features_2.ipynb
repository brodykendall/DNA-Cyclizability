{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/Users/Brody1/Documents/Northwestern/Jiping/benchmarks/deep-learning/\"\n",
    "model_name = \"fourier_two_features_2\"\n",
    "kf = KFold(n_splits = 10, shuffle =True)\n",
    "num_epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### define functions ####\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, out_dim):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_fits(fits):\n",
    "    print(f\"Average correlation on tiling: {np.mean([fits[0][i] for i in range(fits[0].size) if (i % 4) == 2])}\",\n",
    "          f\"\\nAverage MSE on tiling: {np.mean([fits[1][i] for i in range(fits[1].size) if (i % 4) == 2])}\",\n",
    "          f\"\\nAverage correlation on random: {np.mean([fits[0][i] for i in range(fits[0].size) if (i % 4) == 1])}\",\n",
    "          f\"\\nAverage MSE on random: {np.mean([fits[1][i] for i in range(fits[1].size) if (i % 4) == 1])}\",\n",
    "          f\"\\nAverage correlation on ChrV: {np.mean([fits[0][i] for i in range(fits[0].size) if (i % 4) == 3])}\",\n",
    "          f\"\\nAverage MSE on ChrV: {np.mean([fits[1][i] for i in range(fits[1].size) if (i % 4) == 3])}\",\n",
    "          f\"\\nAverage correlation on CN: {np.mean([fits[0][i] for i in range(fits[0].size) if (i % 4) == 0])}\",\n",
    "          f\"\\nAverage MSE on CN: {np.mean([fits[1][i] for i in range(fits[1].size) if (i % 4) == 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct sine and cosine matrices for Fourier features:\n",
    "cos_matrix, sin_matrix = [[] for x in range(49)], [[] for x in range(49)]\n",
    "for n in range(49):\n",
    "    for k in range(49):\n",
    "        cos_matrix[n].append(np.cos(2*np.pi*(n+1)*k/49))\n",
    "        sin_matrix[n].append(np.sin(2*np.pi*(n+1)*k/49))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnaTwoFourier(sequence):\n",
    "    seq_array = array(list(sequence))\n",
    "    code = {\"AA\": [1], \"AT\": [1], \"TA\": [1], \"TT\": [1],\n",
    "            \"CA\": [0], \"CC\": [0], \"CG\": [0], \"CT\": [0],\n",
    "            \"GA\": [0], \"GC\": [0], \"GG\": [0], \"GT\": [0],\n",
    "            \"AC\": [0], \"TC\": [0], \"TG\": [0], \"AG\": [0],\n",
    "            \"aa\": [1], \"at\": [1], \"ta\": [1], \"tt\": [1],\n",
    "            \"ca\": [0], \"cc\": [0], \"cg\": [0], \"ct\": [0],\n",
    "            \"ga\": [0], \"gc\": [0], \"gg\": [0], \"gt\": [0],\n",
    "            \"ac\": [0], \"tc\": [0], \"tg\": [0], \"ag\": [0],}\n",
    "    AT_encoded_seq = []\n",
    "    for i in range(len(seq_array)-1):\n",
    "        AT_encoded_seq.append(code[\"\".join((seq_array[i], seq_array[i+1]))][0])\n",
    "    two_fourier_cos = np.matmul(array(AT_encoded_seq), array(cos_matrix).transpose())\n",
    "    two_fourier_sin = np.matmul(array(AT_encoded_seq), array(sin_matrix).transpose())\n",
    "    return list(np.concatenate((two_fourier_cos, two_fourier_sin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_c0new(dat):\n",
    "  mat = np.empty((3,3), float)\n",
    "  k = 2*np.pi/10.4\n",
    "  n = array([26, 29, 31])\n",
    "  mat[0:3,0] = 1\n",
    "  mat[0:3, 1] = np.sin(n*k)\n",
    "  mat[0:3, 2] = np.cos(n*k)\n",
    "  inv_mat = np.linalg.inv(mat)\n",
    "  c0A1A2 = array(np.matmul(dat[[\"n=26\", \"n=29\", \"n=31\"]], inv_mat))\n",
    "  c0Aphi = c0A1A2\n",
    "  c0Aphi[:,0] = c0A1A2[:,0]\n",
    "  c0Aphi[:,1] = np.sqrt(c0A1A2[:,1]**2 + c0A1A2[:,2]**2)\n",
    "  c0Aphi[:,2] <- np.sign(c0A1A2[:,2]) * np.arccos(c0A1A2[:,1]/c0Aphi[:,1])\n",
    "  return c0Aphi[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### preparing data ####\n",
    "\n",
    "data_cerevisiae_nucle = pd.read_csv(\"/Users/Brody1/Documents/Northwestern/Jiping/cycle1.txt\",delimiter = \",\")\n",
    "X1_all = []\n",
    "for sequence_nt in data_cerevisiae_nucle[\"Sequence\"]:\n",
    "    X1_all.append(dnaTwoFourier(sequence_nt))\n",
    "X1_all = np.float32(array(X1))\n",
    "# X1 = X1.reshape((X1.shape[0],50,4,1))\n",
    "### COMPUTE EIGENVECTORS:\n",
    "\n",
    "# X1_reverse = np.flip(X1,[1,2])\n",
    "# Y1 = data_cerevisiae_nucle[\"C0\"].values.astype(float)\n",
    "Y1 = np.float32(find_c0new(data_cerevisiae_nucle).astype(float))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_random_library = pd.read_csv(\"/Users/Brody1/Documents/Northwestern/Jiping/cycle3.txt\",delimiter = \",\")\n",
    "X3_all = []\n",
    "for sequence_nt in data_random_library[\"Sequence\"]:\n",
    "    X3_all.append(dnaTwoFourier(sequence_nt))\n",
    "X3_all = np.float32(array(X3_all))\n",
    "# X3 = X3.reshape((X3.shape[0],50,2,1))\n",
    "# X3_reverse = np.flip(X3,[1,2])\n",
    "# Y3 = data_random_library[\"C0\"].values.astype(float)\n",
    "Y3 = np.float32(find_c0new(data_random_library).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tiling = pd.read_csv(\"/Users/Brody1/Documents/Northwestern/Jiping/cycle5.txt\",delimiter = \",\")\n",
    "X5_all = []\n",
    "for sequence_nt in data_tiling[\"Sequence\"]:\n",
    "    X5_all.append(dnaTwoFourier(sequence_nt))\n",
    "X5_all = np.float32(array(X5_all))\n",
    "# X5 = X5.reshape((X5.shape[0],50,4,1))\n",
    "# X5_reverse = np.flip(X5,[1,2])\n",
    "# Y5 = data_tiling[\"C0\"].values.astype(float)\n",
    "Y5 = np.float32(find_c0new(data_tiling).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chr5 = pd.read_csv(\"/Users/Brody1/Documents/Northwestern/Jiping/cycle6.txt\",delimiter = \",\")\n",
    "X6_all = []\n",
    "for sequence_nt in data_chr5[\"Sequence\"]:\n",
    "    X6_all.append(dnaTwoFourier(sequence_nt))\n",
    "X6_all = np.float32(array(X6_all))\n",
    "# X6 = X6.reshape((X6.shape[0],50,4,1))\n",
    "# X6_reverse = np.flip(X6,[1,2])\n",
    "# Y6 = data_chr5[\"C0\"].values.astype(float)\n",
    "Y6 = np.float32(find_c0new(data_chr5).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_feature = 4\n",
    "X1 = X1_all[:,(which_feature, which_feature + 49)]\n",
    "X3 = X3_all[:,(which_feature, which_feature + 49)]\n",
    "X5 = X5_all[:,(which_feature, which_feature + 49)]\n",
    "X6 = X6_all[:,(which_feature, which_feature + 49)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/60], Loss: 0.4561\n",
      "Epoch [10/60], Loss: 0.1309\n",
      "Epoch [15/60], Loss: 0.1181\n",
      "Epoch [20/60], Loss: 0.1168\n",
      "Epoch [25/60], Loss: 0.1163\n",
      "Epoch [30/60], Loss: 0.1160\n",
      "Epoch [35/60], Loss: 0.1157\n",
      "Epoch [40/60], Loss: 0.1155\n",
      "Epoch [45/60], Loss: 0.1153\n",
      "Epoch [50/60], Loss: 0.1152\n",
      "Epoch [55/60], Loss: 0.1151\n",
      "Epoch [60/60], Loss: 0.1152\n",
      "Epoch [5/60], Loss: 0.1246\n",
      "Epoch [10/60], Loss: 0.1204\n",
      "Epoch [15/60], Loss: 0.1184\n",
      "Epoch [20/60], Loss: 0.1171\n",
      "Epoch [25/60], Loss: 0.1163\n",
      "Epoch [30/60], Loss: 0.1157\n",
      "Epoch [35/60], Loss: 0.1154\n",
      "Epoch [40/60], Loss: 0.1151\n",
      "Epoch [45/60], Loss: 0.1150\n",
      "Epoch [50/60], Loss: 0.1148\n",
      "Epoch [55/60], Loss: 0.1148\n",
      "Epoch [60/60], Loss: 0.1147\n",
      "Epoch [5/60], Loss: 0.1283\n",
      "Epoch [10/60], Loss: 0.1228\n",
      "Epoch [15/60], Loss: 0.1209\n",
      "Epoch [20/60], Loss: 0.1197\n",
      "Epoch [25/60], Loss: 0.1189\n",
      "Epoch [30/60], Loss: 0.1182\n",
      "Epoch [35/60], Loss: 0.1177\n",
      "Epoch [40/60], Loss: 0.1173\n",
      "Epoch [45/60], Loss: 0.1170\n",
      "Epoch [50/60], Loss: 0.1167\n",
      "Epoch [55/60], Loss: 0.1164\n",
      "Epoch [60/60], Loss: 0.1162\n",
      "Epoch [5/60], Loss: 0.7802\n",
      "Epoch [10/60], Loss: 0.1237\n",
      "Epoch [15/60], Loss: 0.1201\n",
      "Epoch [20/60], Loss: 0.1191\n",
      "Epoch [25/60], Loss: 0.1183\n",
      "Epoch [30/60], Loss: 0.1177\n",
      "Epoch [35/60], Loss: 0.1172\n",
      "Epoch [40/60], Loss: 0.1167\n",
      "Epoch [45/60], Loss: 0.1163\n",
      "Epoch [50/60], Loss: 0.1160\n",
      "Epoch [55/60], Loss: 0.1157\n",
      "Epoch [60/60], Loss: 0.1155\n",
      "Epoch [5/60], Loss: 0.1810\n",
      "Epoch [10/60], Loss: 0.1277\n",
      "Epoch [15/60], Loss: 0.1219\n",
      "Epoch [20/60], Loss: 0.1207\n",
      "Epoch [25/60], Loss: 0.1200\n",
      "Epoch [30/60], Loss: 0.1194\n",
      "Epoch [35/60], Loss: 0.1189\n",
      "Epoch [40/60], Loss: 0.1185\n",
      "Epoch [45/60], Loss: 0.1181\n",
      "Epoch [50/60], Loss: 0.1178\n",
      "Epoch [55/60], Loss: 0.1175\n",
      "Epoch [60/60], Loss: 0.1172\n",
      "Epoch [5/60], Loss: 0.2503\n",
      "Epoch [10/60], Loss: 0.1527\n",
      "Epoch [15/60], Loss: 0.1361\n",
      "Epoch [20/60], Loss: 0.1302\n",
      "Epoch [25/60], Loss: 0.1267\n",
      "Epoch [30/60], Loss: 0.1241\n",
      "Epoch [35/60], Loss: 0.1223\n",
      "Epoch [40/60], Loss: 0.1209\n",
      "Epoch [45/60], Loss: 0.1199\n",
      "Epoch [50/60], Loss: 0.1192\n",
      "Epoch [55/60], Loss: 0.1186\n",
      "Epoch [60/60], Loss: 0.1181\n",
      "Epoch [5/60], Loss: 0.1227\n",
      "Epoch [10/60], Loss: 0.1208\n",
      "Epoch [15/60], Loss: 0.1196\n",
      "Epoch [20/60], Loss: 0.1187\n",
      "Epoch [25/60], Loss: 0.1179\n",
      "Epoch [30/60], Loss: 0.1173\n",
      "Epoch [35/60], Loss: 0.1169\n",
      "Epoch [40/60], Loss: 0.1165\n",
      "Epoch [45/60], Loss: 0.1162\n",
      "Epoch [50/60], Loss: 0.1160\n",
      "Epoch [55/60], Loss: 0.1158\n",
      "Epoch [60/60], Loss: 0.1156\n",
      "Epoch [5/60], Loss: 0.2034\n",
      "Epoch [10/60], Loss: 0.1335\n",
      "Epoch [15/60], Loss: 0.1211\n",
      "Epoch [20/60], Loss: 0.1186\n",
      "Epoch [25/60], Loss: 0.1179\n",
      "Epoch [30/60], Loss: 0.1175\n",
      "Epoch [35/60], Loss: 0.1172\n",
      "Epoch [40/60], Loss: 0.1169\n",
      "Epoch [45/60], Loss: 0.1166\n",
      "Epoch [50/60], Loss: 0.1163\n",
      "Epoch [55/60], Loss: 0.1161\n",
      "Epoch [60/60], Loss: 0.1158\n",
      "Epoch [5/60], Loss: 0.6245\n",
      "Epoch [10/60], Loss: 0.1232\n",
      "Epoch [15/60], Loss: 0.1200\n",
      "Epoch [20/60], Loss: 0.1186\n",
      "Epoch [25/60], Loss: 0.1177\n",
      "Epoch [30/60], Loss: 0.1171\n",
      "Epoch [35/60], Loss: 0.1167\n",
      "Epoch [40/60], Loss: 0.1163\n",
      "Epoch [45/60], Loss: 0.1161\n",
      "Epoch [50/60], Loss: 0.1159\n",
      "Epoch [55/60], Loss: 0.1157\n",
      "Epoch [60/60], Loss: 0.1155\n",
      "Epoch [5/60], Loss: 0.1896\n",
      "Epoch [10/60], Loss: 0.1198\n",
      "Epoch [15/60], Loss: 0.1161\n",
      "Epoch [20/60], Loss: 0.1157\n",
      "Epoch [25/60], Loss: 0.1156\n",
      "Epoch [30/60], Loss: 0.1155\n",
      "Epoch [35/60], Loss: 0.1154\n",
      "Epoch [40/60], Loss: 0.1154\n",
      "Epoch [45/60], Loss: 0.1153\n",
      "Epoch [50/60], Loss: 0.1153\n",
      "Epoch [55/60], Loss: 0.1153\n",
      "Epoch [60/60], Loss: 0.1155\n"
     ]
    }
   ],
   "source": [
    "#### tiling\n",
    "\n",
    "VALIDATION_LOSS = []\n",
    "fold_var = 1\n",
    "n = Y5.shape[0]\n",
    "hidden_size = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "fits = []\n",
    "detrend = []\n",
    "times = []\n",
    "\n",
    "for train_index, val_index in kf.split(Y5):\n",
    "    training_X = np.float32(X5[train_index])\n",
    "    validation_X = np.float32(X5[val_index])\n",
    "    training_Y = np.float32(Y5[train_index])\n",
    "    validation_Y = np.float32(Y5[val_index])\n",
    "\n",
    "    # CREATE NEW MODEL\n",
    "    model = NeuralNet(input_size=2, hidden_size=hidden_size, out_dim=1)\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    # CREATE CALLBACKS\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs = torch.from_numpy(training_X)\n",
    "        inputs = inputs.reshape(inputs.shape[0], 2)\n",
    "        targets = torch.from_numpy(training_Y)\n",
    "        targets = targets.reshape(targets.shape[0], 1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n",
    "    \n",
    "    pred_Y = model(torch.from_numpy(training_X)).detach().numpy()\n",
    "    pred_Y = pred_Y.reshape(pred_Y.shape[0])\n",
    "\n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X1)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Y1)[0,1],np.mean(np.square(fit-Y1)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X3)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Y3)[0,1],np.mean(np.square(fit-Y3)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(validation_X)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, validation_Y)[0,1],np.mean(np.square(fit-validation_Y)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X6)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Y6)[0,1],np.mean(np.square(fit-Y6)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    fold_var += 1\n",
    "\n",
    "fits = array(fits)\n",
    "fits = pd.DataFrame((fits))\n",
    "fits.to_csv(save_path +model_name+\"_fits_tiling.txt\", index = False)\n",
    "\n",
    "with open(save_path +model_name+\"_time_tiling.txt\", \"w\") as file:\n",
    "    for row in times:\n",
    "        s = \" \".join(map(str, row))\n",
    "        file.write(s+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average correlation on tiling: 0.5255317544707367 \n",
      "Average MSE on tiling: 0.11599666774272918 \n",
      "Average correlation on random: 0.48641894369688166 \n",
      "Average MSE on random: 0.09559891000390053 \n",
      "Average correlation on ChrV: 0.41171093958057936 \n",
      "Average MSE on ChrV: 0.2137158066034317 \n",
      "Average correlation on CN: 0.5200955398007451 \n",
      "Average MSE on CN: 0.15908850878477096\n"
     ]
    }
   ],
   "source": [
    "display_fits(fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2cd633bf9703d9b8d2b7bb6e04b82983774c32d5f891ed1890ee26b779f7466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
