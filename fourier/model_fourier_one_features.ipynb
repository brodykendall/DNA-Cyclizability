{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/Users/Brody1/Documents/Northwestern/Jiping/benchmarks/deep-learning/\"\n",
    "model_name = \"fourier_one_features\"\n",
    "kf = KFold(n_splits = 10, shuffle =True)\n",
    "num_epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### define functions ####\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, out_dim):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_fits(fits):\n",
    "    print(f\"Average correlation on tiling: {np.mean([fits[0][i] for i in range(fits[0].size) if (i % 4) == 2])}\",\n",
    "          f\"\\nAverage MSE on tiling: {np.mean([fits[1][i] for i in range(fits[1].size) if (i % 4) == 2])}\",\n",
    "          f\"\\nAverage correlation on random: {np.mean([fits[0][i] for i in range(fits[0].size) if (i % 4) == 1])}\",\n",
    "          f\"\\nAverage MSE on random: {np.mean([fits[1][i] for i in range(fits[1].size) if (i % 4) == 1])}\",\n",
    "          f\"\\nAverage correlation on ChrV: {np.mean([fits[0][i] for i in range(fits[0].size) if (i % 4) == 3])}\",\n",
    "          f\"\\nAverage MSE on ChrV: {np.mean([fits[1][i] for i in range(fits[1].size) if (i % 4) == 3])}\",\n",
    "          f\"\\nAverage correlation on CN: {np.mean([fits[0][i] for i in range(fits[0].size) if (i % 4) == 0])}\",\n",
    "          f\"\\nAverage MSE on CN: {np.mean([fits[1][i] for i in range(fits[1].size) if (i % 4) == 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct sine and cosine matrices for Fourier features:\n",
    "cos_matrix, sin_matrix = [[] for x in range(50)], [[] for x in range(50)]\n",
    "for n in range(50):\n",
    "    for k in range(50):\n",
    "        cos_matrix[n].append(np.cos(2*np.pi*(n+1)*k/50))\n",
    "        sin_matrix[n].append(np.sin(2*np.pi*(n+1)*k/50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnaOneFourier(sequence):\n",
    "    seq_array = array(list(sequence))\n",
    "    code = {\"A\": [1], \"C\": [0], \"G\": [0], \"T\": [1],\n",
    "            \"a\": [1], \"c\": [0], \"g\": [0], \"t\": [1]}\n",
    "    AT_encoded_seq = []\n",
    "    for char in seq_array:\n",
    "        AT_encoded_seq.append(code[char][0])\n",
    "    one_fourier_cos = np.matmul(array(AT_encoded_seq), array(cos_matrix).transpose())\n",
    "    one_fourier_sin = np.matmul(array(AT_encoded_seq), array(sin_matrix).transpose())\n",
    "    return list(np.concatenate((one_fourier_cos, one_fourier_sin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_c0new(dat):\n",
    "  mat = np.empty((3,3), float)\n",
    "  k = 2*np.pi/10.4\n",
    "  n = array([26, 29, 31])\n",
    "  mat[0:3,0] = 1\n",
    "  mat[0:3, 1] = np.sin(n*k)\n",
    "  mat[0:3, 2] = np.cos(n*k)\n",
    "  inv_mat = np.linalg.inv(mat)\n",
    "  c0A1A2 = array(np.matmul(dat[[\"n=26\", \"n=29\", \"n=31\"]], inv_mat))\n",
    "  c0Aphi = c0A1A2\n",
    "  c0Aphi[:,0] = c0A1A2[:,0]\n",
    "  c0Aphi[:,1] = np.sqrt(c0A1A2[:,1]**2 + c0A1A2[:,2]**2)\n",
    "  c0Aphi[:,2] <- np.sign(c0A1A2[:,2]) * np.arccos(c0A1A2[:,1]/c0Aphi[:,1])\n",
    "  return c0Aphi[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### preparing data ####\n",
    "\n",
    "data_cerevisiae_nucle = pd.read_csv(\"/Users/Brody1/Documents/Northwestern/Jiping/cycle1.txt\",delimiter = \",\")\n",
    "X1 = []\n",
    "for sequence_nt in data_cerevisiae_nucle[\"Sequence\"]:\n",
    "    X1.append(dnaOneFourier(sequence_nt))\n",
    "X1 = np.float32(array(X1))\n",
    "# X1 = X1.reshape((X1.shape[0],50,4,1))\n",
    "### COMPUTE EIGENVECTORS:\n",
    "\n",
    "# X1_reverse = np.flip(X1,[1,2])\n",
    "# Y1 = data_cerevisiae_nucle[\"C0\"].values.astype(float)\n",
    "Y1 = np.float32(find_c0new(data_cerevisiae_nucle).astype(float))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_random_library = pd.read_csv(\"/Users/Brody1/Documents/Northwestern/Jiping/cycle3.txt\",delimiter = \",\")\n",
    "X3 = []\n",
    "for sequence_nt in data_random_library[\"Sequence\"]:\n",
    "    X3.append(dnaOneFourier(sequence_nt))\n",
    "X3 = np.float32(array(X3))\n",
    "# X3 = X3.reshape((X3.shape[0],50,2,1))\n",
    "# X3_reverse = np.flip(X3,[1,2])\n",
    "# Y3 = data_random_library[\"C0\"].values.astype(float)\n",
    "Y3 = np.float32(find_c0new(data_random_library).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tiling = pd.read_csv(\"/Users/Brody1/Documents/Northwestern/Jiping/cycle5.txt\",delimiter = \",\")\n",
    "X5 = []\n",
    "for sequence_nt in data_tiling[\"Sequence\"]:\n",
    "    X5.append(dnaOneFourier(sequence_nt))\n",
    "X5 = np.float32(array(X5))\n",
    "# X5 = X5.reshape((X5.shape[0],50,4,1))\n",
    "# X5_reverse = np.flip(X5,[1,2])\n",
    "# Y5 = data_tiling[\"C0\"].values.astype(float)\n",
    "Y5 = np.float32(find_c0new(data_tiling).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chr5 = pd.read_csv(\"/Users/Brody1/Documents/Northwestern/Jiping/cycle6.txt\",delimiter = \",\")\n",
    "X6 = []\n",
    "for sequence_nt in data_chr5[\"Sequence\"]:\n",
    "    X6.append(dnaOneFourier(sequence_nt))\n",
    "X6 = np.float32(array(X6))\n",
    "# X6 = X6.reshape((X6.shape[0],50,4,1))\n",
    "# X6_reverse = np.flip(X6,[1,2])\n",
    "# Y6 = data_chr5[\"C0\"].values.astype(float)\n",
    "Y6 = np.float32(find_c0new(data_chr5).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = np.mean(Y1)\n",
    "std1 = np.std(Y1)\n",
    "Z1 = (Y1-m1)/std1\n",
    "\n",
    "m3 = np.mean(Y3)\n",
    "std3 = np.std(Y3)\n",
    "Z3 = (Y3-m3)/std3\n",
    "\n",
    "\n",
    "m5 = np.mean(Y5)\n",
    "std5 = np.std(Y5)\n",
    "Z5 = (Y5-m5)/std5\n",
    "\n",
    "\n",
    "m6 = np.mean(Y6)\n",
    "std6 = np.std(Y6)\n",
    "Z6 = (Y6-m6)/std6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/60], Loss: 1.1452\n",
      "Epoch [10/60], Loss: 0.8494\n",
      "Epoch [15/60], Loss: 0.7942\n",
      "Epoch [20/60], Loss: 0.7587\n",
      "Epoch [25/60], Loss: 0.7342\n",
      "Epoch [30/60], Loss: 0.7169\n",
      "Epoch [35/60], Loss: 0.7042\n",
      "Epoch [40/60], Loss: 0.6942\n",
      "Epoch [45/60], Loss: 0.6864\n",
      "Epoch [50/60], Loss: 0.6798\n",
      "Epoch [55/60], Loss: 0.6743\n",
      "Epoch [60/60], Loss: 0.6695\n",
      "Epoch [5/60], Loss: 0.9258\n",
      "Epoch [10/60], Loss: 0.7520\n",
      "Epoch [15/60], Loss: 0.7097\n",
      "Epoch [20/60], Loss: 0.6840\n",
      "Epoch [25/60], Loss: 0.6657\n",
      "Epoch [30/60], Loss: 0.6516\n",
      "Epoch [35/60], Loss: 0.6409\n",
      "Epoch [40/60], Loss: 0.6322\n",
      "Epoch [45/60], Loss: 0.6253\n",
      "Epoch [50/60], Loss: 0.6201\n",
      "Epoch [55/60], Loss: 0.6178\n",
      "Epoch [60/60], Loss: 0.6205\n",
      "Epoch [5/60], Loss: 1.1536\n",
      "Epoch [10/60], Loss: 0.7891\n",
      "Epoch [15/60], Loss: 0.7513\n",
      "Epoch [20/60], Loss: 0.7251\n",
      "Epoch [25/60], Loss: 0.7052\n",
      "Epoch [30/60], Loss: 0.6896\n",
      "Epoch [35/60], Loss: 0.6767\n",
      "Epoch [40/60], Loss: 0.6654\n",
      "Epoch [45/60], Loss: 0.6556\n",
      "Epoch [50/60], Loss: 0.6470\n",
      "Epoch [55/60], Loss: 0.6394\n",
      "Epoch [60/60], Loss: 0.6327\n",
      "Epoch [5/60], Loss: 1.6671\n",
      "Epoch [10/60], Loss: 0.8937\n",
      "Epoch [15/60], Loss: 0.8101\n",
      "Epoch [20/60], Loss: 0.7676\n",
      "Epoch [25/60], Loss: 0.7395\n",
      "Epoch [30/60], Loss: 0.7195\n",
      "Epoch [35/60], Loss: 0.7044\n",
      "Epoch [40/60], Loss: 0.6924\n",
      "Epoch [45/60], Loss: 0.6825\n",
      "Epoch [50/60], Loss: 0.6742\n",
      "Epoch [55/60], Loss: 0.6671\n",
      "Epoch [60/60], Loss: 0.6610\n",
      "Epoch [5/60], Loss: 1.1035\n",
      "Epoch [10/60], Loss: 0.7780\n",
      "Epoch [15/60], Loss: 0.7346\n",
      "Epoch [20/60], Loss: 0.7066\n",
      "Epoch [25/60], Loss: 0.6856\n",
      "Epoch [30/60], Loss: 0.6693\n",
      "Epoch [35/60], Loss: 0.6565\n",
      "Epoch [40/60], Loss: 0.6456\n",
      "Epoch [45/60], Loss: 0.6361\n",
      "Epoch [50/60], Loss: 0.6279\n",
      "Epoch [55/60], Loss: 0.6209\n",
      "Epoch [60/60], Loss: 0.6153\n",
      "Epoch [5/60], Loss: 2.0777\n",
      "Epoch [10/60], Loss: 0.8158\n",
      "Epoch [15/60], Loss: 0.7643\n",
      "Epoch [20/60], Loss: 0.7358\n",
      "Epoch [25/60], Loss: 0.7156\n",
      "Epoch [30/60], Loss: 0.7011\n",
      "Epoch [35/60], Loss: 0.6901\n",
      "Epoch [40/60], Loss: 0.6814\n",
      "Epoch [45/60], Loss: 0.6742\n",
      "Epoch [50/60], Loss: 0.6680\n",
      "Epoch [55/60], Loss: 0.6625\n",
      "Epoch [60/60], Loss: 0.6575\n",
      "Epoch [5/60], Loss: 1.0331\n",
      "Epoch [10/60], Loss: 0.7826\n",
      "Epoch [15/60], Loss: 0.7444\n",
      "Epoch [20/60], Loss: 0.7205\n",
      "Epoch [25/60], Loss: 0.7023\n",
      "Epoch [30/60], Loss: 0.6887\n",
      "Epoch [35/60], Loss: 0.6726\n",
      "Epoch [40/60], Loss: 0.6623\n",
      "Epoch [45/60], Loss: 0.6487\n",
      "Epoch [50/60], Loss: 0.6398\n",
      "Epoch [55/60], Loss: 0.6299\n",
      "Epoch [60/60], Loss: 0.6227\n",
      "Epoch [5/60], Loss: 1.1265\n",
      "Epoch [10/60], Loss: 0.7748\n",
      "Epoch [15/60], Loss: 0.7414\n",
      "Epoch [20/60], Loss: 0.7216\n",
      "Epoch [25/60], Loss: 0.7070\n",
      "Epoch [30/60], Loss: 0.6955\n",
      "Epoch [35/60], Loss: 0.6857\n",
      "Epoch [40/60], Loss: 0.6771\n",
      "Epoch [45/60], Loss: 0.6695\n",
      "Epoch [50/60], Loss: 0.6627\n",
      "Epoch [55/60], Loss: 0.6564\n",
      "Epoch [60/60], Loss: 0.6504\n",
      "Epoch [5/60], Loss: 1.4989\n",
      "Epoch [10/60], Loss: 0.9080\n",
      "Epoch [15/60], Loss: 0.8578\n",
      "Epoch [20/60], Loss: 0.8190\n",
      "Epoch [25/60], Loss: 0.7887\n",
      "Epoch [30/60], Loss: 0.7655\n",
      "Epoch [35/60], Loss: 0.7473\n",
      "Epoch [40/60], Loss: 0.7325\n",
      "Epoch [45/60], Loss: 0.7201\n",
      "Epoch [50/60], Loss: 0.7095\n",
      "Epoch [55/60], Loss: 0.7003\n",
      "Epoch [60/60], Loss: 0.6921\n",
      "Epoch [5/60], Loss: 1.4914\n",
      "Epoch [10/60], Loss: 0.7838\n",
      "Epoch [15/60], Loss: 0.7247\n",
      "Epoch [20/60], Loss: 0.7000\n",
      "Epoch [25/60], Loss: 0.6828\n",
      "Epoch [30/60], Loss: 0.6699\n",
      "Epoch [35/60], Loss: 0.6594\n",
      "Epoch [40/60], Loss: 0.6505\n",
      "Epoch [45/60], Loss: 0.6430\n",
      "Epoch [50/60], Loss: 0.6368\n",
      "Epoch [55/60], Loss: 0.6313\n",
      "Epoch [60/60], Loss: 0.6265\n"
     ]
    }
   ],
   "source": [
    "#### random\n",
    "\n",
    "VALIDATION_LOSS = []\n",
    "fold_var = 1\n",
    "n = Z3.shape[0]\n",
    "hidden_size = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "fits = []\n",
    "detrend = []\n",
    "times = []\n",
    "\n",
    "for train_index, val_index in kf.split(Z3):\n",
    "    training_X = np.float32(X3[train_index])\n",
    "    validation_X = np.float32(X3[val_index])\n",
    "    training_Y = np.float32(Z3[train_index])\n",
    "    validation_Y = np.float32(Z3[val_index])\n",
    "\n",
    "    # CREATE NEW MODEL\n",
    "    model = NeuralNet(input_size=100, hidden_size=hidden_size, out_dim=1)\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    # CREATE CALLBACKS\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs = torch.from_numpy(training_X)\n",
    "        inputs = inputs.reshape(inputs.shape[0], 100)\n",
    "        targets = torch.from_numpy(training_Y)\n",
    "        targets = targets.reshape(targets.shape[0], 1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n",
    "    \n",
    "    pred_Y = model(torch.from_numpy(training_X)).detach().numpy()\n",
    "    pred_Y = pred_Y.reshape(pred_Y.shape[0])\n",
    "\n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X1)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Z1)[0,1],np.mean(np.square(fit-Z1)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(validation_X)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, validation_Y)[0,1],np.mean(np.square(fit-validation_Y)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X5)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Z5)[0,1],np.mean(np.square(fit-Z5)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X6)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Z6)[0,1],np.mean(np.square(fit-Z6)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    fold_var += 1\n",
    "\n",
    "fits = array(fits)\n",
    "fits = pd.DataFrame((fits))\n",
    "fits.to_csv(save_path +model_name+\"_fits_random.txt\", index = False)\n",
    "\n",
    "with open(save_path +model_name+\"_time_random.txt\", \"w\") as file:\n",
    "    for row in times:\n",
    "        s = \" \".join(map(str, row))\n",
    "        file.write(s+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2         3\n",
      "0   0.594261  0.724914  0.278317  0.618749\n",
      "1   0.554297  0.725422  0.025980  0.624743\n",
      "2   0.578904  0.665827 -0.004070  0.609573\n",
      "3   0.461610  0.808932 -0.003879  0.609934\n",
      "4   0.590966  0.725120  0.259714  0.674090\n",
      "5   0.517871  0.749966  0.091781  0.638338\n",
      "6   0.585328  0.659687 -0.021348  0.628224\n",
      "7   0.471871  0.801541 -0.024770  0.625461\n",
      "8   0.575896  0.860093  0.436456  0.611327\n",
      "9   0.560209  0.703112  0.011554  0.602277\n",
      "10  0.557572  0.721733  0.174249  0.605079\n",
      "11  0.442919  0.860434  0.176573  0.602396\n",
      "12  0.592064  0.740562  0.301027  0.614080\n",
      "13  0.567323  0.689604  0.019057  0.580961\n",
      "14  0.580643  0.663704  0.027739  0.589605\n",
      "15  0.459491  0.806438  0.029691  0.588673\n",
      "16  0.597280  0.822444  0.421268  0.638758\n",
      "17  0.544669  0.701882 -0.018285  0.613975\n",
      "18  0.578550  0.683828  0.130496  0.617530\n",
      "19  0.460828  0.829230  0.134022  0.614549\n",
      "20  0.592502  0.755624  0.326201  0.609083\n",
      "21  0.573251  0.664405  0.005712  0.594345\n",
      "22  0.582272  0.664669  0.060669  0.587727\n",
      "23  0.464041  0.803601  0.061672  0.587052\n",
      "24  0.582663  0.769647  0.329381  0.608192\n",
      "25  0.541231  0.735674 -0.014892  0.599554\n",
      "26  0.571359  0.677676  0.060731  0.592313\n",
      "27  0.456391  0.813813  0.061269  0.591862\n",
      "28  0.595340  0.750901  0.324183  0.579979\n",
      "29  0.567801  0.687679  0.011950  0.584302\n",
      "30  0.581338  0.665351  0.057150  0.575103\n",
      "31  0.464182  0.800005  0.058575  0.573904\n",
      "32  0.559193  0.840268  0.384895  0.628626\n",
      "33  0.511836  0.726990  0.004984  0.626922\n",
      "34  0.544853  0.724858  0.113574  0.638786\n",
      "35  0.430372  0.870452  0.115248  0.636259\n",
      "36  0.594518  0.822381  0.418504  0.620713\n",
      "37  0.555461  0.654551  0.027074  0.618100\n",
      "38  0.580392  0.685684  0.149866  0.589259\n",
      "39  0.459289  0.828368  0.151580  0.587107\n"
     ]
    }
   ],
   "source": [
    "print(fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/60], Loss: 1.8938\n",
      "Epoch [10/60], Loss: 0.9418\n",
      "Epoch [15/60], Loss: 0.8712\n",
      "Epoch [20/60], Loss: 0.8402\n",
      "Epoch [25/60], Loss: 0.8197\n",
      "Epoch [30/60], Loss: 0.8055\n",
      "Epoch [35/60], Loss: 0.7956\n",
      "Epoch [40/60], Loss: 0.7886\n",
      "Epoch [45/60], Loss: 0.7834\n",
      "Epoch [50/60], Loss: 0.7797\n",
      "Epoch [55/60], Loss: 0.7768\n",
      "Epoch [60/60], Loss: 0.7744\n",
      "Epoch [5/60], Loss: 1.3737\n",
      "Epoch [10/60], Loss: 0.9851\n",
      "Epoch [15/60], Loss: 0.9201\n",
      "Epoch [20/60], Loss: 0.8821\n",
      "Epoch [25/60], Loss: 0.8578\n",
      "Epoch [30/60], Loss: 0.8414\n",
      "Epoch [35/60], Loss: 0.8298\n",
      "Epoch [40/60], Loss: 0.8212\n",
      "Epoch [45/60], Loss: 0.8144\n",
      "Epoch [50/60], Loss: 0.8089\n",
      "Epoch [55/60], Loss: 0.8044\n",
      "Epoch [60/60], Loss: 0.8006\n",
      "Epoch [5/60], Loss: 1.6197\n",
      "Epoch [10/60], Loss: 0.9571\n",
      "Epoch [15/60], Loss: 0.8897\n",
      "Epoch [20/60], Loss: 0.8555\n",
      "Epoch [25/60], Loss: 0.8345\n",
      "Epoch [30/60], Loss: 0.8200\n",
      "Epoch [35/60], Loss: 0.8095\n",
      "Epoch [40/60], Loss: 0.8016\n",
      "Epoch [45/60], Loss: 0.7955\n",
      "Epoch [50/60], Loss: 0.7905\n",
      "Epoch [55/60], Loss: 0.7861\n",
      "Epoch [60/60], Loss: 0.7821\n",
      "Epoch [5/60], Loss: 1.0563\n",
      "Epoch [10/60], Loss: 0.8355\n",
      "Epoch [15/60], Loss: 0.8106\n",
      "Epoch [20/60], Loss: 0.7965\n",
      "Epoch [25/60], Loss: 0.7859\n",
      "Epoch [30/60], Loss: 0.7769\n",
      "Epoch [35/60], Loss: 0.7692\n",
      "Epoch [40/60], Loss: 0.7627\n",
      "Epoch [45/60], Loss: 0.7569\n",
      "Epoch [50/60], Loss: 0.7521\n",
      "Epoch [55/60], Loss: 0.7482\n",
      "Epoch [60/60], Loss: 0.7656\n",
      "Epoch [5/60], Loss: 1.2229\n",
      "Epoch [10/60], Loss: 0.8717\n",
      "Epoch [15/60], Loss: 0.8505\n",
      "Epoch [20/60], Loss: 0.8354\n",
      "Epoch [25/60], Loss: 0.8238\n",
      "Epoch [30/60], Loss: 0.8148\n",
      "Epoch [35/60], Loss: 0.8077\n",
      "Epoch [40/60], Loss: 0.8018\n",
      "Epoch [45/60], Loss: 0.7969\n",
      "Epoch [50/60], Loss: 0.7927\n",
      "Epoch [55/60], Loss: 0.7890\n",
      "Epoch [60/60], Loss: 0.7857\n",
      "Epoch [5/60], Loss: 1.5483\n",
      "Epoch [10/60], Loss: 0.8675\n",
      "Epoch [15/60], Loss: 0.8265\n",
      "Epoch [20/60], Loss: 0.8084\n",
      "Epoch [25/60], Loss: 0.7978\n",
      "Epoch [30/60], Loss: 0.7905\n",
      "Epoch [35/60], Loss: 0.7850\n",
      "Epoch [40/60], Loss: 0.7807\n",
      "Epoch [45/60], Loss: 0.7770\n",
      "Epoch [50/60], Loss: 0.7738\n",
      "Epoch [55/60], Loss: 0.7710\n",
      "Epoch [60/60], Loss: 0.7684\n",
      "Epoch [5/60], Loss: 1.1585\n",
      "Epoch [10/60], Loss: 0.9398\n",
      "Epoch [15/60], Loss: 0.8803\n",
      "Epoch [20/60], Loss: 0.8488\n",
      "Epoch [25/60], Loss: 0.8290\n",
      "Epoch [30/60], Loss: 0.8155\n",
      "Epoch [35/60], Loss: 0.8058\n",
      "Epoch [40/60], Loss: 0.7985\n",
      "Epoch [45/60], Loss: 0.7928\n",
      "Epoch [50/60], Loss: 0.7881\n",
      "Epoch [55/60], Loss: 0.7841\n",
      "Epoch [60/60], Loss: 0.7807\n",
      "Epoch [5/60], Loss: 1.0770\n",
      "Epoch [10/60], Loss: 0.8852\n",
      "Epoch [15/60], Loss: 0.8522\n",
      "Epoch [20/60], Loss: 0.8348\n",
      "Epoch [25/60], Loss: 0.8223\n",
      "Epoch [30/60], Loss: 0.8120\n",
      "Epoch [35/60], Loss: 0.8031\n",
      "Epoch [40/60], Loss: 0.7952\n",
      "Epoch [45/60], Loss: 0.7882\n",
      "Epoch [50/60], Loss: 0.7820\n",
      "Epoch [55/60], Loss: 0.7764\n",
      "Epoch [60/60], Loss: 0.7714\n",
      "Epoch [5/60], Loss: 1.3635\n",
      "Epoch [10/60], Loss: 0.9511\n",
      "Epoch [15/60], Loss: 0.9094\n",
      "Epoch [20/60], Loss: 0.8834\n",
      "Epoch [25/60], Loss: 0.8637\n",
      "Epoch [30/60], Loss: 0.8479\n",
      "Epoch [35/60], Loss: 0.8353\n",
      "Epoch [40/60], Loss: 0.8249\n",
      "Epoch [45/60], Loss: 0.8164\n",
      "Epoch [50/60], Loss: 0.8092\n",
      "Epoch [55/60], Loss: 0.8031\n",
      "Epoch [60/60], Loss: 0.7978\n",
      "Epoch [5/60], Loss: 1.8207\n",
      "Epoch [10/60], Loss: 0.8981\n",
      "Epoch [15/60], Loss: 0.8421\n",
      "Epoch [20/60], Loss: 0.8120\n",
      "Epoch [25/60], Loss: 0.7940\n",
      "Epoch [30/60], Loss: 0.7828\n",
      "Epoch [35/60], Loss: 0.7755\n",
      "Epoch [40/60], Loss: 0.7702\n",
      "Epoch [45/60], Loss: 0.7659\n",
      "Epoch [50/60], Loss: 0.7622\n",
      "Epoch [55/60], Loss: 0.7587\n",
      "Epoch [60/60], Loss: 0.7551\n"
     ]
    }
   ],
   "source": [
    "#### chrv\n",
    "\n",
    "VALIDATION_LOSS = []\n",
    "fold_var = 1\n",
    "n = Z6.shape[0]\n",
    "hidden_size = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "fits = []\n",
    "detrend = []\n",
    "times = []\n",
    "\n",
    "for train_index, val_index in kf.split(Z6):\n",
    "    training_X = np.float32(X6[train_index])\n",
    "    validation_X = np.float32(X6[val_index])\n",
    "    training_Y = np.float32(Z6[train_index])\n",
    "    validation_Y = np.float32(Z6[val_index])\n",
    "\n",
    "    # CREATE NEW MODEL\n",
    "    model = NeuralNet(input_size=100, hidden_size=hidden_size, out_dim=1)\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    # CREATE CALLBACKS\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs = torch.from_numpy(training_X)\n",
    "        inputs = inputs.reshape(inputs.shape[0], 100)\n",
    "        targets = torch.from_numpy(training_Y)\n",
    "        targets = targets.reshape(targets.shape[0], 1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n",
    "    \n",
    "    pred_Y = model(torch.from_numpy(training_X)).detach().numpy()\n",
    "    pred_Y = pred_Y.reshape(pred_Y.shape[0])\n",
    "\n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X1)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Z1)[0,1],np.mean(np.square(fit-Z1)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X3)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Z3)[0,1],np.mean(np.square(fit-Z3)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X5)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Z5)[0,1],np.mean(np.square(fit-Z5)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(validation_X)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, validation_Y)[0,1],np.mean(np.square(fit-validation_Y)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    fold_var += 1\n",
    "\n",
    "fits = array(fits)\n",
    "fits = pd.DataFrame((fits))\n",
    "fits.to_csv(save_path +model_name+\"_fits_chrv.txt\", index = False)\n",
    "\n",
    "with open(save_path +model_name+\"_time_chrv.txt\", \"w\") as file:\n",
    "    for row in times:\n",
    "        s = \" \".join(map(str, row))\n",
    "        file.write(s+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/60], Loss: 2.3442\n",
      "Epoch [10/60], Loss: 0.9713\n",
      "Epoch [15/60], Loss: 0.8410\n",
      "Epoch [20/60], Loss: 0.7805\n",
      "Epoch [25/60], Loss: 0.7432\n",
      "Epoch [30/60], Loss: 0.7181\n",
      "Epoch [35/60], Loss: 0.6999\n",
      "Epoch [40/60], Loss: 0.6863\n",
      "Epoch [45/60], Loss: 0.6754\n",
      "Epoch [50/60], Loss: 0.6665\n",
      "Epoch [55/60], Loss: 0.6588\n",
      "Epoch [60/60], Loss: 0.6519\n",
      "Epoch [5/60], Loss: 1.3257\n",
      "Epoch [10/60], Loss: 0.7826\n",
      "Epoch [15/60], Loss: 0.7196\n",
      "Epoch [20/60], Loss: 0.6998\n",
      "Epoch [25/60], Loss: 0.6885\n",
      "Epoch [30/60], Loss: 0.6798\n",
      "Epoch [35/60], Loss: 0.6723\n",
      "Epoch [40/60], Loss: 0.6656\n",
      "Epoch [45/60], Loss: 0.6594\n",
      "Epoch [50/60], Loss: 0.6536\n",
      "Epoch [55/60], Loss: 0.6505\n",
      "Epoch [60/60], Loss: 0.6701\n",
      "Epoch [5/60], Loss: 1.4790\n",
      "Epoch [10/60], Loss: 0.9680\n",
      "Epoch [15/60], Loss: 0.8651\n",
      "Epoch [20/60], Loss: 0.8109\n",
      "Epoch [25/60], Loss: 0.7749\n",
      "Epoch [30/60], Loss: 0.7486\n",
      "Epoch [35/60], Loss: 0.7282\n",
      "Epoch [40/60], Loss: 0.7120\n",
      "Epoch [45/60], Loss: 0.6985\n",
      "Epoch [50/60], Loss: 0.6877\n",
      "Epoch [55/60], Loss: 0.6781\n",
      "Epoch [60/60], Loss: 0.6697\n",
      "Epoch [5/60], Loss: 1.9539\n",
      "Epoch [10/60], Loss: 0.8508\n",
      "Epoch [15/60], Loss: 0.7704\n",
      "Epoch [20/60], Loss: 0.7258\n",
      "Epoch [25/60], Loss: 0.6967\n",
      "Epoch [30/60], Loss: 0.6757\n",
      "Epoch [35/60], Loss: 0.6599\n",
      "Epoch [40/60], Loss: 0.6476\n",
      "Epoch [45/60], Loss: 0.6382\n",
      "Epoch [50/60], Loss: 0.6306\n",
      "Epoch [55/60], Loss: 0.6267\n",
      "Epoch [60/60], Loss: 0.6905\n",
      "Epoch [5/60], Loss: 23.5584\n",
      "Epoch [10/60], Loss: 1.0089\n",
      "Epoch [15/60], Loss: 0.9128\n",
      "Epoch [20/60], Loss: 0.8678\n",
      "Epoch [25/60], Loss: 0.8355\n",
      "Epoch [30/60], Loss: 0.8096\n",
      "Epoch [35/60], Loss: 0.7880\n",
      "Epoch [40/60], Loss: 0.7696\n",
      "Epoch [45/60], Loss: 0.7533\n",
      "Epoch [50/60], Loss: 0.7387\n",
      "Epoch [55/60], Loss: 0.7256\n",
      "Epoch [60/60], Loss: 0.7137\n",
      "Epoch [5/60], Loss: 2.3420\n",
      "Epoch [10/60], Loss: 0.8301\n",
      "Epoch [15/60], Loss: 0.7614\n",
      "Epoch [20/60], Loss: 0.7240\n",
      "Epoch [25/60], Loss: 0.6992\n",
      "Epoch [30/60], Loss: 0.6817\n",
      "Epoch [35/60], Loss: 0.6689\n",
      "Epoch [40/60], Loss: 0.6588\n",
      "Epoch [45/60], Loss: 0.6526\n",
      "Epoch [50/60], Loss: 0.6699\n",
      "Epoch [55/60], Loss: 0.6834\n",
      "Epoch [60/60], Loss: 0.6545\n",
      "Epoch [5/60], Loss: 1.5229\n",
      "Epoch [10/60], Loss: 0.8391\n",
      "Epoch [15/60], Loss: 0.7396\n",
      "Epoch [20/60], Loss: 0.6932\n",
      "Epoch [25/60], Loss: 0.6654\n",
      "Epoch [30/60], Loss: 0.6467\n",
      "Epoch [35/60], Loss: 0.6341\n",
      "Epoch [40/60], Loss: 0.6250\n",
      "Epoch [45/60], Loss: 0.6180\n",
      "Epoch [50/60], Loss: 0.6126\n",
      "Epoch [55/60], Loss: 0.6081\n",
      "Epoch [60/60], Loss: 0.6044\n",
      "Epoch [5/60], Loss: 1.1868\n",
      "Epoch [10/60], Loss: 0.8441\n",
      "Epoch [15/60], Loss: 0.7736\n",
      "Epoch [20/60], Loss: 0.7463\n",
      "Epoch [25/60], Loss: 0.7283\n",
      "Epoch [30/60], Loss: 0.7153\n",
      "Epoch [35/60], Loss: 0.7121\n",
      "Epoch [40/60], Loss: 0.7437\n",
      "Epoch [45/60], Loss: 0.7114\n",
      "Epoch [50/60], Loss: 0.6894\n",
      "Epoch [55/60], Loss: 0.6823\n",
      "Epoch [60/60], Loss: 0.6765\n",
      "Epoch [5/60], Loss: 1.9656\n",
      "Epoch [10/60], Loss: 1.0272\n",
      "Epoch [15/60], Loss: 0.8911\n",
      "Epoch [20/60], Loss: 0.8205\n",
      "Epoch [25/60], Loss: 0.7716\n",
      "Epoch [30/60], Loss: 0.7373\n",
      "Epoch [35/60], Loss: 0.7141\n",
      "Epoch [40/60], Loss: 0.6978\n",
      "Epoch [45/60], Loss: 0.6858\n",
      "Epoch [50/60], Loss: 0.6762\n",
      "Epoch [55/60], Loss: 0.6683\n",
      "Epoch [60/60], Loss: 0.6619\n",
      "Epoch [5/60], Loss: 1.9552\n",
      "Epoch [10/60], Loss: 0.8903\n",
      "Epoch [15/60], Loss: 0.7713\n",
      "Epoch [20/60], Loss: 0.7340\n",
      "Epoch [25/60], Loss: 0.7159\n",
      "Epoch [30/60], Loss: 0.7091\n",
      "Epoch [35/60], Loss: 0.7085\n",
      "Epoch [40/60], Loss: 0.7059\n",
      "Epoch [45/60], Loss: 0.6862\n",
      "Epoch [50/60], Loss: 0.6763\n",
      "Epoch [55/60], Loss: 0.6678\n",
      "Epoch [60/60], Loss: 0.6637\n"
     ]
    }
   ],
   "source": [
    "#### CN\n",
    "\n",
    "VALIDATION_LOSS = []\n",
    "fold_var = 1\n",
    "n = Z1.shape[0]\n",
    "hidden_size = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "fits = []\n",
    "detrend = []\n",
    "times = []\n",
    "\n",
    "for train_index, val_index in kf.split(Z1):\n",
    "    training_X = np.float32(X1[train_index])\n",
    "    validation_X = np.float32(X1[val_index])\n",
    "    training_Y = np.float32(Z1[train_index])\n",
    "    validation_Y = np.float32(Z1[val_index])\n",
    "\n",
    "    # CREATE NEW MODEL\n",
    "    model = NeuralNet(input_size=100, hidden_size=hidden_size, out_dim=1)\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    # CREATE CALLBACKS\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs = torch.from_numpy(training_X)\n",
    "        inputs = inputs.reshape(inputs.shape[0], 100)\n",
    "        targets = torch.from_numpy(training_Y)\n",
    "        targets = targets.reshape(targets.shape[0], 1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n",
    "    \n",
    "    pred_Y = model(torch.from_numpy(training_X)).detach().numpy()\n",
    "    pred_Y = pred_Y.reshape(pred_Y.shape[0])\n",
    "\n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(validation_X)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, validation_Y)[0,1],np.mean(np.square(fit-validation_Y)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X3)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Z3)[0,1],np.mean(np.square(fit-Z3)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X5)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Z5)[0,1],np.mean(np.square(fit-Z5)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X6)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Z6)[0,1],np.mean(np.square(fit-Z6)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    fold_var += 1\n",
    "\n",
    "fits = array(fits)\n",
    "fits = pd.DataFrame((fits))\n",
    "fits.to_csv(save_path +model_name+\"_fits_CN.txt\", index = False)\n",
    "\n",
    "with open(save_path +model_name+\"_time_CN.txt\", \"w\") as file:\n",
    "    for row in times:\n",
    "        s = \" \".join(map(str, row))\n",
    "        file.write(s+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2         3\n",
      "0   0.580772  0.659283 -0.002047  0.631271\n",
      "1   0.499352  0.857777 -0.249705  0.710957\n",
      "2   0.548732  0.813266 -0.299729  0.705372\n",
      "3   0.432755  0.977771 -0.303229  0.703125\n",
      "4   0.577450  0.713217 -0.199187  0.626237\n",
      "5   0.495847  1.078646 -0.515294  0.738712\n",
      "6   0.540857  0.982943 -0.494656  0.716314\n",
      "7   0.428603  1.142328 -0.495226  0.712820\n",
      "8   0.537355  0.706825  0.001726  0.617399\n",
      "9   0.481694  0.892503 -0.272453  0.705975\n",
      "10  0.529213  0.825892 -0.284226  0.687878\n",
      "11  0.420780  0.976534 -0.286725  0.687946\n",
      "12  0.546918  0.839280  0.338472  0.542846\n",
      "13  0.499460  0.760358  0.028604  0.594333\n",
      "14  0.537224  0.726268  0.104909  0.599446\n",
      "15  0.422351  0.864306  0.105174  0.600185\n",
      "16  0.490513  0.782137  0.016641  0.655864\n",
      "17  0.460435  0.939219 -0.295250  0.713510\n",
      "18  0.498921  0.865613 -0.260987  0.714376\n",
      "19  0.394683  1.012208 -0.258131  0.713041\n",
      "20  0.584588  0.630891  0.145385  0.590723\n",
      "21  0.504818  0.800184 -0.163465  0.673057\n",
      "22  0.549489  0.729794 -0.142180  0.656807\n",
      "23  0.433764  0.881912 -0.141383  0.657537\n",
      "24  0.593658  0.706537 -0.020730  0.641561\n",
      "25  0.536523  0.823347 -0.290475  0.700316\n",
      "26  0.574665  0.770299 -0.302361  0.670151\n",
      "27  0.453692  0.933230 -0.303781  0.669987\n",
      "28  0.539719  0.713843  0.039816  0.557743\n",
      "29  0.458570  0.842601 -0.115637  0.657354\n",
      "30  0.516441  0.790171 -0.194229  0.654855\n",
      "31  0.408085  0.932099 -0.196760  0.652865\n",
      "32  0.562042  0.668816  0.035591  0.607568\n",
      "33  0.491411  0.868815 -0.254717  0.704527\n",
      "34  0.538934  0.810674 -0.276937  0.695233\n",
      "35  0.427945  0.963594 -0.275395  0.694195\n",
      "36  0.563883  0.707808 -0.077411  0.609864\n",
      "37  0.500944  0.930858 -0.366038  0.719618\n",
      "38  0.545786  0.870151 -0.380365  0.698617\n",
      "39  0.436080  1.022088 -0.379676  0.697039\n"
     ]
    }
   ],
   "source": [
    "print(fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/60], Loss: 0.7920\n",
      "Epoch [10/60], Loss: 0.2726\n",
      "Epoch [15/60], Loss: 0.2092\n",
      "Epoch [20/60], Loss: 0.1842\n",
      "Epoch [25/60], Loss: 0.1699\n",
      "Epoch [30/60], Loss: 0.1602\n",
      "Epoch [35/60], Loss: 0.1528\n",
      "Epoch [40/60], Loss: 0.1470\n",
      "Epoch [45/60], Loss: 0.1422\n",
      "Epoch [50/60], Loss: 0.1381\n",
      "Epoch [55/60], Loss: 0.1346\n",
      "Epoch [60/60], Loss: 0.1316\n",
      "Epoch [5/60], Loss: 0.6497\n",
      "Epoch [10/60], Loss: 0.2890\n",
      "Epoch [15/60], Loss: 0.2419\n",
      "Epoch [20/60], Loss: 0.2123\n",
      "Epoch [25/60], Loss: 0.1916\n",
      "Epoch [30/60], Loss: 0.1762\n",
      "Epoch [35/60], Loss: 0.1643\n",
      "Epoch [40/60], Loss: 0.1549\n",
      "Epoch [45/60], Loss: 0.1472\n",
      "Epoch [50/60], Loss: 0.1409\n",
      "Epoch [55/60], Loss: 0.1356\n",
      "Epoch [60/60], Loss: 0.1312\n",
      "Epoch [5/60], Loss: 0.4841\n",
      "Epoch [10/60], Loss: 0.2370\n",
      "Epoch [15/60], Loss: 0.1969\n",
      "Epoch [20/60], Loss: 0.1731\n",
      "Epoch [25/60], Loss: 0.1572\n",
      "Epoch [30/60], Loss: 0.1460\n",
      "Epoch [35/60], Loss: 0.1380\n",
      "Epoch [40/60], Loss: 0.1320\n",
      "Epoch [45/60], Loss: 0.1276\n",
      "Epoch [50/60], Loss: 0.1242\n",
      "Epoch [55/60], Loss: 0.1215\n",
      "Epoch [60/60], Loss: 0.1193\n",
      "Epoch [5/60], Loss: 0.4668\n",
      "Epoch [10/60], Loss: 0.1890\n",
      "Epoch [15/60], Loss: 0.1588\n",
      "Epoch [20/60], Loss: 0.1420\n",
      "Epoch [25/60], Loss: 0.1310\n",
      "Epoch [30/60], Loss: 0.1235\n",
      "Epoch [35/60], Loss: 0.1183\n",
      "Epoch [40/60], Loss: 0.1147\n",
      "Epoch [45/60], Loss: 0.1120\n",
      "Epoch [50/60], Loss: 0.1101\n",
      "Epoch [55/60], Loss: 0.1086\n",
      "Epoch [60/60], Loss: 0.1075\n",
      "Epoch [5/60], Loss: 2.4630\n",
      "Epoch [10/60], Loss: 0.2796\n",
      "Epoch [15/60], Loss: 0.2165\n",
      "Epoch [20/60], Loss: 0.1871\n",
      "Epoch [25/60], Loss: 0.1679\n",
      "Epoch [30/60], Loss: 0.1546\n",
      "Epoch [35/60], Loss: 0.1450\n",
      "Epoch [40/60], Loss: 0.1380\n",
      "Epoch [45/60], Loss: 0.1326\n",
      "Epoch [50/60], Loss: 0.1285\n",
      "Epoch [55/60], Loss: 0.1252\n",
      "Epoch [60/60], Loss: 0.1225\n",
      "Epoch [5/60], Loss: 0.5977\n",
      "Epoch [10/60], Loss: 0.2731\n",
      "Epoch [15/60], Loss: 0.2076\n",
      "Epoch [20/60], Loss: 0.1793\n",
      "Epoch [25/60], Loss: 0.1626\n",
      "Epoch [30/60], Loss: 0.1514\n",
      "Epoch [35/60], Loss: 0.1433\n",
      "Epoch [40/60], Loss: 0.1372\n",
      "Epoch [45/60], Loss: 0.1324\n",
      "Epoch [50/60], Loss: 0.1285\n",
      "Epoch [55/60], Loss: 0.1254\n",
      "Epoch [60/60], Loss: 0.1228\n",
      "Epoch [5/60], Loss: 0.3992\n",
      "Epoch [10/60], Loss: 0.2198\n",
      "Epoch [15/60], Loss: 0.1971\n",
      "Epoch [20/60], Loss: 0.1815\n",
      "Epoch [25/60], Loss: 0.1693\n",
      "Epoch [30/60], Loss: 0.1595\n",
      "Epoch [35/60], Loss: 0.1516\n",
      "Epoch [40/60], Loss: 0.1451\n",
      "Epoch [45/60], Loss: 0.1396\n",
      "Epoch [50/60], Loss: 0.1350\n",
      "Epoch [55/60], Loss: 0.1311\n",
      "Epoch [60/60], Loss: 0.1278\n",
      "Epoch [5/60], Loss: 1.5629\n",
      "Epoch [10/60], Loss: 0.3910\n",
      "Epoch [15/60], Loss: 0.3010\n",
      "Epoch [20/60], Loss: 0.2478\n",
      "Epoch [25/60], Loss: 0.2124\n",
      "Epoch [30/60], Loss: 0.1881\n",
      "Epoch [35/60], Loss: 0.1712\n",
      "Epoch [40/60], Loss: 0.1592\n",
      "Epoch [45/60], Loss: 0.1505\n",
      "Epoch [50/60], Loss: 0.1441\n",
      "Epoch [55/60], Loss: 0.1392\n",
      "Epoch [60/60], Loss: 0.1354\n",
      "Epoch [5/60], Loss: 0.9175\n",
      "Epoch [10/60], Loss: 0.1844\n",
      "Epoch [15/60], Loss: 0.1525\n",
      "Epoch [20/60], Loss: 0.1400\n",
      "Epoch [25/60], Loss: 0.1329\n",
      "Epoch [30/60], Loss: 0.1285\n",
      "Epoch [35/60], Loss: 0.1253\n",
      "Epoch [40/60], Loss: 0.1229\n",
      "Epoch [45/60], Loss: 0.1211\n",
      "Epoch [50/60], Loss: 0.1195\n",
      "Epoch [55/60], Loss: 0.1181\n",
      "Epoch [60/60], Loss: 0.1169\n",
      "Epoch [5/60], Loss: 0.5980\n",
      "Epoch [10/60], Loss: 0.2020\n",
      "Epoch [15/60], Loss: 0.1526\n",
      "Epoch [20/60], Loss: 0.1398\n",
      "Epoch [25/60], Loss: 0.1951\n",
      "Epoch [30/60], Loss: 0.1883\n",
      "Epoch [35/60], Loss: 0.1350\n",
      "Epoch [40/60], Loss: 0.1202\n",
      "Epoch [45/60], Loss: 0.1165\n",
      "Epoch [50/60], Loss: 0.1146\n",
      "Epoch [55/60], Loss: 0.1135\n",
      "Epoch [60/60], Loss: 0.1132\n"
     ]
    }
   ],
   "source": [
    "#### tiling\n",
    "\n",
    "VALIDATION_LOSS = []\n",
    "fold_var = 1\n",
    "n = Y5.shape[0]\n",
    "hidden_size = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "fits = []\n",
    "detrend = []\n",
    "times = []\n",
    "\n",
    "for train_index, val_index in kf.split(Y5):\n",
    "    training_X = np.float32(X5[train_index])\n",
    "    validation_X = np.float32(X5[val_index])\n",
    "    training_Y = np.float32(Y5[train_index])\n",
    "    validation_Y = np.float32(Y5[val_index])\n",
    "\n",
    "    # CREATE NEW MODEL\n",
    "    model = NeuralNet(input_size=100, hidden_size=hidden_size, out_dim=1)\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    # CREATE CALLBACKS\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs = torch.from_numpy(training_X)\n",
    "        inputs = inputs.reshape(inputs.shape[0], 100)\n",
    "        targets = torch.from_numpy(training_Y)\n",
    "        targets = targets.reshape(targets.shape[0], 1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n",
    "    \n",
    "    pred_Y = model(torch.from_numpy(training_X)).detach().numpy()\n",
    "    pred_Y = pred_Y.reshape(pred_Y.shape[0])\n",
    "\n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X1)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Y1)[0,1],np.mean(np.square(fit-Y1)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X3)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Y3)[0,1],np.mean(np.square(fit-Y3)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(validation_X)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, validation_Y)[0,1],np.mean(np.square(fit-validation_Y)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X6)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Y6)[0,1],np.mean(np.square(fit-Y6)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    fold_var += 1\n",
    "\n",
    "fits = array(fits)\n",
    "fits = pd.DataFrame((fits))\n",
    "fits.to_csv(save_path +model_name+\"_fits_tiling.txt\", index = False)\n",
    "\n",
    "with open(save_path +model_name+\"_time_tiling.txt\", \"w\") as file:\n",
    "    for row in times:\n",
    "        s = \" \".join(map(str, row))\n",
    "        file.write(s+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average correlation on tiling: 0.5119961597181356 \n",
      "Average MSE on tiling: 0.12354688867926597 \n",
      "Average correlation on random: 0.45938177087521676 \n",
      "Average MSE on random: 0.11350463777780533 \n",
      "Average correlation on ChrV: 0.4041449483872988 \n",
      "Average MSE on ChrV: 0.22059142738580703 \n",
      "Average correlation on CN: 0.5235572529772334 \n",
      "Average MSE on CN: 0.16049545854330063\n"
     ]
    }
   ],
   "source": [
    "display_fits(fits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2cd633bf9703d9b8d2b7bb6e04b82983774c32d5f891ed1890ee26b779f7466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
