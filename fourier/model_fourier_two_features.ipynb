{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/Users/Brody1/Documents/Northwestern/Jiping/benchmarks/deep-learning/\"\n",
    "model_name = \"fourier_two_features\"\n",
    "kf = KFold(n_splits = 10, shuffle =True)\n",
    "num_epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### define functions ####\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, out_dim):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_fits(fits):\n",
    "    print(f\"Average correlation on tiling: {np.mean([fits[0][i] for i in range(fits[0].size) if (i % 4) == 2])}\",\n",
    "          f\"\\nAverage MSE on tiling: {np.mean([fits[1][i] for i in range(fits[1].size) if (i % 4) == 2])}\",\n",
    "          f\"\\nAverage correlation on random: {np.mean([fits[0][i] for i in range(fits[0].size) if (i % 4) == 1])}\",\n",
    "          f\"\\nAverage MSE on random: {np.mean([fits[1][i] for i in range(fits[1].size) if (i % 4) == 1])}\",\n",
    "          f\"\\nAverage correlation on ChrV: {np.mean([fits[0][i] for i in range(fits[0].size) if (i % 4) == 3])}\",\n",
    "          f\"\\nAverage MSE on ChrV: {np.mean([fits[1][i] for i in range(fits[1].size) if (i % 4) == 3])}\",\n",
    "          f\"\\nAverage correlation on CN: {np.mean([fits[0][i] for i in range(fits[0].size) if (i % 4) == 0])}\",\n",
    "          f\"\\nAverage MSE on CN: {np.mean([fits[1][i] for i in range(fits[1].size) if (i % 4) == 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct sine and cosine matrices for Fourier features:\n",
    "cos_matrix, sin_matrix = [[] for x in range(49)], [[] for x in range(49)]\n",
    "for n in range(49):\n",
    "    for k in range(49):\n",
    "        cos_matrix[n].append(np.cos(2*np.pi*(n+1)*k/49))\n",
    "        sin_matrix[n].append(np.sin(2*np.pi*(n+1)*k/49))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnaTwoFourier(sequence):\n",
    "    seq_array = array(list(sequence))\n",
    "    code = {\"AA\": [1], \"AT\": [1], \"TA\": [1], \"TT\": [1],\n",
    "            \"CA\": [0], \"CC\": [0], \"CG\": [0], \"CT\": [0],\n",
    "            \"GA\": [0], \"GC\": [0], \"GG\": [0], \"GT\": [0],\n",
    "            \"AC\": [0], \"TC\": [0], \"TG\": [0], \"AG\": [0],\n",
    "            \"aa\": [1], \"at\": [1], \"ta\": [1], \"tt\": [1],\n",
    "            \"ca\": [0], \"cc\": [0], \"cg\": [0], \"ct\": [0],\n",
    "            \"ga\": [0], \"gc\": [0], \"gg\": [0], \"gt\": [0],\n",
    "            \"ac\": [0], \"tc\": [0], \"tg\": [0], \"ag\": [0],}\n",
    "    AT_encoded_seq = []\n",
    "    for i in range(len(seq_array)-1):\n",
    "        AT_encoded_seq.append(code[\"\".join((seq_array[i], seq_array[i+1]))][0])\n",
    "    two_fourier_cos = np.matmul(array(AT_encoded_seq), array(cos_matrix).transpose())\n",
    "    two_fourier_sin = np.matmul(array(AT_encoded_seq), array(sin_matrix).transpose())\n",
    "    return list(np.concatenate((two_fourier_cos, two_fourier_sin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_c0new(dat):\n",
    "  mat = np.empty((3,3), float)\n",
    "  k = 2*np.pi/10.4\n",
    "  n = array([26, 29, 31])\n",
    "  mat[0:3,0] = 1\n",
    "  mat[0:3, 1] = np.sin(n*k)\n",
    "  mat[0:3, 2] = np.cos(n*k)\n",
    "  inv_mat = np.linalg.inv(mat)\n",
    "  c0A1A2 = array(np.matmul(dat[[\"n=26\", \"n=29\", \"n=31\"]], inv_mat))\n",
    "  c0Aphi = c0A1A2\n",
    "  c0Aphi[:,0] = c0A1A2[:,0]\n",
    "  c0Aphi[:,1] = np.sqrt(c0A1A2[:,1]**2 + c0A1A2[:,2]**2)\n",
    "  c0Aphi[:,2] <- np.sign(c0A1A2[:,2]) * np.arccos(c0A1A2[:,1]/c0Aphi[:,1])\n",
    "  return c0Aphi[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### preparing data ####\n",
    "\n",
    "data_cerevisiae_nucle = pd.read_csv(\"/Users/Brody1/Documents/Northwestern/Jiping/cycle1.txt\",delimiter = \",\")\n",
    "X1 = []\n",
    "for sequence_nt in data_cerevisiae_nucle[\"Sequence\"]:\n",
    "    X1.append(dnaTwoFourier(sequence_nt))\n",
    "X1 = np.float32(array(X1))\n",
    "# X1 = X1.reshape((X1.shape[0],50,4,1))\n",
    "### COMPUTE EIGENVECTORS:\n",
    "\n",
    "# X1_reverse = np.flip(X1,[1,2])\n",
    "# Y1 = data_cerevisiae_nucle[\"C0\"].values.astype(float)\n",
    "Y1 = np.float32(find_c0new(data_cerevisiae_nucle).astype(float))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_random_library = pd.read_csv(\"/Users/Brody1/Documents/Northwestern/Jiping/cycle3.txt\",delimiter = \",\")\n",
    "X3 = []\n",
    "for sequence_nt in data_random_library[\"Sequence\"]:\n",
    "    X3.append(dnaTwoFourier(sequence_nt))\n",
    "X3 = np.float32(array(X3))\n",
    "# X3 = X3.reshape((X3.shape[0],50,2,1))\n",
    "# X3_reverse = np.flip(X3,[1,2])\n",
    "# Y3 = data_random_library[\"C0\"].values.astype(float)\n",
    "Y3 = np.float32(find_c0new(data_random_library).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tiling = pd.read_csv(\"/Users/Brody1/Documents/Northwestern/Jiping/cycle5.txt\",delimiter = \",\")\n",
    "X5 = []\n",
    "for sequence_nt in data_tiling[\"Sequence\"]:\n",
    "    X5.append(dnaTwoFourier(sequence_nt))\n",
    "X5 = np.float32(array(X5))\n",
    "# X5 = X5.reshape((X5.shape[0],50,4,1))\n",
    "# X5_reverse = np.flip(X5,[1,2])\n",
    "# Y5 = data_tiling[\"C0\"].values.astype(float)\n",
    "Y5 = np.float32(find_c0new(data_tiling).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chr5 = pd.read_csv(\"/Users/Brody1/Documents/Northwestern/Jiping/cycle6.txt\",delimiter = \",\")\n",
    "X6 = []\n",
    "for sequence_nt in data_chr5[\"Sequence\"]:\n",
    "    X6.append(dnaTwoFourier(sequence_nt))\n",
    "X6 = np.float32(array(X6))\n",
    "# X6 = X6.reshape((X6.shape[0],50,4,1))\n",
    "# X6_reverse = np.flip(X6,[1,2])\n",
    "# Y6 = data_chr5[\"C0\"].values.astype(float)\n",
    "Y6 = np.float32(find_c0new(data_chr5).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = np.mean(Y1)\n",
    "std1 = np.std(Y1)\n",
    "Z1 = (Y1-m1)/std1\n",
    "\n",
    "m3 = np.mean(Y3)\n",
    "std3 = np.std(Y3)\n",
    "Z3 = (Y3-m3)/std3\n",
    "\n",
    "\n",
    "m5 = np.mean(Y5)\n",
    "std5 = np.std(Y5)\n",
    "Z5 = (Y5-m5)/std5\n",
    "\n",
    "\n",
    "m6 = np.mean(Y6)\n",
    "std6 = np.std(Y6)\n",
    "Z6 = (Y6-m6)/std6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/60], Loss: 1.0630\n",
      "Epoch [10/60], Loss: 0.7679\n",
      "Epoch [15/60], Loss: 0.7203\n",
      "Epoch [20/60], Loss: 0.6929\n",
      "Epoch [25/60], Loss: 0.6742\n",
      "Epoch [30/60], Loss: 0.6599\n",
      "Epoch [35/60], Loss: 0.6479\n",
      "Epoch [40/60], Loss: 0.6382\n",
      "Epoch [45/60], Loss: 0.6299\n",
      "Epoch [50/60], Loss: 0.6229\n",
      "Epoch [55/60], Loss: 0.6166\n",
      "Epoch [60/60], Loss: 0.6111\n",
      "Epoch [5/60], Loss: 1.0408\n",
      "Epoch [10/60], Loss: 0.7910\n",
      "Epoch [15/60], Loss: 0.7426\n",
      "Epoch [20/60], Loss: 0.7135\n",
      "Epoch [25/60], Loss: 0.6936\n",
      "Epoch [30/60], Loss: 0.6783\n",
      "Epoch [35/60], Loss: 0.6663\n",
      "Epoch [40/60], Loss: 0.6565\n",
      "Epoch [45/60], Loss: 0.6482\n",
      "Epoch [50/60], Loss: 0.6404\n",
      "Epoch [55/60], Loss: 0.6333\n",
      "Epoch [60/60], Loss: 0.6266\n",
      "Epoch [5/60], Loss: 0.8798\n",
      "Epoch [10/60], Loss: 0.7375\n",
      "Epoch [15/60], Loss: 0.7097\n",
      "Epoch [20/60], Loss: 0.6923\n",
      "Epoch [25/60], Loss: 0.6798\n",
      "Epoch [30/60], Loss: 0.6699\n",
      "Epoch [35/60], Loss: 0.6614\n",
      "Epoch [40/60], Loss: 0.6536\n",
      "Epoch [45/60], Loss: 0.6462\n",
      "Epoch [50/60], Loss: 0.6399\n",
      "Epoch [55/60], Loss: 0.6342\n",
      "Epoch [60/60], Loss: 0.6289\n",
      "Epoch [5/60], Loss: 1.1275\n",
      "Epoch [10/60], Loss: 0.7496\n",
      "Epoch [15/60], Loss: 0.7128\n",
      "Epoch [20/60], Loss: 0.6941\n",
      "Epoch [25/60], Loss: 0.6814\n",
      "Epoch [30/60], Loss: 0.6717\n",
      "Epoch [35/60], Loss: 0.6639\n",
      "Epoch [40/60], Loss: 0.6571\n",
      "Epoch [45/60], Loss: 0.6508\n",
      "Epoch [50/60], Loss: 0.6451\n",
      "Epoch [55/60], Loss: 0.6396\n",
      "Epoch [60/60], Loss: 0.6345\n",
      "Epoch [5/60], Loss: 1.1108\n",
      "Epoch [10/60], Loss: 0.7509\n",
      "Epoch [15/60], Loss: 0.7229\n",
      "Epoch [20/60], Loss: 0.7025\n",
      "Epoch [25/60], Loss: 0.6869\n",
      "Epoch [30/60], Loss: 0.6749\n",
      "Epoch [35/60], Loss: 0.6652\n",
      "Epoch [40/60], Loss: 0.6572\n",
      "Epoch [45/60], Loss: 0.6504\n",
      "Epoch [50/60], Loss: 0.6444\n",
      "Epoch [55/60], Loss: 0.6391\n",
      "Epoch [60/60], Loss: 0.6346\n",
      "Epoch [5/60], Loss: 1.0184\n",
      "Epoch [10/60], Loss: 0.7507\n",
      "Epoch [15/60], Loss: 0.7148\n",
      "Epoch [20/60], Loss: 0.6935\n",
      "Epoch [25/60], Loss: 0.6790\n",
      "Epoch [30/60], Loss: 0.6678\n",
      "Epoch [35/60], Loss: 0.6588\n",
      "Epoch [40/60], Loss: 0.6513\n",
      "Epoch [45/60], Loss: 0.6450\n",
      "Epoch [50/60], Loss: 0.6396\n",
      "Epoch [55/60], Loss: 0.6346\n",
      "Epoch [60/60], Loss: 0.6300\n",
      "Epoch [5/60], Loss: 1.0589\n",
      "Epoch [10/60], Loss: 0.7578\n",
      "Epoch [15/60], Loss: 0.7228\n",
      "Epoch [20/60], Loss: 0.7016\n",
      "Epoch [25/60], Loss: 0.6867\n",
      "Epoch [30/60], Loss: 0.6750\n",
      "Epoch [35/60], Loss: 0.6657\n",
      "Epoch [40/60], Loss: 0.6579\n",
      "Epoch [45/60], Loss: 0.6513\n",
      "Epoch [50/60], Loss: 0.6453\n",
      "Epoch [55/60], Loss: 0.6398\n",
      "Epoch [60/60], Loss: 0.6347\n",
      "Epoch [5/60], Loss: 0.9740\n",
      "Epoch [10/60], Loss: 0.7322\n",
      "Epoch [15/60], Loss: 0.7046\n",
      "Epoch [20/60], Loss: 0.6888\n",
      "Epoch [25/60], Loss: 0.6766\n",
      "Epoch [30/60], Loss: 0.6666\n",
      "Epoch [35/60], Loss: 0.6583\n",
      "Epoch [40/60], Loss: 0.6512\n",
      "Epoch [45/60], Loss: 0.6450\n",
      "Epoch [50/60], Loss: 0.6395\n",
      "Epoch [55/60], Loss: 0.6343\n",
      "Epoch [60/60], Loss: 0.6296\n",
      "Epoch [5/60], Loss: 3.0625\n",
      "Epoch [10/60], Loss: 0.8313\n",
      "Epoch [15/60], Loss: 0.7878\n",
      "Epoch [20/60], Loss: 0.7610\n",
      "Epoch [25/60], Loss: 0.7414\n",
      "Epoch [30/60], Loss: 0.7254\n",
      "Epoch [35/60], Loss: 0.7121\n",
      "Epoch [40/60], Loss: 0.7009\n",
      "Epoch [45/60], Loss: 0.6911\n",
      "Epoch [50/60], Loss: 0.6825\n",
      "Epoch [55/60], Loss: 0.6748\n",
      "Epoch [60/60], Loss: 0.6678\n",
      "Epoch [5/60], Loss: 1.2276\n",
      "Epoch [10/60], Loss: 0.7653\n",
      "Epoch [15/60], Loss: 0.7367\n",
      "Epoch [20/60], Loss: 0.7174\n",
      "Epoch [25/60], Loss: 0.7014\n",
      "Epoch [30/60], Loss: 0.6873\n",
      "Epoch [35/60], Loss: 0.6750\n",
      "Epoch [40/60], Loss: 0.6648\n",
      "Epoch [45/60], Loss: 0.6559\n",
      "Epoch [50/60], Loss: 0.6482\n",
      "Epoch [55/60], Loss: 0.6413\n",
      "Epoch [60/60], Loss: 0.6352\n"
     ]
    }
   ],
   "source": [
    "#### random\n",
    "\n",
    "VALIDATION_LOSS = []\n",
    "fold_var = 1\n",
    "n = Z3.shape[0]\n",
    "hidden_size = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "fits = []\n",
    "detrend = []\n",
    "times = []\n",
    "\n",
    "for train_index, val_index in kf.split(Z3):\n",
    "    training_X = np.float32(X3[train_index])\n",
    "    validation_X = np.float32(X3[val_index])\n",
    "    training_Y = np.float32(Z3[train_index])\n",
    "    validation_Y = np.float32(Z3[val_index])\n",
    "\n",
    "    # CREATE NEW MODEL\n",
    "    model = NeuralNet(input_size=98, hidden_size=hidden_size, out_dim=1)\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    # CREATE CALLBACKS\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs = torch.from_numpy(training_X)\n",
    "        inputs = inputs.reshape(inputs.shape[0], 98)\n",
    "        targets = torch.from_numpy(training_Y)\n",
    "        targets = targets.reshape(targets.shape[0], 1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n",
    "    \n",
    "    pred_Y = model(torch.from_numpy(training_X)).detach().numpy()\n",
    "    pred_Y = pred_Y.reshape(pred_Y.shape[0])\n",
    "\n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X1)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Z1)[0,1],np.mean(np.square(fit-Z1)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(validation_X)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, validation_Y)[0,1],np.mean(np.square(fit-validation_Y)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X5)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Z5)[0,1],np.mean(np.square(fit-Z5)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X6)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Z6)[0,1],np.mean(np.square(fit-Z6)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    fold_var += 1\n",
    "\n",
    "fits = array(fits)\n",
    "fits = pd.DataFrame((fits))\n",
    "fits.to_csv(save_path +model_name+\"_fits_random.txt\", index = False)\n",
    "\n",
    "with open(save_path +model_name+\"_time_random.txt\", \"w\") as file:\n",
    "    for row in times:\n",
    "        s = \" \".join(map(str, row))\n",
    "        file.write(s+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average correlation on tiling: 0.5930295159282337 \n",
      "Average MSE on tiling: 0.6793038725852967 \n",
      "Average correlation on random: 0.5493016231856259 \n",
      "Average MSE on random: 0.7014491081237793 \n",
      "Average correlation on ChrV: 0.47164845281173695 \n",
      "Average MSE on ChrV: 0.8494952499866486 \n",
      "Average correlation on CN: 0.5935115206887798 \n",
      "Average MSE on CN: 0.8720184922218323\n"
     ]
    }
   ],
   "source": [
    "display_fits(fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/60], Loss: 1.2091\n",
      "Epoch [10/60], Loss: 0.8911\n",
      "Epoch [15/60], Loss: 0.8513\n",
      "Epoch [20/60], Loss: 0.8330\n",
      "Epoch [25/60], Loss: 0.8210\n",
      "Epoch [30/60], Loss: 0.8118\n",
      "Epoch [35/60], Loss: 0.8042\n",
      "Epoch [40/60], Loss: 0.7979\n",
      "Epoch [45/60], Loss: 0.7925\n",
      "Epoch [50/60], Loss: 0.7877\n",
      "Epoch [55/60], Loss: 0.7834\n",
      "Epoch [60/60], Loss: 0.7796\n",
      "Epoch [5/60], Loss: 1.5198\n",
      "Epoch [10/60], Loss: 0.8777\n",
      "Epoch [15/60], Loss: 0.8378\n",
      "Epoch [20/60], Loss: 0.8179\n",
      "Epoch [25/60], Loss: 0.8056\n",
      "Epoch [30/60], Loss: 0.7974\n",
      "Epoch [35/60], Loss: 0.7912\n",
      "Epoch [40/60], Loss: 0.7862\n",
      "Epoch [45/60], Loss: 0.7819\n",
      "Epoch [50/60], Loss: 0.7782\n",
      "Epoch [55/60], Loss: 0.7747\n",
      "Epoch [60/60], Loss: 0.7714\n",
      "Epoch [5/60], Loss: 1.2597\n",
      "Epoch [10/60], Loss: 0.8784\n",
      "Epoch [15/60], Loss: 0.8403\n",
      "Epoch [20/60], Loss: 0.8196\n",
      "Epoch [25/60], Loss: 0.8049\n",
      "Epoch [30/60], Loss: 0.7936\n",
      "Epoch [35/60], Loss: 0.7848\n",
      "Epoch [40/60], Loss: 0.7776\n",
      "Epoch [45/60], Loss: 0.7717\n",
      "Epoch [50/60], Loss: 0.7667\n",
      "Epoch [55/60], Loss: 0.7626\n",
      "Epoch [60/60], Loss: 0.7591\n",
      "Epoch [5/60], Loss: 1.9374\n",
      "Epoch [10/60], Loss: 0.9193\n",
      "Epoch [15/60], Loss: 0.8823\n",
      "Epoch [20/60], Loss: 0.8616\n",
      "Epoch [25/60], Loss: 0.8460\n",
      "Epoch [30/60], Loss: 0.8335\n",
      "Epoch [35/60], Loss: 0.8232\n",
      "Epoch [40/60], Loss: 0.8147\n",
      "Epoch [45/60], Loss: 0.8075\n",
      "Epoch [50/60], Loss: 0.8012\n",
      "Epoch [55/60], Loss: 0.7958\n",
      "Epoch [60/60], Loss: 0.7910\n",
      "Epoch [5/60], Loss: 1.0829\n",
      "Epoch [10/60], Loss: 0.8669\n",
      "Epoch [15/60], Loss: 0.8393\n",
      "Epoch [20/60], Loss: 0.8233\n",
      "Epoch [25/60], Loss: 0.8110\n",
      "Epoch [30/60], Loss: 0.8007\n",
      "Epoch [35/60], Loss: 0.7919\n",
      "Epoch [40/60], Loss: 0.7843\n",
      "Epoch [45/60], Loss: 0.7779\n",
      "Epoch [50/60], Loss: 0.7726\n",
      "Epoch [55/60], Loss: 0.7680\n",
      "Epoch [60/60], Loss: 0.7641\n",
      "Epoch [5/60], Loss: 1.4251\n",
      "Epoch [10/60], Loss: 0.8742\n",
      "Epoch [15/60], Loss: 0.8203\n",
      "Epoch [20/60], Loss: 0.7991\n",
      "Epoch [25/60], Loss: 0.7880\n",
      "Epoch [30/60], Loss: 0.7807\n",
      "Epoch [35/60], Loss: 0.7750\n",
      "Epoch [40/60], Loss: 0.7704\n",
      "Epoch [45/60], Loss: 0.7662\n",
      "Epoch [50/60], Loss: 0.7625\n",
      "Epoch [55/60], Loss: 0.7592\n",
      "Epoch [60/60], Loss: 0.7563\n",
      "Epoch [5/60], Loss: 1.8979\n",
      "Epoch [10/60], Loss: 0.9533\n",
      "Epoch [15/60], Loss: 0.8811\n",
      "Epoch [20/60], Loss: 0.8486\n",
      "Epoch [25/60], Loss: 0.8281\n",
      "Epoch [30/60], Loss: 0.8138\n",
      "Epoch [35/60], Loss: 0.8031\n",
      "Epoch [40/60], Loss: 0.7947\n",
      "Epoch [45/60], Loss: 0.7880\n",
      "Epoch [50/60], Loss: 0.7823\n",
      "Epoch [55/60], Loss: 0.7775\n",
      "Epoch [60/60], Loss: 0.7732\n",
      "Epoch [5/60], Loss: 1.5361\n",
      "Epoch [10/60], Loss: 0.9778\n",
      "Epoch [15/60], Loss: 0.8982\n",
      "Epoch [20/60], Loss: 0.8570\n",
      "Epoch [25/60], Loss: 0.8313\n",
      "Epoch [30/60], Loss: 0.8142\n",
      "Epoch [35/60], Loss: 0.8021\n",
      "Epoch [40/60], Loss: 0.7928\n",
      "Epoch [45/60], Loss: 0.7855\n",
      "Epoch [50/60], Loss: 0.7795\n",
      "Epoch [55/60], Loss: 0.7743\n",
      "Epoch [60/60], Loss: 0.7697\n",
      "Epoch [5/60], Loss: 1.3724\n",
      "Epoch [10/60], Loss: 0.8804\n",
      "Epoch [15/60], Loss: 0.8361\n",
      "Epoch [20/60], Loss: 0.8098\n",
      "Epoch [25/60], Loss: 0.7923\n",
      "Epoch [30/60], Loss: 0.7805\n",
      "Epoch [35/60], Loss: 0.7722\n",
      "Epoch [40/60], Loss: 0.7659\n",
      "Epoch [45/60], Loss: 0.7609\n",
      "Epoch [50/60], Loss: 0.7570\n",
      "Epoch [55/60], Loss: 0.7537\n",
      "Epoch [60/60], Loss: 0.7510\n",
      "Epoch [5/60], Loss: 1.2718\n",
      "Epoch [10/60], Loss: 0.8819\n",
      "Epoch [15/60], Loss: 0.8382\n",
      "Epoch [20/60], Loss: 0.8156\n",
      "Epoch [25/60], Loss: 0.8014\n",
      "Epoch [30/60], Loss: 0.7915\n",
      "Epoch [35/60], Loss: 0.7840\n",
      "Epoch [40/60], Loss: 0.7779\n",
      "Epoch [45/60], Loss: 0.7728\n",
      "Epoch [50/60], Loss: 0.7684\n",
      "Epoch [55/60], Loss: 0.7645\n",
      "Epoch [60/60], Loss: 0.7610\n"
     ]
    }
   ],
   "source": [
    "#### chrv\n",
    "\n",
    "VALIDATION_LOSS = []\n",
    "fold_var = 1\n",
    "n = Z6.shape[0]\n",
    "hidden_size = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "fits = []\n",
    "detrend = []\n",
    "times = []\n",
    "\n",
    "for train_index, val_index in kf.split(Z6):\n",
    "    training_X = np.float32(X6[train_index])\n",
    "    validation_X = np.float32(X6[val_index])\n",
    "    training_Y = np.float32(Z6[train_index])\n",
    "    validation_Y = np.float32(Z6[val_index])\n",
    "\n",
    "    # CREATE NEW MODEL\n",
    "    model = NeuralNet(input_size=98, hidden_size=hidden_size, out_dim=1)\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    # CREATE CALLBACKS\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs = torch.from_numpy(training_X)\n",
    "        inputs = inputs.reshape(inputs.shape[0], 98)\n",
    "        targets = torch.from_numpy(training_Y)\n",
    "        targets = targets.reshape(targets.shape[0], 1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n",
    "    \n",
    "    pred_Y = model(torch.from_numpy(training_X)).detach().numpy()\n",
    "    pred_Y = pred_Y.reshape(pred_Y.shape[0])\n",
    "\n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X1)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Z1)[0,1],np.mean(np.square(fit-Z1)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X3)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Z3)[0,1],np.mean(np.square(fit-Z3)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X5)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Z5)[0,1],np.mean(np.square(fit-Z5)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(validation_X)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, validation_Y)[0,1],np.mean(np.square(fit-validation_Y)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    fold_var += 1\n",
    "\n",
    "fits = array(fits)\n",
    "fits = pd.DataFrame((fits))\n",
    "fits.to_csv(save_path +model_name+\"_fits_chrv.txt\", index = False)\n",
    "\n",
    "with open(save_path +model_name+\"_time_chrv.txt\", \"w\") as file:\n",
    "    for row in times:\n",
    "        s = \" \".join(map(str, row))\n",
    "        file.write(s+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average correlation on tiling: 0.5938283358539344 \n",
      "Average MSE on tiling: 0.6557923197746277 \n",
      "Average correlation on random: 0.5371012369290451 \n",
      "Average MSE on random: 0.7216444253921509 \n",
      "Average correlation on ChrV: 0.4748011714363708 \n",
      "Average MSE on ChrV: 0.7759105622768402 \n",
      "Average correlation on CN: 0.5943925875827899 \n",
      "Average MSE on CN: 0.7102785110473633\n"
     ]
    }
   ],
   "source": [
    "display_fits(fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/60], Loss: 1.9453\n",
      "Epoch [10/60], Loss: 0.8976\n",
      "Epoch [15/60], Loss: 0.7984\n",
      "Epoch [20/60], Loss: 0.7570\n",
      "Epoch [25/60], Loss: 0.7316\n",
      "Epoch [30/60], Loss: 0.7136\n",
      "Epoch [35/60], Loss: 0.6999\n",
      "Epoch [40/60], Loss: 0.6889\n",
      "Epoch [45/60], Loss: 0.6795\n",
      "Epoch [50/60], Loss: 0.6713\n",
      "Epoch [55/60], Loss: 0.6641\n",
      "Epoch [60/60], Loss: 0.6574\n",
      "Epoch [5/60], Loss: 1.4342\n",
      "Epoch [10/60], Loss: 0.9067\n",
      "Epoch [15/60], Loss: 0.8152\n",
      "Epoch [20/60], Loss: 0.7706\n",
      "Epoch [25/60], Loss: 0.7424\n",
      "Epoch [30/60], Loss: 0.7221\n",
      "Epoch [35/60], Loss: 0.7066\n",
      "Epoch [40/60], Loss: 0.6941\n",
      "Epoch [45/60], Loss: 0.6836\n",
      "Epoch [50/60], Loss: 0.6748\n",
      "Epoch [55/60], Loss: 0.6670\n",
      "Epoch [60/60], Loss: 0.6602\n",
      "Epoch [5/60], Loss: 1.4603\n",
      "Epoch [10/60], Loss: 0.9055\n",
      "Epoch [15/60], Loss: 0.7994\n",
      "Epoch [20/60], Loss: 0.7443\n",
      "Epoch [25/60], Loss: 0.7101\n",
      "Epoch [30/60], Loss: 0.6868\n",
      "Epoch [35/60], Loss: 0.6693\n",
      "Epoch [40/60], Loss: 0.6544\n",
      "Epoch [45/60], Loss: 0.6416\n",
      "Epoch [50/60], Loss: 0.6298\n",
      "Epoch [55/60], Loss: 0.6190\n",
      "Epoch [60/60], Loss: 0.6103\n",
      "Epoch [5/60], Loss: 1.5310\n",
      "Epoch [10/60], Loss: 0.8559\n",
      "Epoch [15/60], Loss: 0.7891\n",
      "Epoch [20/60], Loss: 0.7525\n",
      "Epoch [25/60], Loss: 0.7270\n",
      "Epoch [30/60], Loss: 0.7069\n",
      "Epoch [35/60], Loss: 0.6909\n",
      "Epoch [40/60], Loss: 0.6775\n",
      "Epoch [45/60], Loss: 0.6664\n",
      "Epoch [50/60], Loss: 0.6566\n",
      "Epoch [55/60], Loss: 0.6482\n",
      "Epoch [60/60], Loss: 0.6405\n",
      "Epoch [5/60], Loss: 1.0455\n",
      "Epoch [10/60], Loss: 0.7786\n",
      "Epoch [15/60], Loss: 0.7363\n",
      "Epoch [20/60], Loss: 0.7103\n",
      "Epoch [25/60], Loss: 0.6926\n",
      "Epoch [30/60], Loss: 0.6925\n",
      "Epoch [35/60], Loss: 0.6883\n",
      "Epoch [40/60], Loss: 0.6671\n",
      "Epoch [45/60], Loss: 0.6521\n",
      "Epoch [50/60], Loss: 0.6460\n",
      "Epoch [55/60], Loss: 0.6425\n",
      "Epoch [60/60], Loss: 0.6462\n",
      "Epoch [5/60], Loss: 1.2075\n",
      "Epoch [10/60], Loss: 0.9175\n",
      "Epoch [15/60], Loss: 0.8257\n",
      "Epoch [20/60], Loss: 0.7744\n",
      "Epoch [25/60], Loss: 0.7422\n",
      "Epoch [30/60], Loss: 0.7187\n",
      "Epoch [35/60], Loss: 0.6987\n",
      "Epoch [40/60], Loss: 0.6833\n",
      "Epoch [45/60], Loss: 0.6708\n",
      "Epoch [50/60], Loss: 0.6618\n",
      "Epoch [55/60], Loss: 0.6539\n",
      "Epoch [60/60], Loss: 0.6458\n",
      "Epoch [5/60], Loss: 1.5330\n",
      "Epoch [10/60], Loss: 0.9629\n",
      "Epoch [15/60], Loss: 0.8506\n",
      "Epoch [20/60], Loss: 0.7856\n",
      "Epoch [25/60], Loss: 0.7428\n",
      "Epoch [30/60], Loss: 0.7131\n",
      "Epoch [35/60], Loss: 0.6911\n",
      "Epoch [40/60], Loss: 0.6745\n",
      "Epoch [45/60], Loss: 0.6618\n",
      "Epoch [50/60], Loss: 0.6514\n",
      "Epoch [55/60], Loss: 0.6425\n",
      "Epoch [60/60], Loss: 0.6345\n",
      "Epoch [5/60], Loss: 1.2476\n",
      "Epoch [10/60], Loss: 0.8404\n",
      "Epoch [15/60], Loss: 0.7784\n",
      "Epoch [20/60], Loss: 0.7434\n",
      "Epoch [25/60], Loss: 0.7211\n",
      "Epoch [30/60], Loss: 0.7093\n",
      "Epoch [35/60], Loss: 0.7118\n",
      "Epoch [40/60], Loss: 0.6922\n",
      "Epoch [45/60], Loss: 0.6792\n",
      "Epoch [50/60], Loss: 0.6643\n",
      "Epoch [55/60], Loss: 0.6595\n",
      "Epoch [60/60], Loss: 0.6529\n",
      "Epoch [5/60], Loss: 1.1932\n",
      "Epoch [10/60], Loss: 0.8763\n",
      "Epoch [15/60], Loss: 0.7665\n",
      "Epoch [20/60], Loss: 0.7377\n",
      "Epoch [25/60], Loss: 0.7137\n",
      "Epoch [30/60], Loss: 0.7081\n",
      "Epoch [35/60], Loss: 0.6907\n",
      "Epoch [40/60], Loss: 0.6874\n",
      "Epoch [45/60], Loss: 0.6719\n",
      "Epoch [50/60], Loss: 0.6701\n",
      "Epoch [55/60], Loss: 0.6588\n",
      "Epoch [60/60], Loss: 0.6600\n",
      "Epoch [5/60], Loss: 1.5861\n",
      "Epoch [10/60], Loss: 0.8883\n",
      "Epoch [15/60], Loss: 0.7842\n",
      "Epoch [20/60], Loss: 0.7404\n",
      "Epoch [25/60], Loss: 0.7144\n",
      "Epoch [30/60], Loss: 0.6957\n",
      "Epoch [35/60], Loss: 0.6808\n",
      "Epoch [40/60], Loss: 0.6685\n",
      "Epoch [45/60], Loss: 0.6581\n",
      "Epoch [50/60], Loss: 0.6492\n",
      "Epoch [55/60], Loss: 0.6416\n",
      "Epoch [60/60], Loss: 0.6350\n"
     ]
    }
   ],
   "source": [
    "#### CN\n",
    "\n",
    "VALIDATION_LOSS = []\n",
    "fold_var = 1\n",
    "n = Z1.shape[0]\n",
    "hidden_size = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "fits = []\n",
    "detrend = []\n",
    "times = []\n",
    "\n",
    "for train_index, val_index in kf.split(Z1):\n",
    "    training_X = np.float32(X1[train_index])\n",
    "    validation_X = np.float32(X1[val_index])\n",
    "    training_Y = np.float32(Z1[train_index])\n",
    "    validation_Y = np.float32(Z1[val_index])\n",
    "\n",
    "    # CREATE NEW MODEL\n",
    "    model = NeuralNet(input_size=98, hidden_size=hidden_size, out_dim=1)\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    # CREATE CALLBACKS\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs = torch.from_numpy(training_X)\n",
    "        inputs = inputs.reshape(inputs.shape[0], 98)\n",
    "        targets = torch.from_numpy(training_Y)\n",
    "        targets = targets.reshape(targets.shape[0], 1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n",
    "    \n",
    "    pred_Y = model(torch.from_numpy(training_X)).detach().numpy()\n",
    "    pred_Y = pred_Y.reshape(pred_Y.shape[0])\n",
    "\n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(validation_X)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, validation_Y)[0,1],np.mean(np.square(fit-validation_Y)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X3)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Z3)[0,1],np.mean(np.square(fit-Z3)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X5)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Z5)[0,1],np.mean(np.square(fit-Z5)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X6)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Z6)[0,1],np.mean(np.square(fit-Z6)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    fold_var += 1\n",
    "\n",
    "fits = array(fits)\n",
    "fits = pd.DataFrame((fits))\n",
    "fits.to_csv(save_path +model_name+\"_fits_CN.txt\", index = False)\n",
    "\n",
    "with open(save_path +model_name+\"_time_CN.txt\", \"w\") as file:\n",
    "    for row in times:\n",
    "        s = \" \".join(map(str, row))\n",
    "        file.write(s+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average correlation on tiling: 0.5500206456345935 \n",
      "Average MSE on tiling: 0.8169593989849091 \n",
      "Average correlation on random: 0.49109461931923165 \n",
      "Average MSE on random: 0.8820307493209839 \n",
      "Average correlation on ChrV: 0.4329616145496148 \n",
      "Average MSE on ChrV: 0.978108137845993 \n",
      "Average correlation on CN: 0.5597121300299535 \n",
      "Average MSE on CN: 0.6956876754760742\n"
     ]
    }
   ],
   "source": [
    "display_fits(fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/60], Loss: 0.3343\n",
      "Epoch [10/60], Loss: 0.1663\n",
      "Epoch [15/60], Loss: 0.1455\n",
      "Epoch [20/60], Loss: 0.1328\n",
      "Epoch [25/60], Loss: 0.1244\n",
      "Epoch [30/60], Loss: 0.1184\n",
      "Epoch [35/60], Loss: 0.1141\n",
      "Epoch [40/60], Loss: 0.1110\n",
      "Epoch [45/60], Loss: 0.1086\n",
      "Epoch [50/60], Loss: 0.1068\n",
      "Epoch [55/60], Loss: 0.1054\n",
      "Epoch [60/60], Loss: 0.1043\n",
      "Epoch [5/60], Loss: 1.3945\n",
      "Epoch [10/60], Loss: 0.2584\n",
      "Epoch [15/60], Loss: 0.2095\n",
      "Epoch [20/60], Loss: 0.1832\n",
      "Epoch [25/60], Loss: 0.1653\n",
      "Epoch [30/60], Loss: 0.1522\n",
      "Epoch [35/60], Loss: 0.1424\n",
      "Epoch [40/60], Loss: 0.1349\n",
      "Epoch [45/60], Loss: 0.1290\n",
      "Epoch [50/60], Loss: 0.1243\n",
      "Epoch [55/60], Loss: 0.1205\n",
      "Epoch [60/60], Loss: 0.1174\n",
      "Epoch [5/60], Loss: 0.2692\n",
      "Epoch [10/60], Loss: 0.1724\n",
      "Epoch [15/60], Loss: 0.1486\n",
      "Epoch [20/60], Loss: 0.1344\n",
      "Epoch [25/60], Loss: 0.1253\n",
      "Epoch [30/60], Loss: 0.1192\n",
      "Epoch [35/60], Loss: 0.1149\n",
      "Epoch [40/60], Loss: 0.1118\n",
      "Epoch [45/60], Loss: 0.1095\n",
      "Epoch [50/60], Loss: 0.1078\n",
      "Epoch [55/60], Loss: 0.1064\n",
      "Epoch [60/60], Loss: 0.1053\n",
      "Epoch [5/60], Loss: 0.7139\n",
      "Epoch [10/60], Loss: 0.1947\n",
      "Epoch [15/60], Loss: 0.1682\n",
      "Epoch [20/60], Loss: 0.1550\n",
      "Epoch [25/60], Loss: 0.1460\n",
      "Epoch [30/60], Loss: 0.1391\n",
      "Epoch [35/60], Loss: 0.1336\n",
      "Epoch [40/60], Loss: 0.1291\n",
      "Epoch [45/60], Loss: 0.1253\n",
      "Epoch [50/60], Loss: 0.1222\n",
      "Epoch [55/60], Loss: 0.1195\n",
      "Epoch [60/60], Loss: 0.1172\n",
      "Epoch [5/60], Loss: 0.4180\n",
      "Epoch [10/60], Loss: 0.1706\n",
      "Epoch [15/60], Loss: 0.1521\n",
      "Epoch [20/60], Loss: 0.1407\n",
      "Epoch [25/60], Loss: 0.1325\n",
      "Epoch [30/60], Loss: 0.1262\n",
      "Epoch [35/60], Loss: 0.1214\n",
      "Epoch [40/60], Loss: 0.1174\n",
      "Epoch [45/60], Loss: 0.1142\n",
      "Epoch [50/60], Loss: 0.1116\n",
      "Epoch [55/60], Loss: 0.1094\n",
      "Epoch [60/60], Loss: 0.1076\n",
      "Epoch [5/60], Loss: 0.6627\n",
      "Epoch [10/60], Loss: 0.1992\n",
      "Epoch [15/60], Loss: 0.1743\n",
      "Epoch [20/60], Loss: 0.1595\n",
      "Epoch [25/60], Loss: 0.1490\n",
      "Epoch [30/60], Loss: 0.1410\n",
      "Epoch [35/60], Loss: 0.1346\n",
      "Epoch [40/60], Loss: 0.1295\n",
      "Epoch [45/60], Loss: 0.1252\n",
      "Epoch [50/60], Loss: 0.1216\n",
      "Epoch [55/60], Loss: 0.1186\n",
      "Epoch [60/60], Loss: 0.1159\n",
      "Epoch [5/60], Loss: 0.7306\n",
      "Epoch [10/60], Loss: 0.2193\n",
      "Epoch [15/60], Loss: 0.1917\n",
      "Epoch [20/60], Loss: 0.1738\n",
      "Epoch [25/60], Loss: 0.1608\n",
      "Epoch [30/60], Loss: 0.1511\n",
      "Epoch [35/60], Loss: 0.1435\n",
      "Epoch [40/60], Loss: 0.1375\n",
      "Epoch [45/60], Loss: 0.1327\n",
      "Epoch [50/60], Loss: 0.1287\n",
      "Epoch [55/60], Loss: 0.1254\n",
      "Epoch [60/60], Loss: 0.1227\n",
      "Epoch [5/60], Loss: 0.9768\n",
      "Epoch [10/60], Loss: 0.3262\n",
      "Epoch [15/60], Loss: 0.2581\n",
      "Epoch [20/60], Loss: 0.2190\n",
      "Epoch [25/60], Loss: 0.1934\n",
      "Epoch [30/60], Loss: 0.1754\n",
      "Epoch [35/60], Loss: 0.1623\n",
      "Epoch [40/60], Loss: 0.1526\n",
      "Epoch [45/60], Loss: 0.1451\n",
      "Epoch [50/60], Loss: 0.1392\n",
      "Epoch [55/60], Loss: 0.1345\n",
      "Epoch [60/60], Loss: 0.1306\n",
      "Epoch [5/60], Loss: 0.9039\n",
      "Epoch [10/60], Loss: 0.1947\n",
      "Epoch [15/60], Loss: 0.1664\n",
      "Epoch [20/60], Loss: 0.1497\n",
      "Epoch [25/60], Loss: 0.1382\n",
      "Epoch [30/60], Loss: 0.1299\n",
      "Epoch [35/60], Loss: 0.1238\n",
      "Epoch [40/60], Loss: 0.1192\n",
      "Epoch [45/60], Loss: 0.1158\n",
      "Epoch [50/60], Loss: 0.1131\n",
      "Epoch [55/60], Loss: 0.1110\n",
      "Epoch [60/60], Loss: 0.1093\n",
      "Epoch [5/60], Loss: 0.7089\n",
      "Epoch [10/60], Loss: 0.2033\n",
      "Epoch [15/60], Loss: 0.1748\n",
      "Epoch [20/60], Loss: 0.1584\n",
      "Epoch [25/60], Loss: 0.1467\n",
      "Epoch [30/60], Loss: 0.1379\n",
      "Epoch [35/60], Loss: 0.1311\n",
      "Epoch [40/60], Loss: 0.1258\n",
      "Epoch [45/60], Loss: 0.1215\n",
      "Epoch [50/60], Loss: 0.1182\n",
      "Epoch [55/60], Loss: 0.1154\n",
      "Epoch [60/60], Loss: 0.1131\n"
     ]
    }
   ],
   "source": [
    "#### tiling\n",
    "\n",
    "VALIDATION_LOSS = []\n",
    "fold_var = 1\n",
    "n = Y5.shape[0]\n",
    "hidden_size = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "fits = []\n",
    "detrend = []\n",
    "times = []\n",
    "\n",
    "for train_index, val_index in kf.split(Y5):\n",
    "    training_X = np.float32(X5[train_index])\n",
    "    validation_X = np.float32(X5[val_index])\n",
    "    training_Y = np.float32(Y5[train_index])\n",
    "    validation_Y = np.float32(Y5[val_index])\n",
    "\n",
    "    # CREATE NEW MODEL\n",
    "    model = NeuralNet(input_size=98, hidden_size=hidden_size, out_dim=1)\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    # CREATE CALLBACKS\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs = torch.from_numpy(training_X)\n",
    "        inputs = inputs.reshape(inputs.shape[0], 98)\n",
    "        targets = torch.from_numpy(training_Y)\n",
    "        targets = targets.reshape(targets.shape[0], 1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n",
    "    \n",
    "    pred_Y = model(torch.from_numpy(training_X)).detach().numpy()\n",
    "    pred_Y = pred_Y.reshape(pred_Y.shape[0])\n",
    "\n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X1)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Y1)[0,1],np.mean(np.square(fit-Y1)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X3)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Y3)[0,1],np.mean(np.square(fit-Y3)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(validation_X)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, validation_Y)[0,1],np.mean(np.square(fit-validation_Y)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    fit = model(torch.from_numpy(X6)).detach().numpy()\n",
    "    fit = fit.reshape(fit.shape[0])\n",
    "    fit_tmp =[np.corrcoef(fit, Y6)[0,1],np.mean(np.square(fit-Y6)),np.mean(fit),np.std(fit)]\n",
    "    fits.append(fit_tmp)\n",
    "    time0 = time.process_time() - start_time\n",
    "    times.append([time0])\n",
    "    \n",
    "    fold_var += 1\n",
    "\n",
    "fits = array(fits)\n",
    "fits = pd.DataFrame((fits))\n",
    "fits.to_csv(save_path +model_name+\"_fits_tiling.txt\", index = False)\n",
    "\n",
    "with open(save_path +model_name+\"_time_tiling.txt\", \"w\") as file:\n",
    "    for row in times:\n",
    "        s = \" \".join(map(str, row))\n",
    "        file.write(s+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average correlation on tiling: 0.5465018162996097 \n",
      "Average MSE on tiling: 0.11556075289845466 \n",
      "Average correlation on random: 0.48127783575928235 \n",
      "Average MSE on random: 0.10168596431612968 \n",
      "Average correlation on ChrV: 0.4301213486764387 \n",
      "Average MSE on ChrV: 0.21321986019611358 \n",
      "Average correlation on CN: 0.5484018044118694 \n",
      "Average MSE on CN: 0.15506017059087754\n"
     ]
    }
   ],
   "source": [
    "display_fits(fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2cd633bf9703d9b8d2b7bb6e04b82983774c32d5f891ed1890ee26b779f7466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
