{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "# from process_data import *\n",
    "from train import initialize_model\n",
    "from datasets import *\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch_geometric.explain import Explainer\n",
    "from torch_geometric.explain import ModelConfig\n",
    "from torch_geometric.explain import ExplainerConfig\n",
    "from torch_geometric.explain import DummyExplainer\n",
    "from explain import *\n",
    "\n",
    "import types\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiling_data = DNASeqGraphSeparateEdges(\"/Users/Brody1/Dropbox/Northwestern/DNA_Cyclizability/benchmarks/gnns\", \n",
    "                                       raw_graph_name=\"cycle5\", \n",
    "                                       processed_graph_name=\"cycle5_1_2_3_5_10_15_20_25_30_edge_separate_include_indices\", \n",
    "                                       edge_distances=[1,2,3,5,10,15,20,25,30],\n",
    "                                       include_indices=True)\n",
    "# random_data = DNASeqGraphSeparateEdges(\"/Users/Brody1/Dropbox/Northwestern/DNA_Cyclizability/benchmarks/gnns\", \n",
    "#                                        raw_graph_name=\"cycle3\", \n",
    "#                                        processed_graph_name=\"cycle3_1_2_3_5_10_15_20_25_30_edge_separate_include_indices\", \n",
    "#                                        edge_distances=[1,2,3,5,10,15,20,25,30],\n",
    "#                                        include_indices=True)\n",
    "# yeast_data = DNASeqGraphSeparateEdges(\"/Users/Brody1/Dropbox/Northwestern/DNA_Cyclizability/benchmarks/gnns\", \n",
    "#                                       raw_graph_name=\"cycle1\", \n",
    "#                                       processed_graph_name=\"cycle1_1_2_3_5_10_15_20_25_30_edge_separate_include_indices\", \n",
    "#                                       edge_distances=[1,2,3,5,10,15,20,25,30],\n",
    "#                                       include_indices=True)\n",
    "# chrv_data = DNASeqGraphSeparateEdges(\"/Users/Brody1/Dropbox/Northwestern/DNA_Cyclizability/benchmarks/gnns\", \n",
    "#                                      raw_graph_name=\"cycle6\", \n",
    "#                                      processed_graph_name=\"cycle6_1_2_3_5_10_15_20_25_30_edge_separate_include_indices\", \n",
    "#                                      edge_distances=[1,2,3,5,10,15,20,25,30],\n",
    "#                                      include_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_list = [\n",
    "    torch.Tensor(\n",
    "        [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4])\n",
    "]\n",
    "hyperparameters = {\"mp_hidden_channels\":48, \"mlp_hidden_channels\":128, \"num_layers\":[1], \"num_epochs\":200, \"learning_rate\":0.0001, \"batch_size\":32,\n",
    "                   \"pool_ratios\":[], \"pool_types\":[], \"mlp_num_layers\":2, \"edge_lengths\":[[1,2,3],[5,10,15,20,25,30]], \n",
    "                   \"mp_layer\":GCN, \"jk\":\"lstm\", \"num_nodes\":50, \"max_time\":8*60*60, \"final_dropout\": 0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_str = \"tiling\"\n",
    "\n",
    "if data_str == \"tiling\":\n",
    "    data = tiling_data\n",
    "elif data_str == \"random\":\n",
    "    data = random_data\n",
    "elif data_str == \"yeast\":\n",
    "    data = yeast_data\n",
    "elif data_str == \"chrv\":\n",
    "    data = chrv_data\n",
    "\n",
    "\n",
    "num_obs=len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_list = []\n",
    "std_list = []\n",
    "for i in range(4):\n",
    "    avg_list.append(torch.mean(data.x[:,i]).item())\n",
    "    std_list.append(torch.std(data.x[:,i]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_info = {\"print_every\":1, \"folder_path\":\"/Users/Brody1/Dropbox/Northwestern/DNA_Cyclizability/benchmarks/gnns/\",\n",
    "                \"file_name\": \"edge_separate_1_2_3__5_10_15_20_25_30_with_mlps_2layer_GCN_1layer_48mphc_128mlphc_include_indices_no_final_pooling_02finaldropout_8hr\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model(model_name=\"GraphSAGESeparateEdgeLengthsWithMLPsNoFinalPooling\", num_node_features=data.x.shape[1],\n",
    "                         output_features=data.y.shape[1], hyperparameters=hyperparameters)\n",
    "model.load_state_dict(torch.load(logging_info[\"folder_path\"] + \"models/\" + logging_info[\"file_name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which_graph = np.random.random_integers(0, len(data), num_obs)\n",
    "_, which_graph = torch.topk(data.y.flatten(), k=num_obs)\n",
    "which_graph = np.array(which_graph)\n",
    "# explanation_data = DataLoader(data[which_graph], batch_size=num_obs//10)\n",
    "explanation_data = DataLoader(data[which_graph], batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# predicted_values = []\n",
    "# true_values = []\n",
    "# for explanation_batch in explanation_data:\n",
    "#     predicted_values.append(model(explanation_batch))\n",
    "#     true_values.append(explanation_batch.y)\n",
    "# # np.corrcoef((predicted_values[0].reshape(-1).detach().numpy(), true_values[0].reshape(-1).numpy()))[0,1]\n",
    "\n",
    "# for i in range(len(predicted_values)):\n",
    "#     for j in range(len(predicted_values[i])):\n",
    "#         # print(f\"i: {i}, j: {j}, i+j: {i+j}\")\n",
    "#         print(f\"Index {i*len(predicted_values[i]) + j}:\")\n",
    "#         print(f\"\\t Predicted: {predicted_values[i][j].item()}, Actual: {true_values[i][j].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GraphSAGESeparateEdgeLengths\n",
    "\n",
    "# def forward_until_mlps(model, data):\n",
    "#     x, batch = data.x, data.batch\n",
    "#     network_outputs = []\n",
    "\n",
    "#     for network_idx, edge_length in enumerate(model.edge_lengths):\n",
    "\n",
    "#         # edge_length_attr_name = f\"edge_index_{edge_length}_locations\"\n",
    "#         edge_index_locations = abs(data.edge_index[0,:] - data.edge_index[1,:])==edge_length\n",
    "#         edge_index_locations += (data.edge_index[0,:] - data.edge_index[1,:]) == 0\n",
    "#         # edge_index_locations = getattr(data, edge_length_attr_name, None)\n",
    "#         edge_index = data.edge_index[:,edge_index_locations]\n",
    "#         if edge_index is None:\n",
    "#             raise ValueError(f\"Edge length of {edge_length} not found in data\")\n",
    "        \n",
    "#         current_network = model.edge_length_networks[network_idx]\n",
    "\n",
    "#         x_cur = current_network.convs[0](x=x, edge_index=edge_index, edge_weight=None)\n",
    "#         batch_cur = batch\n",
    "\n",
    "#         x_pooling = []\n",
    "#         x_pooling.append(x_cur)\n",
    "\n",
    "#         pooling_indices = []\n",
    "#         pooling_edge_indices = []\n",
    "\n",
    "#         for i, pooler in enumerate(current_network.poolers):\n",
    "#             if current_network.pool_types[i] == \"ASA\":\n",
    "#                 x_cur, edge_index, edge_weight, batch_cur, index = pooler(\n",
    "#                     x=x_cur, edge_index=edge_index, batch=batch_cur)\n",
    "#             else:\n",
    "#                 x_cur, edge_index, edge_weight, batch_cur, index, score = pooler(\n",
    "#                     x=x_cur, edge_index=edge_index, batch=batch_cur)\n",
    "            \n",
    "#             pooling_indices.append(index)\n",
    "#             pooling_edge_indices.append(edge_index)\n",
    "            \n",
    "#             x_cur = current_network.convs[i+1](x=x_cur, edge_index=edge_index)\n",
    "#             if i < len(current_network.poolers) - 1:\n",
    "#                 x_pooling.append(x_cur)\n",
    "        \n",
    "#         for i, unpooler in enumerate(current_network.unpoolers):\n",
    "#             unpooling_indices = pooling_indices.pop()\n",
    "#             unpooling_edge_index = pooling_edge_indices.pop()\n",
    "#             x_unpooling = torch.zeros(x_pooling[-1].shape)\n",
    "#             x_unpooling[unpooling_indices] = x_cur\n",
    "#             x_unpooling = unpooler(x=x_unpooling, edge_index=unpooling_edge_index)\n",
    "\n",
    "#             x_unpooling = torch.cat((x_unpooling, x_pooling.pop()), dim=1)\n",
    "            \n",
    "#             x_cur = current_network.unpooling_mlps[i](x_unpooling)\n",
    "\n",
    "#         network_outputs.append(current_network.final_mlp(x_cur))\n",
    "\n",
    "#     return torch.cat(network_outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GraphSAGESeparateEdgeLengthsWithMLPs or GraphSAGESeparateEdgeLengthsWithMLPsNoFinalPooling\n",
    "\n",
    "# def forward_until_mlps(model, data):\n",
    "#     x, batch = data.x, data.batch\n",
    "#     network_outputs = []\n",
    "\n",
    "#     network_idx = 0\n",
    "\n",
    "#     for outer_idx, edge_length_list in enumerate(model.edge_lengths):\n",
    "#         inner_loop_network_outputs = []\n",
    "#         for edge_length in edge_length_list:\n",
    "\n",
    "#             # edge_length_attr_name = f\"edge_index_{edge_length}_locations\"\n",
    "#             edge_index_locations = abs(data.edge_index[0,:] - data.edge_index[1,:])==edge_length\n",
    "#             edge_index_locations += (data.edge_index[0,:] - data.edge_index[1,:]) == 0\n",
    "#             # edge_index_locations = getattr(data, edge_length_attr_name, None)\n",
    "#             edge_index = data.edge_index[:,edge_index_locations]\n",
    "#             if edge_index is None:\n",
    "#                 raise ValueError(f\"Edge length of {edge_length} not found in data\")\n",
    "            \n",
    "#             current_network = model.edge_length_networks[network_idx]\n",
    "\n",
    "#             x_cur = current_network.convs[0](x=x, edge_index=edge_index, edge_weight=None)\n",
    "#             batch_cur = batch\n",
    "\n",
    "#             x_pooling = []\n",
    "#             x_pooling.append(x_cur)\n",
    "\n",
    "#             pooling_indices = []\n",
    "#             pooling_edge_indices = []\n",
    "\n",
    "#             for i, pooler in enumerate(current_network.poolers):\n",
    "#                 if current_network.pool_types[i] == \"ASA\":\n",
    "#                     x_cur, edge_index, edge_weight, batch_cur, index = pooler(\n",
    "#                         x=x_cur, edge_index=edge_index, batch=batch_cur)\n",
    "#                 else:\n",
    "#                     x_cur, edge_index, edge_weight, batch_cur, index, score = pooler(\n",
    "#                         x=x_cur, edge_index=edge_index, batch=batch_cur)\n",
    "                \n",
    "#                 pooling_indices.append(index)\n",
    "#                 pooling_edge_indices.append(edge_index)\n",
    "                \n",
    "#                 x_cur = current_network.convs[i+1](x=x_cur, edge_index=edge_index)\n",
    "#                 if i < len(current_network.poolers) - 1:\n",
    "#                     x_pooling.append(x_cur)\n",
    "            \n",
    "#             for i, unpooler in enumerate(current_network.unpoolers):\n",
    "#                 unpooling_indices = pooling_indices.pop()\n",
    "#                 unpooling_edge_index = pooling_edge_indices.pop()\n",
    "#                 x_unpooling = torch.zeros(x_pooling[-1].shape)\n",
    "#                 x_unpooling[unpooling_indices] = x_cur\n",
    "#                 x_unpooling = unpooler(x=x_unpooling, edge_index=unpooling_edge_index)\n",
    "\n",
    "#                 x_unpooling = torch.cat((x_unpooling, x_pooling.pop()), dim=1)\n",
    "                \n",
    "#                 x_cur = current_network.unpooling_mlps[i](x_unpooling)\n",
    "\n",
    "#             inner_loop_network_outputs.append(current_network.final_mlp(x_cur))\n",
    "\n",
    "#             network_idx += 1\n",
    "\n",
    "#         network_outputs.append(torch.cat(inner_loop_network_outputs, dim=1))\n",
    "\n",
    "#     return torch.cat(network_outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphSAGESeparateEdgeLengthsWithMLPs or GraphSAGESeparateEdgeLengthsWithMLPsNoFinalPooling\n",
    "\n",
    "def forward_until_mlps(model, data):\n",
    "    x, batch = data.x, data.batch\n",
    "    network_outputs = []\n",
    "\n",
    "    network_idx = 0\n",
    "\n",
    "    for outer_idx, edge_length_list in enumerate(model.edge_lengths):\n",
    "        inner_loop_network_outputs = []\n",
    "        for edge_length in edge_length_list:\n",
    "\n",
    "            # edge_length_attr_name = f\"edge_index_{edge_length}_locations\"\n",
    "            edge_index_locations = abs(data.edge_index[0,:] - data.edge_index[1,:])==edge_length\n",
    "            edge_index_locations += (data.edge_index[0,:] - data.edge_index[1,:]) == 0\n",
    "            # edge_index_locations = getattr(data, edge_length_attr_name, None)\n",
    "            edge_index = data.edge_index[:,edge_index_locations]\n",
    "            if edge_index is None:\n",
    "                raise ValueError(f\"Edge length of {edge_length} not found in data\")\n",
    "            \n",
    "            current_network = model.edge_length_networks[network_idx]\n",
    "\n",
    "            x_cur = current_network.convs[0](x=x, edge_index=edge_index, edge_weight=None)\n",
    "            batch_cur = batch\n",
    "\n",
    "            x_pooling = []\n",
    "            x_pooling.append(x_cur)\n",
    "\n",
    "            pooling_indices = []\n",
    "            pooling_edge_indices = []\n",
    "\n",
    "            for i, pooler in enumerate(current_network.poolers):\n",
    "                if current_network.pool_types[i] == \"ASA\":\n",
    "                    x_cur, edge_index, edge_weight, batch_cur, index = pooler(\n",
    "                        x=x_cur, edge_index=edge_index, batch=batch_cur)\n",
    "                else:\n",
    "                    x_cur, edge_index, edge_weight, batch_cur, index, score = pooler(\n",
    "                        x=x_cur, edge_index=edge_index, batch=batch_cur)\n",
    "                \n",
    "                pooling_indices.append(index)\n",
    "                pooling_edge_indices.append(edge_index)\n",
    "                \n",
    "                x_cur = current_network.convs[i+1](x=x_cur, edge_index=edge_index)\n",
    "                if i < len(current_network.poolers) - 1:\n",
    "                    x_pooling.append(x_cur)\n",
    "            \n",
    "            for i, unpooler in enumerate(current_network.unpoolers):\n",
    "                unpooling_indices = pooling_indices.pop()\n",
    "                unpooling_edge_index = pooling_edge_indices.pop()\n",
    "                x_unpooling = torch.zeros(x_pooling[-1].shape)\n",
    "                x_unpooling[unpooling_indices] = x_cur\n",
    "                x_unpooling = unpooler(x=x_unpooling, edge_index=unpooling_edge_index)\n",
    "\n",
    "                x_unpooling = torch.cat((x_unpooling, x_pooling.pop()), dim=1)\n",
    "                \n",
    "                x_cur = current_network.unpooling_mlps[i](x_unpooling)\n",
    "\n",
    "            inner_loop_network_outputs.append(current_network.final_mlp(x_cur))\n",
    "\n",
    "            network_idx += 1\n",
    "        \n",
    "        network_outputs.append(torch.cat(inner_loop_network_outputs, dim=1))\n",
    "\n",
    "    return torch.cat(network_outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling_x_by_cluster(x, batch, cluster_list):\n",
    "    for cluster in cluster_list:\n",
    "        cur_cluster = cluster.repeat(len(batch)//len(cluster)) + (torch.arange(len(batch))//len(cluster))*(torch.max(cluster)+1)\n",
    "        x, batch = max_pool_x(cur_cluster, x, batch)\n",
    "    return global_mean_pool(x, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_length_final_mlps(x, model, hidden_channels):\n",
    "    outer_loop_network_outputs = []\n",
    "    prev_idx = 0\n",
    "    for outer_idx, edge_length_list in enumerate(model.edge_lengths):\n",
    "        x_cur = x[:,prev_idx:(prev_idx+hidden_channels*len(edge_length_list))]\n",
    "        outer_loop_network_outputs.append(model.edge_lengths_final_mlps[outer_idx](x_cur))\n",
    "        prev_idx += hidden_channels*len(edge_length_list)\n",
    "    return torch.cat(outer_loop_network_outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_mlps_no_pooling(x, model, batch):\n",
    "    x = model.final_mlp_1(x)\n",
    "    x = x.reshape(torch.max(batch)+1, -1)\n",
    "    x = model.final_mlp_2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_2 = tg_nn.Sequential('x, edge_index, batch, cluster_list', [(model.final_mlp_1, 'x -> x'),\n",
    "#                                                     (max_pooling_x_by_cluster, 'x, batch, cluster_list -> x'),\n",
    "#                                                     (model.final_mlp_2, 'x -> x')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_2 = tg_nn.Sequential('x, edge_index, batch, cluster_list, orig_model, hidden_channels', [\n",
    "#     (edge_length_final_mlps, 'x, orig_model, hidden_channels -> x'), \n",
    "#     (model.final_mlp_1, 'x -> x'),\n",
    "#     (max_pooling_x_by_cluster, 'x, batch, cluster_list -> x'),\n",
    "#     (model.final_mlp_2, 'x -> x')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = tg_nn.Sequential('x, batch, orig_model, hidden_channels', [\n",
    "    (edge_length_final_mlps, 'x, orig_model, hidden_channels -> x'), \n",
    "    (final_mlps_no_pooling, 'x, orig_model, batch -> x')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelConfig(mode=\"regression\", task_level=\"graph\", return_type=\"raw\")\n",
    "explainer_config = ExplainerConfig(explanation_type=\"phenomenon\", node_mask_type=\"attributes\")\n",
    "separate_network_explainer = CustomExplainer2(lr = .1)\n",
    "separate_network_explainer.connect(explainer_config=explainer_config, model_config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_network_explainer.epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1, 5, 10, 15\n",
    "\n",
    "# def explain_message(self, inputs: torch.Tensor, size_i: int) -> torch.Tensor:\n",
    "#         # NOTE Replace this method in custom explainers per message-passing\n",
    "#         # layer to customize how messages shall be explained, e.g., via:\n",
    "#         # conv.explain_message = explain_message.__get__(conv, MessagePassing)\n",
    "#         # see stackoverflow.com: 394770/override-a-method-at-instance-level\n",
    "\n",
    "#      edge_mask = self._edge_mask\n",
    "\n",
    "#      if edge_mask is None:\n",
    "#           raise ValueError(f\"Could not find a pre-defined 'edge_mask' as \"\n",
    "#                          f\"part of {self.__class__.__name__}.\")\n",
    "\n",
    "#      if self._apply_sigmoid:\n",
    "#           edge_mask = edge_mask.sigmoid()\n",
    "\n",
    "#      # batch_size = num_obs\n",
    "#      batch_size = 1\n",
    "#      if inputs.size(self.node_dim) == 148*batch_size:\n",
    "#           # Edge length 1\n",
    "#           edge_mask = edge_mask[0:98*batch_size]\n",
    "#      elif inputs.size(self.node_dim) == 140*batch_size:\n",
    "#           # Edge length 5\n",
    "#           edge_mask = edge_mask[98*batch_size:188*batch_size]\n",
    "#      elif inputs.size(self.node_dim) == 130*batch_size:\n",
    "#           # Edge length 10\n",
    "#           edge_mask = edge_mask[188*batch_size:268*batch_size]\n",
    "#      elif inputs.size(self.node_dim) == 120*batch_size:\n",
    "#           # Edge length 15\n",
    "#           edge_mask = edge_mask[268*batch_size:338*batch_size]\n",
    "\n",
    "#      # Some ops add self-loops to `edge_index`. We need to do the same for\n",
    "#      # `edge_mask` (but do not train these entries).\n",
    "#      if inputs.size(self.node_dim) != edge_mask.size(0):\n",
    "#           # edge_mask = edge_mask[self._loop_mask]\n",
    "#           loop = edge_mask.new_ones(size_i)\n",
    "#           edge_mask = torch.cat([edge_mask, loop], dim=0)\n",
    "#      if inputs.size(self.node_dim) != edge_mask.size(0):\n",
    "#           print(f\"\\t Inputs: {inputs.shape} \\n \\t Edge mask: {edge_mask.shape}\")\n",
    "#           assert inputs.size(self.node_dim) == edge_mask.size(0)\n",
    "\n",
    "#      size = [1] * inputs.dim()\n",
    "#      size[self.node_dim] = -1\n",
    "#      return inputs * edge_mask.view(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1,2,3,5,10,15,20,25,30\n",
    "\n",
    "def explain_message(self, inputs: torch.Tensor, size_i: int) -> torch.Tensor:\n",
    "        # NOTE Replace this method in custom explainers per message-passing\n",
    "        # layer to customize how messages shall be explained, e.g., via:\n",
    "        # conv.explain_message = explain_message.__get__(conv, MessagePassing)\n",
    "        # see stackoverflow.com: 394770/override-a-method-at-instance-level\n",
    "\n",
    "    edge_mask = self._edge_mask\n",
    "\n",
    "    if edge_mask is None:\n",
    "        raise ValueError(f\"Could not find a pre-defined 'edge_mask' as \"\n",
    "                         f\"part of {self.__class__.__name__}.\")\n",
    "\n",
    "    if self._apply_sigmoid:\n",
    "        edge_mask = edge_mask.sigmoid()\n",
    "\n",
    "    # batch_size = num_obs//10\n",
    "    batch_size = 1\n",
    "    if inputs.size(self.node_dim) == 148*batch_size:\n",
    "        # Edge length 1 (98)\n",
    "        edge_mask = edge_mask[0:98*batch_size]\n",
    "    if inputs.size(self.node_dim) == 146*batch_size:\n",
    "        # Edge length 2 (96)\n",
    "        edge_mask = edge_mask[98*batch_size:194*batch_size]\n",
    "    if inputs.size(self.node_dim) == 144*batch_size:\n",
    "        # Edge length 3 (94)\n",
    "        edge_mask = edge_mask[194*batch_size:288*batch_size]\n",
    "    elif inputs.size(self.node_dim) == 140*batch_size:\n",
    "        # Edge length 5 (90)\n",
    "        edge_mask = edge_mask[288*batch_size:378*batch_size]\n",
    "    elif inputs.size(self.node_dim) == 130*batch_size:\n",
    "        # Edge length 10 (80)\n",
    "        edge_mask = edge_mask[378*batch_size:458*batch_size]\n",
    "    elif inputs.size(self.node_dim) == 120*batch_size:\n",
    "        # Edge length 15 (70)\n",
    "        edge_mask = edge_mask[458*batch_size:528*batch_size]\n",
    "    if inputs.size(self.node_dim) == 110*batch_size:\n",
    "        # Edge length 20 (60)\n",
    "        edge_mask = edge_mask[528*batch_size:588*batch_size]\n",
    "    if inputs.size(self.node_dim) == 100*batch_size:\n",
    "        # Edge length 25 (50)\n",
    "        edge_mask = edge_mask[588*batch_size:638*batch_size]\n",
    "    if inputs.size(self.node_dim) == 90*batch_size:\n",
    "        # Edge length 30 (40)\n",
    "        edge_mask = edge_mask[638*batch_size:678*batch_size]\n",
    "\n",
    "     # Some ops add self-loops to `edge_index`. We need to do the same for\n",
    "     # `edge_mask` (but do not train these entries).\n",
    "    if inputs.size(self.node_dim) != edge_mask.size(0):\n",
    "        # edge_mask = edge_mask[self._loop_mask]\n",
    "        loop = edge_mask.new_ones(size_i)\n",
    "        edge_mask = torch.cat([edge_mask, loop], dim=0)\n",
    "    if inputs.size(self.node_dim) != edge_mask.size(0):\n",
    "        print(f\"\\t Inputs: {inputs.shape} \\n \\t Edge mask: {edge_mask.shape}\")\n",
    "        assert inputs.size(self.node_dim) == edge_mask.size(0)\n",
    "\n",
    "    size = [1] * inputs.dim()\n",
    "    size[self.node_dim] = -1\n",
    "    return inputs * edge_mask.view(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, module in enumerate(model.modules()):\n",
    "    if hasattr(module, \"explain_message\"):\n",
    "        # funcType = type(module.explain_message)\n",
    "        # module.explain_message = funcType(explain_message2, module)\n",
    "        module.explain_message = types.MethodType(explain_message, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate_network_explanations=[]\n",
    "# for i, explanation_batch in enumerate(explanation_data):\n",
    "#     until_mlps_out = forward_until_mlps(model, explanation_batch)\n",
    "\n",
    "#     explanation_batch.x = until_mlps_out\n",
    "#     model_2.eval()\n",
    "#     separate_network_explanations.append(separate_network_explainer(model=model_2, data=explanation_batch, \n",
    "#                                                                     edge_index=torch.Tensor().reshape(2,-1), \n",
    "#                                                                     target=explanation_batch.y,\n",
    "#                                                                     batch=explanation_batch.batch, \n",
    "#                                                                     cluster_list=model.cluster_list,\n",
    "#                                                                     orig_model=model,\n",
    "#                                                                     hidden_channels=hyperparameters[\"mlp_hidden_channels\"]))\n",
    "#     print(f\"Completed batch {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate_network_explanations=[]\n",
    "# for i, explanation_batch in enumerate(explanation_data):\n",
    "#     until_mlps_out = forward_until_mlps(model, explanation_batch)\n",
    "\n",
    "#     explanation_batch.x = until_mlps_out\n",
    "#     model_2.eval()\n",
    "#     separate_network_explanations.append(separate_network_explainer(model=model_2, data=explanation_batch, \n",
    "#                                                                     target=explanation_batch.y,\n",
    "#                                                                     batch=explanation_batch.batch, \n",
    "#                                                                     orig_model=model,\n",
    "#                                                                     hidden_channels=hyperparameters[\"mlp_hidden_channels\"]))\n",
    "#     print(f\"Completed batch {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_channels = hyperparameters[\"mlp_hidden_channels\"]\n",
    "# all_network_contributions = []\n",
    "# for network_explanations in separate_network_explanations:\n",
    "#     # temp_explain = separate_network_explanations[0]\n",
    "#     total_contribution = torch.sum(network_explanations.node_mask)\n",
    "#     cur_network_contributions = []\n",
    "#     for i in range(network_explanations.node_mask.shape[1]//hyperparameters[\"mlp_hidden_channels\"]):\n",
    "#         cur_network_contribution = torch.sum(network_explanations.node_mask[:,i*hidden_channels:(i+1)*hidden_channels])\n",
    "#         cur_network_contributions.append(cur_network_contribution)\n",
    "#         # print(f\"Contribution from network corresponding to edge length {hyperparameters['edge_lengths'][i]}: {cur_network_contribution/total_contribution}\")\n",
    "#     all_network_contributions.append(cur_network_contributions)\n",
    "# avg_network_contributions = []\n",
    "# # for i in range(len(all_network_contributions[0])):\n",
    "# #     avg_network_contributions.append(np.mean([all_network_contributions[j][i]/sum(all_network_contributions[j]) for j in range(len(all_network_contributions))]))\n",
    "# #     print(f\"Average contribution from network corresponding to edge length {hyperparameters['edge_lengths'][i]}: {avg_network_contributions[i]}\")\n",
    "# inner_idx_addition = 0\n",
    "# for outer_idx, edge_length_list in enumerate(hyperparameters[\"edge_lengths\"]):\n",
    "#     for inner_idx, edge_length in enumerate(edge_length_list):\n",
    "#         avg_network_contributions.append(np.mean([all_network_contributions[j][inner_idx_addition+inner_idx]/sum(all_network_contributions[j]) for j in range(len(all_network_contributions))]))\n",
    "#         print(f\"Average contribution from network corresponding to edge length {edge_length}: {avg_network_contributions[-1]}\")\n",
    "#     inner_idx_addition += len(edge_length_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelConfig(mode=\"regression\", task_level=\"graph\", return_type=\"raw\")\n",
    "explainer_config = ExplainerConfig(explanation_type=\"model\", edge_mask_type=\"object\")\n",
    "# explainer_config = ExplainerConfig(explanation_type=\"phenomenon\", node_mask_type=\"attributes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_explainer = CustomExplainer(lr = 0.1)\n",
    "test_explainer.connect(explainer_config=explainer_config, model_config=model_config)\n",
    "test_explainer.epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_explanations=[]\n",
    "# for i, explanation_batch in enumerate(explanation_data):\n",
    "#     test_explanations.append(test_explainer(model=model, data=explanation_batch, target=explanation_batch.y))\n",
    "#     if (i+1) % 100 == 0:\n",
    "#         torch.save(test_explanations, logging_info[\"folder_path\"] + \"explanations/\" + logging_info[\"file_name\"])\n",
    "#         print(f\"Completed batch {i}\")\n",
    "# torch.save(test_explanations, logging_info[\"folder_path\"] + \"explanations/\" + logging_info[\"file_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_explanations = torch.load(logging_info[\"folder_path\"] + \"explanations/\" + logging_info[\"file_name\"] + \"_\" + data_str + \"_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_explanations2 = torch.load(logging_info[\"folder_path\"] + \"explanations/\" + logging_info[\"file_name\"] + \"_\" + data_str + \"_TEMPORARY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_explanations = test_explanations + test_explanations2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(test_explanations) != num_obs:\n",
    "    print(f\"Resetting num obs: {len(test_explanations)}\")\n",
    "    num_obs = len(test_explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First find the average contribution for each batch:\n",
    "batch_wise_processed_edge_weight_directed = []\n",
    "for test_explanation in test_explanations:\n",
    "    # Average of all weights in test_explanation\n",
    "    # batch_wise_processed_edge_weight_directed.append(torch.mean(\n",
    "    #     torch.stack([test_explanation.edge_mask[i*338:(i+1)*338] for i in range(test_explanation.edge_mask.shape[0]//338)]),\n",
    "    #     dim=0))\n",
    "    batch_wise_processed_edge_weight_directed.append(torch.mean(\n",
    "        torch.stack([test_explanation.edge_mask[i*678:(i+1)*678] for i in range(test_explanation.edge_mask.shape[0]//678)]),\n",
    "        dim=0))\n",
    "\n",
    "batch_wise_processed_edge_weight_directed = torch.cat(batch_wise_processed_edge_weight_directed)\n",
    "\n",
    "# processed_edge_weight_directed = torch.mean(\n",
    "#     torch.stack([batch_wise_processed_edge_weight_directed[i*338:(i+1)*338] for i in range(batch_wise_processed_edge_weight_directed.shape[0]//338)]),\n",
    "#     dim=0)\n",
    "processed_edge_weight_directed = torch.mean(\n",
    "    torch.stack([batch_wise_processed_edge_weight_directed[i*678:(i+1)*678] for i in range(batch_wise_processed_edge_weight_directed.shape[0]//678)]),\n",
    "    dim=0)\n",
    "\n",
    "processed_weighted_edges_directed = [(test_explanations[0].edge_index[0, i].item(), \n",
    "                                      test_explanations[0].edge_index[1, i].item(), \n",
    "                                      processed_edge_weight_directed[i].item()\n",
    "                                      ) for i in range(len(processed_edge_weight_directed))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_edge_weight = []\n",
    "undirected_edges = []\n",
    "prev_start = 0\n",
    "# for i, edge_length in enumerate(hyperparameters[\"edge_lengths\"]):\n",
    "#     processed_edge_weight.append(torch.mean(torch.stack((\n",
    "#         processed_edge_weight_directed[prev_start:prev_start+(50-edge_length)],\n",
    "#         processed_edge_weight_directed[prev_start+(50-edge_length):prev_start+(50-edge_length)*2])),\n",
    "#         dim=0))\n",
    "#     undirected_edges.append(test_explanations[0].edge_index[:,prev_start:prev_start+(50-edge_length)])\n",
    "#     prev_start += (50-edge_length)*2\n",
    "i = 0\n",
    "for edge_length_list in hyperparameters[\"edge_lengths\"]:\n",
    "    for edge_length in edge_length_list:\n",
    "        processed_edge_weight.append(torch.mean(torch.stack((\n",
    "            processed_edge_weight_directed[prev_start:prev_start+(50-edge_length)],\n",
    "            processed_edge_weight_directed[prev_start+(50-edge_length):prev_start+(50-edge_length)*2])),\n",
    "            dim=0))\n",
    "        undirected_edges.append(test_explanations[0].edge_index[:,prev_start:prev_start+(50-edge_length)])\n",
    "        prev_start += (50-edge_length)*2\n",
    "        i+=1\n",
    "processed_edge_weight = torch.cat(processed_edge_weight)\n",
    "undirected_edges = torch.cat(undirected_edges, dim=1)\n",
    "\n",
    "processed_weighted_edges = [(undirected_edges[0, i].item(), \n",
    "                             undirected_edges[1, i].item(), \n",
    "                             processed_edge_weight[i].item()\n",
    "                             ) for i in range(len(processed_edge_weight))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_weighted_edges = []\n",
    "\n",
    "# prev_start = 0\n",
    "# for i, edge_length in enumerate(hyperparameters[\"edge_lengths\"]):\n",
    "#     print(\"This:\")\n",
    "#     print(test_explanations[0].edge_index[:,prev_start:prev_start+(50-edge_length)])\n",
    "#     print(\"Should be the opposite of this:\")\n",
    "#     print(test_explanations[0].edge_index[:,prev_start+(50-edge_length):prev_start+(50-edge_length)*2])\n",
    "#     prev_start += (50-edge_length)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,12))\n",
    "\n",
    "g = nx.Graph()\n",
    "\n",
    "g.add_weighted_edges_from(processed_weighted_edges)\n",
    "plotting_pos = nx.circular_layout(g)\n",
    "\n",
    "edges,weights = zip(*nx.get_edge_attributes(g,'weight').items())\n",
    "vmin = min(weights)\n",
    "vmax = max(weights)\n",
    "cmap=plt.cm.RdYlBu_r\n",
    "plotting_labels = {node: str(node) for node in g.nodes()}\n",
    "\n",
    "widths = [weight*20 for weight in weights]\n",
    "alphas = [weight*3 for weight in weights]\n",
    "\n",
    "nx.draw_networkx_labels(g, plotting_pos, labels=plotting_labels)\n",
    "nx.draw_networkx_nodes(g,plotting_pos,label=plotting_labels)\n",
    "edges = nx.draw_networkx_edges(g,plotting_pos,edge_color=weights,width=widths,\n",
    "                               edge_cmap=cmap, alpha=alphas)\n",
    "cbar = plt.colorbar(edges)\n",
    "cbar.set_label(\"edge importance\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average score for edge length 1: {torch.mean(processed_edge_weight[0:49]).item()}\")\n",
    "print(f\"Average score for edge length 2: {torch.mean(processed_edge_weight[49:97]).item()}\")\n",
    "print(f\"Average score for edge length 3: {torch.mean(processed_edge_weight[97:144]).item()}\")\n",
    "print(f\"Average score for edge length 5: {torch.mean(processed_edge_weight[144:189]).item()}\")\n",
    "print(f\"Average score for edge length 10: {torch.mean(processed_edge_weight[189:229]).item()}\")\n",
    "print(f\"Average score for edge length 15: {torch.mean(processed_edge_weight[229:264]).item()}\")\n",
    "print(f\"Average score for edge length 20: {torch.mean(processed_edge_weight[264:294]).item()}\")\n",
    "print(f\"Average score for edge length 25: {torch.mean(processed_edge_weight[294:319]).item()}\")\n",
    "print(f\"Average score for edge length 30: {torch.mean(processed_edge_weight[319:339]).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=3\n",
    "\n",
    "# If all in one batch:\n",
    "# processed_edge_weight_directed_j = test_explanations[0].edge_mask[j*338:(j+1)*338]\n",
    "# processed_edge_weight_directed_j = test_explanations[0].edge_mask[j*678:(j+1)*678]\n",
    "\n",
    "# If all in separate batches:\n",
    "processed_edge_weight_directed_j = test_explanations[j].edge_mask\n",
    "\n",
    "# If 10 batches:\n",
    "# processed_edge_weight_directed_j = test_explanations[j//10].edge_mask[(j//10)*338:((j//10)+1)*338]\n",
    "# processed_edge_weight_directed_j = test_explanations[j//10].edge_mask[(j//10)*678:((j//10)+1)*678]\n",
    "\n",
    "processed_weighted_edges_directed_j = [(test_explanations[0].edge_index[0, i].item(), \n",
    "                                        test_explanations[0].edge_index[1, i].item(), \n",
    "                                        processed_edge_weight_directed_j[i].item()\n",
    "                                        ) for i in range(len(processed_edge_weight_directed_j))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_edge_weight_j = []\n",
    "undirected_edges_j = []\n",
    "prev_start = 0\n",
    "# for i, edge_length in enumerate(hyperparameters[\"edge_lengths\"]):\n",
    "#     processed_edge_weight_j.append(torch.mean(torch.stack((\n",
    "#         processed_edge_weight_directed_j[prev_start:prev_start+(50-edge_length)],\n",
    "#         processed_edge_weight_directed_j[prev_start+(50-edge_length):prev_start+(50-edge_length)*2])),\n",
    "#         dim=0))\n",
    "#     undirected_edges_j.append(test_explanations[0].edge_index[:,prev_start:prev_start+(50-edge_length)])\n",
    "#     prev_start += (50-edge_length)*2\n",
    "i=0\n",
    "for edge_length_list in hyperparameters[\"edge_lengths\"]:\n",
    "    for edge_length in edge_length_list:\n",
    "        processed_edge_weight_j.append(torch.mean(torch.stack((\n",
    "            processed_edge_weight_directed_j[prev_start:prev_start+(50-edge_length)],\n",
    "            processed_edge_weight_directed_j[prev_start+(50-edge_length):prev_start+(50-edge_length)*2])),\n",
    "            dim=0))\n",
    "        undirected_edges_j.append(test_explanations[0].edge_index[:,prev_start:prev_start+(50-edge_length)])\n",
    "        prev_start += (50-edge_length)*2\n",
    "        i+=1\n",
    "processed_edge_weight_j = torch.cat(processed_edge_weight_j)\n",
    "undirected_edges_j = torch.cat(undirected_edges_j, dim=1)\n",
    "\n",
    "processed_weighted_edges_j = [(undirected_edges_j[0, i].item(), \n",
    "                               undirected_edges_j[1, i].item(), \n",
    "                               processed_edge_weight_j[i].item()\n",
    "                               ) for i in range(len(processed_edge_weight_j))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_weighted_edges_j = [edge if edge[2] > 0.5 else (edge[0], edge[1], 0) for edge in processed_weighted_edges_j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,12))\n",
    "\n",
    "g = nx.Graph()\n",
    "\n",
    "g.add_weighted_edges_from(processed_weighted_edges_j)\n",
    "plotting_pos = nx.circular_layout(g)\n",
    "\n",
    "edges,weights = zip(*nx.get_edge_attributes(g,'weight').items())\n",
    "vmin = min(weights)\n",
    "vmax = max(weights)\n",
    "cmap=plt.cm.RdYlBu_r\n",
    "\n",
    "widths = [weight*5 for weight in weights]\n",
    "alphas = [weight for weight in weights]\n",
    "code = {0:\"A\", 1:\"C\", 2:\"G\", 3:\"T\"}\n",
    "plotting_label_values = []\n",
    "for i, intval in enumerate((data[which_graph][j].x[:,:4] == 1).nonzero(as_tuple=True)[1]):\n",
    "    plotting_label_values.append(code[intval.item()])\n",
    "plotting_labels = {node: plotting_label_values[i] for i,node in enumerate(g.nodes())}\n",
    "\n",
    "node_color_dict = {\"A\": \"red\", \"C\": \"green\", \"G\": \"blue\", \"T\": \"orange\"}\n",
    "node_colors = []\n",
    "for label in plotting_label_values:\n",
    "    node_colors.append(node_color_dict[label])\n",
    "\n",
    "nx.draw_networkx_labels(g, plotting_pos, labels=plotting_labels)\n",
    "nx.draw_networkx_nodes(g,plotting_pos,label=plotting_labels, node_color=node_colors)\n",
    "edges = nx.draw_networkx_edges(g,plotting_pos,edge_color=weights,width=widths,\n",
    "                               edge_cmap=cmap, alpha=alphas)\n",
    "cbar = plt.colorbar(edges)\n",
    "cbar.set_label(\"edge importance\")\n",
    "plt.axis('off')\n",
    "plt.title(f\"Sequence number {j}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_weighted_edges_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_labels = [\"A\", \"C\", \"G\", \"T\"]\n",
    "# for position in range(50):\n",
    "#     feature_labels.append(f\"Position {position} index\")\n",
    "# test_explanations[0].visualize_feature_importance(feat_labels=feature_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmeans_pytorch import kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_input = batch_wise_processed_edge_weight_directed.reshape(len(test_explanations), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 25\n",
    "cluster_ids_x, cluster_centers = kmeans(X=kmeans_input, num_clusters=num_clusters, distance='euclidean', tol=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_processed_edge_weight_directed_list = []\n",
    "kmeans_processed_weighted_edges_directed_list = []\n",
    "for j in range(num_clusters):\n",
    "    cur_cluster_processed_edge_weight_directed = kmeans_input[cluster_ids_x == j]\n",
    "    kmeans_processed_edge_weight_directed_list.append(\n",
    "        torch.mean(cur_cluster_processed_edge_weight_directed, dim=0)\n",
    "    )\n",
    "\n",
    "    kmeans_processed_weighted_edges_directed_list.append(\n",
    "        [(test_explanations[0].edge_index[0, i].item(), \n",
    "          test_explanations[0].edge_index[1, i].item(), \n",
    "          kmeans_processed_edge_weight_directed_list[-1][i].item()\n",
    "          ) for i in range(len(kmeans_processed_edge_weight_directed_list[-1]))]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_processed_edge_weight_list = []\n",
    "kmeans_processed_weighted_edges_list = []\n",
    "\n",
    "# for i, edge_length in enumerate(hyperparameters[\"edge_lengths\"]):\n",
    "#     processed_edge_weight.append(torch.mean(torch.stack((\n",
    "#         processed_edge_weight_directed[prev_start:prev_start+(50-edge_length)],\n",
    "#         processed_edge_weight_directed[prev_start+(50-edge_length):prev_start+(50-edge_length)*2])),\n",
    "#         dim=0))\n",
    "#     undirected_edges.append(test_explanations[0].edge_index[:,prev_start:prev_start+(50-edge_length)])\n",
    "#     prev_start += (50-edge_length)*2\n",
    "for cluster_id in range(num_clusters):\n",
    "    i = 0\n",
    "    prev_start = 0\n",
    "    kmeans_processed_edge_weight_list.append([])\n",
    "    undirected_edges = []\n",
    "    for edge_length_list in hyperparameters[\"edge_lengths\"]:\n",
    "        for edge_length in edge_length_list:\n",
    "            kmeans_processed_edge_weight_list[-1].append(torch.mean(torch.stack((\n",
    "                kmeans_processed_edge_weight_directed_list[cluster_id][prev_start:prev_start+(50-edge_length)],\n",
    "                kmeans_processed_edge_weight_directed_list[cluster_id][prev_start+(50-edge_length):prev_start+(50-edge_length)*2])),\n",
    "                dim=0))\n",
    "            undirected_edges.append(test_explanations[0].edge_index[:,prev_start:prev_start+(50-edge_length)])\n",
    "            prev_start += (50-edge_length)*2\n",
    "            i+=1\n",
    "    kmeans_processed_edge_weight_list[-1] = torch.cat(kmeans_processed_edge_weight_list[-1])\n",
    "    undirected_edges = torch.cat(undirected_edges, dim=1)\n",
    "\n",
    "    kmeans_processed_weighted_edges_list.append(\n",
    "        [(undirected_edges[0, i].item(), \n",
    "          undirected_edges[1, i].item(), \n",
    "          kmeans_processed_edge_weight_list[-1][i].item()\n",
    "          ) for i in range(len(kmeans_processed_edge_weight_list[-1]))]\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model(model_name=\"GraphSAGESeparateEdgeLengthsWithMLPsNoFinalPooling\", num_node_features=data.x.shape[1],\n",
    "                         output_features=data.y.shape[1], hyperparameters=hyperparameters)\n",
    "model.load_state_dict(torch.load(logging_info[\"folder_path\"] + \"models/\" + logging_info[\"file_name\"]))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [-3.5, 3.5]\n",
    "y = [-3.5, 3.5]\n",
    "code = {0:\"A\", 1:\"C\", 2:\"G\", 3:\"T\"}\n",
    "\n",
    "true_y_by_cluster = []\n",
    "pred_y_by_cluster = []\n",
    "\n",
    "colors = [\"b\",\"g\",\"r\",\"c\",\"m\",\"y\",\"tab:blue\",\"tab:orange\",\"tab:green\",\"tab:red\",\"tab:purple\",\"tab:brown\",\"tab:brown\",\n",
    "          \"tab:pink\",\"tab:gray\",\"lightcoral\",\"peru\",\"gold\",\"greenyellow\",\"aquamarine\",\"darkslategray\",\"deepskyblue\",\n",
    "          \"navy\",\"deeppink\",\"lavender\"]\n",
    "\n",
    "for cluster_id in range(num_clusters):\n",
    "\n",
    "    # Network plot with weighted edges:\n",
    "    g = nx.Graph()\n",
    "\n",
    "    g.add_weighted_edges_from(kmeans_processed_weighted_edges_list[cluster_id])\n",
    "    plotting_pos = nx.circular_layout(g)\n",
    "\n",
    "    edges,weights = zip(*nx.get_edge_attributes(g,'weight').items())\n",
    "    vmin = min(weights)\n",
    "    vmax = max(weights)\n",
    "    cmap=plt.cm.RdYlBu_r\n",
    "    plotting_labels = {node: str(node) for node in g.nodes()}\n",
    "\n",
    "    widths = [weight*20 for weight in weights]\n",
    "    alphas = [weight for weight in weights]\n",
    "\n",
    "    plt.figure(figsize=(12,10))\n",
    "    nx.draw_networkx_labels(g, plotting_pos, labels=plotting_labels)\n",
    "    nx.draw_networkx_nodes(g,plotting_pos,label=plotting_labels)\n",
    "    \n",
    "    edges = nx.draw_networkx_edges(g,plotting_pos,edge_color=weights,width=widths,\n",
    "                                   edge_cmap=cmap, alpha=alphas)\n",
    "    cbar = plt.colorbar(edges)\n",
    "    cbar.set_label(\"edge importance\")\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Cluster number {cluster_id} ({kmeans_input[cluster_ids_x == cluster_id].shape[0]} sequences)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Network plot with most common nucleotide by position:\n",
    "    cur_cluster_data = data[which_graph[0:len(test_explanations)][cluster_ids_x == cluster_id]]\n",
    "\n",
    "    plotting_label_array = np.zeros((len(cur_cluster_data), 50))\n",
    "    for i, cur_graph in enumerate(cur_cluster_data):\n",
    "        for j, intval in enumerate((cur_graph.x[:,:4] == 1).nonzero(as_tuple=True)[1]):\n",
    "            plotting_label_array[i, j] = intval.item()\n",
    "    \n",
    "    plotting_label_distr = np.zeros((50, 4))\n",
    "    for i in range(4):\n",
    "        plotting_label_distr[:,i] = np.sum(plotting_label_array==i, axis=0)/len(cur_cluster_data)\n",
    "\n",
    "    plotting_label_values = [code[np.argmax(plotting_label_point)] for plotting_label_point in plotting_label_distr]\n",
    "    plotting_label_pcts = np.round(np.max(plotting_label_distr, axis=1), 2)\n",
    "\n",
    "    plotting_labels = {node: f\"{plotting_label_values[i]}\\n{plotting_label_pcts[i]}\" for i,node in enumerate(g.nodes())}\n",
    "    node_color_dict = {\"A\": \"red\", \"C\": \"green\", \"G\": \"dodgerblue\", \"T\": \"orange\"}\n",
    "    node_colors = []\n",
    "    for label in plotting_label_values:\n",
    "        node_colors.append(node_color_dict[label])\n",
    "\n",
    "    plt.figure(figsize=(12,10))\n",
    "    nx.draw_networkx_labels(g, plotting_pos, labels=plotting_labels, font_size=8)\n",
    "    nx.draw_networkx_nodes(g,plotting_pos, node_color=node_colors)\n",
    "    \n",
    "    edges = nx.draw_networkx_edges(g,plotting_pos,edge_color=weights,width=widths,\n",
    "                                   edge_cmap=cmap, alpha=alphas)\n",
    "    cbar = plt.colorbar(edges)\n",
    "    cbar.set_label(\"edge importance\")\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Cluster number {cluster_id}, most common nucleotide ({kmeans_input[cluster_ids_x == cluster_id].shape[0]} sequences)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Network plot with 2 most common nucleotides by position:\n",
    "    plotting_label_ids = [[arg_index for arg_index in np.argsort(-plotting_label_point)[:2]] for plotting_label_point in plotting_label_distr]\n",
    "    plotting_label_values = [[code[plotting_label_id] for plotting_label_id in plotting_label_id_list] for plotting_label_id_list in plotting_label_ids]\n",
    "    plotting_label_pcts = np.round([sum([plotting_label_point[plotting_label_id] for plotting_label_id in plotting_label_ids[i]]) for i, plotting_label_point in enumerate(plotting_label_distr)], 2)\n",
    "\n",
    "    plotting_labels = {node: f\"{plotting_label_values[i][0]}, {plotting_label_values[i][1]}\\n{plotting_label_pcts[i]}\" for i,node in enumerate(g.nodes())}\n",
    "\n",
    "    plt.figure(figsize=(12,10))\n",
    "    nx.draw_networkx_labels(g, plotting_pos, labels=plotting_labels, font_size=8)\n",
    "    nx.draw_networkx_nodes(g,plotting_pos, node_color=node_colors)\n",
    "    \n",
    "    edges = nx.draw_networkx_edges(g,plotting_pos,edge_color=weights,width=widths,\n",
    "                                   edge_cmap=cmap, alpha=alphas)\n",
    "    cbar = plt.colorbar(edges)\n",
    "    cbar.set_label(\"edge importance\")\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Cluster number {cluster_id}, 2 most common nucleotides ({kmeans_input[cluster_ids_x == cluster_id].shape[0]} sequences)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Network plot with highest deviation from mean nucleotide by position:\n",
    "    plotting_label_distr2 = np.zeros((50, 4))\n",
    "    for i in range(4):\n",
    "        plotting_label_distr2[:,i] = (np.sum(plotting_label_array==i, axis=0)/len(cur_cluster_data) - avg_list[i])/std_list[i]\n",
    "\n",
    "    plotting_label_values = [code[np.argmax(plotting_label_point)] for plotting_label_point in plotting_label_distr2]\n",
    "    plotting_label_pcts = np.round(np.max(plotting_label_distr2, axis=1), 2)\n",
    "\n",
    "    plotting_labels = {node: f\"{plotting_label_values[i]}\\n{plotting_label_pcts[i]}\" for i,node in enumerate(g.nodes())}\n",
    "    node_color_dict = {\"A\": \"red\", \"C\": \"green\", \"G\": \"dodgerblue\", \"T\": \"orange\"}\n",
    "    node_colors = []\n",
    "    for label in plotting_label_values:\n",
    "        node_colors.append(node_color_dict[label])\n",
    "\n",
    "    plt.figure(figsize=(12,10))\n",
    "    nx.draw_networkx_labels(g, plotting_pos, labels=plotting_labels, font_size=8)\n",
    "    nx.draw_networkx_nodes(g,plotting_pos, node_color=node_colors)\n",
    "    \n",
    "    edges = nx.draw_networkx_edges(g,plotting_pos,edge_color=weights,width=widths,\n",
    "                                   edge_cmap=cmap, alpha=alphas)\n",
    "    cbar = plt.colorbar(edges)\n",
    "    cbar.set_label(\"edge importance\")\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Cluster number {cluster_id}, highest deviation from mean nucleotide ({kmeans_input[cluster_ids_x == cluster_id].shape[0]} sequences)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Mean True C0 for cluster number {cluster_id}: {torch.mean(cur_cluster_data.y)}\")\n",
    "\n",
    "    cur_cluster_loader = DataLoader(cur_cluster_data, batch_size = 1)\n",
    "    cur_cluster_pred = []\n",
    "    for cur_cluster_batch in cur_cluster_loader:\n",
    "        cur_cluster_pred.append(model(cur_cluster_batch).item())\n",
    "\n",
    "    print(f\"Mean Predicted C0 for cluster number {cluster_id}: {np.mean(cur_cluster_pred)}\")\n",
    "\n",
    "    # plt.hist(cur_cluster_data.y.flatten())\n",
    "    # plt.title(f\"Cluster number {cluster_id} True C0\")\n",
    "    # plt.xlim(x)\n",
    "    # plt.show()\n",
    "\n",
    "    # plt.hist(cur_cluster_pred)\n",
    "    # plt.title(f\"Cluster number {cluster_id} Predicted C0\")\n",
    "    # plt.xlim(x)\n",
    "    # plt.show()\n",
    "\n",
    "    plt.xlim(x)\n",
    "    plt.ylim(y)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Observed\")\n",
    "    plt.plot(x, y, color=\"black\")\n",
    "    plt.scatter(cur_cluster_pred, cur_cluster_data.y.flatten(), marker=\"o\", alpha=0.1)\n",
    "    plt.title(f\"Cluster number {cluster_id} Observed vs. Predicted C0\")\n",
    "    plt.show()\n",
    "\n",
    "    true_y_by_cluster.append(cur_cluster_data.y.flatten())\n",
    "    pred_y_by_cluster.append(cur_cluster_pred)\n",
    "\n",
    "        # node_color_dict = {\"A\": \"red\", \"C\": \"green\", \"G\": \"blue\", \"T\": \"orange\"}\n",
    "\n",
    "    plt.plot(plotting_label_distr[:,0], \"red\", label=\"A\")\n",
    "    plt.axhline(avg_list[0], color=\"red\", alpha=0.5)\n",
    "    plt.plot(plotting_label_distr[:,1], \"green\", label=\"C\")\n",
    "    plt.axhline(avg_list[1], color=\"green\", alpha=0.5)\n",
    "    plt.plot(plotting_label_distr[:,2], \"dodgerblue\", label=\"G\")\n",
    "    plt.axhline(avg_list[2], color=\"dodgerblue\", alpha=0.5)\n",
    "    plt.plot(plotting_label_distr[:,3], \"orange\", label=\"T\")\n",
    "    plt.axhline(avg_list[3], color=\"orange\", alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.title(f\"Cluster number {cluster_id} Nucleotide Content\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(plotting_label_distr[:,0] + plotting_label_distr[:,3], \"coral\", label=\"A or T\")\n",
    "    plt.axhline(avg_list[0]+avg_list[3], color=\"coral\", alpha=0.5)\n",
    "    plt.plot(plotting_label_distr[:,1] + plotting_label_distr[:,2], \"mediumaquamarine\", label=\"C or G\")\n",
    "    plt.axhline(avg_list[1]+avg_list[2], color=\"mediumaquamarine\", alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.title(f\"Cluster number {cluster_id} Nucleotide Content, Grouped\")\n",
    "    plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.xlim(x)\n",
    "plt.ylim(y)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Observed\")\n",
    "plt.plot(x, y, color=\"black\")\n",
    "for i in range(len(true_y_by_cluster)):\n",
    "    plt.scatter(pred_y_by_cluster[i], true_y_by_cluster[i], marker=\"o\", alpha=0.1, color=colors[i], label=f\"Cluster {i}\")\n",
    "plt.legend()\n",
    "plt.title(f\"Observed vs. Predicted C0 By Cluster Assignment\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Individual Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [-3.5, 3.5]\n",
    "y = [-3.5, 3.5]\n",
    "code = {0:\"A\", 1:\"C\", 2:\"G\", 3:\"T\"}\n",
    "\n",
    "cluster_id = 2\n",
    "bases_of_interest = [1]\n",
    "position_of_interest = 22\n",
    "\n",
    "if len(bases_of_interest) == 1:\n",
    "    bases_of_interest_str = code[bases_of_interest[0]]\n",
    "else:\n",
    "    bases_of_interest_str = \"\"\n",
    "    for i, base_of_interest in enumerate(bases_of_interest):\n",
    "        if i==0:\n",
    "            bases_of_interest_str = code[base_of_interest]\n",
    "        else: \n",
    "            bases_of_interest_str = bases_of_interest_str + \", \" + code[base_of_interest]\n",
    "\n",
    "g = nx.Graph()\n",
    "\n",
    "g.add_weighted_edges_from(kmeans_processed_weighted_edges_list[cluster_id])\n",
    "plotting_pos = nx.circular_layout(g)\n",
    "\n",
    "edges,weights = zip(*nx.get_edge_attributes(g,'weight').items())\n",
    "# weights = np.log(weights/(1-weights))\n",
    "vmin = min(weights)\n",
    "vmax = max(weights)\n",
    "cmap=plt.cm.RdYlBu_r\n",
    "plotting_labels = {node: str(node) for node in g.nodes()}\n",
    "\n",
    "widths = [weight*20 for weight in weights]\n",
    "alphas = [weight for weight in weights]\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "nx.draw_networkx_labels(g, plotting_pos, labels=plotting_labels)\n",
    "nx.draw_networkx_nodes(g,plotting_pos,label=plotting_labels)\n",
    "\n",
    "edges = nx.draw_networkx_edges(g,plotting_pos,edge_color=weights,width=widths,\n",
    "                                edge_cmap=cmap, alpha=alphas)\n",
    "cbar = plt.colorbar(edges)\n",
    "cbar.set_label(\"edge importance\")\n",
    "plt.axis('off')\n",
    "plt.title(f\"Cluster number {cluster_id} ({kmeans_input[cluster_ids_x == cluster_id].shape[0]} sequences)\")\n",
    "plt.show()\n",
    "\n",
    "cur_cluster_data = tiling_data[which_graph[0:len(test_explanations)][cluster_ids_x == cluster_id]]\n",
    "\n",
    "plotting_label_array = np.zeros((len(cur_cluster_data), 50))\n",
    "for i, cur_graph in enumerate(cur_cluster_data):\n",
    "    for j, intval in enumerate((cur_graph.x[:,:4] == 1).nonzero(as_tuple=True)[1]):\n",
    "        plotting_label_array[i, j] = intval.item()\n",
    "\n",
    "plotting_label_distr = np.zeros((50, 4))\n",
    "for i in range(4):\n",
    "    plotting_label_distr[:,i] = np.sum(plotting_label_array==i, axis=0)/len(cur_cluster_data)\n",
    "\n",
    "plotting_label_values = [code[np.argmax(plotting_label_point)] for plotting_label_point in plotting_label_distr]\n",
    "plotting_label_pcts = np.round(np.max(plotting_label_distr, axis=1), 2)\n",
    "\n",
    "plotting_labels = {node: f\"{plotting_label_values[i]}\\n{plotting_label_pcts[i]}\" for i,node in enumerate(g.nodes())}\n",
    "node_color_dict = {\"A\": \"red\", \"C\": \"green\", \"G\": \"blue\", \"T\": \"orange\"}\n",
    "node_colors = []\n",
    "for label in plotting_label_values:\n",
    "    node_colors.append(node_color_dict[label])\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "nx.draw_networkx_labels(g, plotting_pos, labels=plotting_labels, font_size=8)\n",
    "nx.draw_networkx_nodes(g,plotting_pos, node_color=node_colors)\n",
    "\n",
    "edges = nx.draw_networkx_edges(g,plotting_pos,edge_color=weights,width=widths,\n",
    "                                edge_cmap=cmap, alpha=alphas)\n",
    "cbar = plt.colorbar(edges)\n",
    "cbar.set_label(\"edge importance\")\n",
    "plt.axis('off')\n",
    "plt.title(f\"Cluster number {cluster_id}, most common nucleotide ({kmeans_input[cluster_ids_x == cluster_id].shape[0]} sequences)\")\n",
    "plt.show()\n",
    "\n",
    "plotting_label_ids = [[arg_index for arg_index in np.argsort(-plotting_label_point)[:2]] for plotting_label_point in plotting_label_distr]\n",
    "plotting_label_values = [[code[plotting_label_id] for plotting_label_id in plotting_label_id_list] for plotting_label_id_list in plotting_label_ids]\n",
    "plotting_label_pcts = np.round([sum([plotting_label_point[plotting_label_id] for plotting_label_id in plotting_label_ids[i]]) for i, plotting_label_point in enumerate(plotting_label_distr)], 2)\n",
    "\n",
    "plotting_labels = {node: f\"{plotting_label_values[i][0]}, {plotting_label_values[i][1]}\\n{plotting_label_pcts[i]}\" for i,node in enumerate(g.nodes())}\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "nx.draw_networkx_labels(g, plotting_pos, labels=plotting_labels, font_size=8)\n",
    "nx.draw_networkx_nodes(g,plotting_pos, node_color=node_colors)\n",
    "\n",
    "edges = nx.draw_networkx_edges(g,plotting_pos,edge_color=weights,width=widths,\n",
    "                                edge_cmap=cmap, alpha=alphas)\n",
    "cbar = plt.colorbar(edges)\n",
    "cbar.set_label(\"edge importance\")\n",
    "plt.axis('off')\n",
    "plt.title(f\"Cluster number {cluster_id}, 2 most common nucleotides ({kmeans_input[cluster_ids_x == cluster_id].shape[0]} sequences)\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean True C0 for cluster number {cluster_id}: {torch.mean(cur_cluster_data.y)}\")\n",
    "\n",
    "plt.plot(plotting_label_distr[:,0], \"red\", label=\"A\")\n",
    "plt.plot(plotting_label_distr[:,1], \"green\", label=\"C\")\n",
    "plt.plot(plotting_label_distr[:,2], \"blue\", label=\"G\")\n",
    "plt.plot(plotting_label_distr[:,3], \"orange\", label=\"T\")\n",
    "plt.legend()\n",
    "plt.title(f\"Cluster number {cluster_id} Nucleotide Content\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "of_interest_0_graphs = []\n",
    "of_interest_0_true_y = []\n",
    "\n",
    "of_interest_1_graphs = []\n",
    "of_interest_1_true_y = []\n",
    "for graph in cur_cluster_data:\n",
    "    of_interest = False\n",
    "    for base_of_interest in bases_of_interest:\n",
    "        if graph.x[position_of_interest, base_of_interest] == 1:\n",
    "            of_interest = True\n",
    "    if of_interest:\n",
    "        of_interest_1_graphs.append(graph)\n",
    "        of_interest_1_true_y.append(graph.y.item())\n",
    "    else:\n",
    "        of_interest_0_graphs.append(graph)\n",
    "        of_interest_0_true_y.append(graph.y.item())\n",
    "\n",
    "print(f\"Mean True C0 for cluster number {cluster_id} when position {position_of_interest} is {bases_of_interest_str}: {np.mean(of_interest_1_true_y)}\")\n",
    "\n",
    "cur_cluster_loader = DataLoader(of_interest_1_graphs, batch_size = 1)\n",
    "cur_cluster_pred = []\n",
    "for cur_cluster_batch in cur_cluster_loader:\n",
    "    cur_cluster_pred.append(model(cur_cluster_batch).item())\n",
    "\n",
    "print(f\"Mean Predicted C0 for cluster number {cluster_id} when position {position_of_interest} is {bases_of_interest_str}: {np.mean(cur_cluster_pred)}\")\n",
    "\n",
    "plt.xlim(x)\n",
    "plt.ylim(y)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Observed\")\n",
    "plt.plot(x, y, color=\"black\")\n",
    "plt.scatter(cur_cluster_pred, of_interest_1_true_y, marker=\"o\", alpha=0.1)\n",
    "plt.title(f\"Cluster number {cluster_id} Observed vs. Predicted C0 when position {position_of_interest} is {bases_of_interest_str}\")\n",
    "plt.show()\n",
    "\n",
    "of_interest_1_plotting_label_array = np.zeros((len(of_interest_1_graphs), 50))\n",
    "for i, cur_graph in enumerate(of_interest_1_graphs):\n",
    "    for j, intval in enumerate((cur_graph.x[:,:4] == 1).nonzero(as_tuple=True)[1]):\n",
    "        of_interest_1_plotting_label_array[i, j] = intval.item()\n",
    "\n",
    "of_interest_1_plotting_label_distr = np.zeros((50, 4))\n",
    "for i in range(4):\n",
    "    of_interest_1_plotting_label_distr[:,i] = np.sum(of_interest_1_plotting_label_array==i, axis=0)/len(of_interest_1_graphs)\n",
    "\n",
    "plt.plot(of_interest_1_plotting_label_distr[:,0], \"red\", label=\"A\")\n",
    "plt.plot(of_interest_1_plotting_label_distr[:,1], \"green\", label=\"C\")\n",
    "plt.plot(of_interest_1_plotting_label_distr[:,2], \"blue\", label=\"G\")\n",
    "plt.plot(of_interest_1_plotting_label_distr[:,3], \"orange\", label=\"T\")\n",
    "plt.legend()\n",
    "plt.title(f\"Cluster number {cluster_id} Nucleotide Content when position {position_of_interest} is {bases_of_interest_str}\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean True C0 for cluster number {cluster_id} when position {position_of_interest} is not {bases_of_interest_str}: {np.mean(of_interest_0_true_y)}\")\n",
    "\n",
    "cur_cluster_loader = DataLoader(of_interest_0_graphs, batch_size = 1)\n",
    "cur_cluster_pred = []\n",
    "for cur_cluster_batch in cur_cluster_loader:\n",
    "    cur_cluster_pred.append(model(cur_cluster_batch).item())\n",
    "\n",
    "print(f\"Mean Predicted C0 for cluster number {cluster_id} when position {position_of_interest} is not {bases_of_interest_str}: {np.mean(cur_cluster_pred)}\")\n",
    "\n",
    "plt.xlim(x)\n",
    "plt.ylim(y)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Observed\")\n",
    "plt.plot(x, y, color=\"black\")\n",
    "plt.scatter(cur_cluster_pred, of_interest_0_true_y, marker=\"o\", alpha=0.1)\n",
    "plt.title(f\"Cluster number {cluster_id} Observed vs. Predicted C0 when position {position_of_interest} is not {bases_of_interest_str}\")\n",
    "plt.show()\n",
    "\n",
    "of_interest_0_plotting_label_array = np.zeros((len(of_interest_0_graphs), 50))\n",
    "for i, cur_graph in enumerate(of_interest_0_graphs):\n",
    "    for j, intval in enumerate((cur_graph.x[:,:4] == 1).nonzero(as_tuple=True)[1]):\n",
    "        of_interest_0_plotting_label_array[i, j] = intval.item()\n",
    "\n",
    "of_interest_0_plotting_label_distr = np.zeros((50, 4))\n",
    "for i in range(4):\n",
    "    of_interest_0_plotting_label_distr[:,i] = np.sum(of_interest_0_plotting_label_array==i, axis=0)/len(of_interest_0_graphs)\n",
    "\n",
    "plt.plot(of_interest_0_plotting_label_distr[:,0], \"red\", label=\"A\")\n",
    "plt.plot(of_interest_0_plotting_label_distr[:,1], \"green\", label=\"C\")\n",
    "plt.plot(of_interest_0_plotting_label_distr[:,2], \"blue\", label=\"G\")\n",
    "plt.plot(of_interest_0_plotting_label_distr[:,3], \"orange\", label=\"T\")\n",
    "plt.legend()\n",
    "plt.title(f\"Cluster number {cluster_id} Nucleotide Content when position {position_of_interest} is not {bases_of_interest_str}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# of_interest_0_true_y = []\n",
    "# of_interest_1_true_y = []\n",
    "# for graph in cur_cluster_data:\n",
    "#     of_interest = False\n",
    "#     for base_of_interest in bases_of_interest:\n",
    "#         if graph.x[position_of_interest, base_of_interest] == 1:\n",
    "#             of_interest = True\n",
    "#     if of_interest:\n",
    "#         of_interest_0_true_y.append(graph.y.item())\n",
    "#     else:\n",
    "#         of_interest_1_true_y.append(graph.y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(of_interest_0_true_y + of_interest_1_true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_cluster_id = 16 # Actual Outlier Cluster\n",
    "outlier_cluster_id = 3 # Highest Average C0 Cluster\n",
    "outlier_cluster_data = data[which_graph[0:len(test_explanations)][cluster_ids_x == outlier_cluster_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_cluster_processed_edge_weight_directed = kmeans_input[cluster_ids_x == outlier_cluster_id]\n",
    "\n",
    "for j in range(len(outlier_cluster_data)):\n",
    "\n",
    "    processed_edge_weight_directed_j = outlier_cluster_processed_edge_weight_directed[j]\n",
    "\n",
    "\n",
    "    processed_weighted_edges_directed_j = [(test_explanations[0].edge_index[0, i].item(), \n",
    "                                            test_explanations[0].edge_index[1, i].item(), \n",
    "                                            processed_edge_weight_directed_j[i].item()\n",
    "                                            ) for i in range(len(processed_edge_weight_directed_j))]\n",
    "    \n",
    "    processed_edge_weight_j = []\n",
    "    undirected_edges_j = []\n",
    "    prev_start = 0\n",
    "\n",
    "    i=0\n",
    "    for edge_length_list in hyperparameters[\"edge_lengths\"]:\n",
    "        for edge_length in edge_length_list:\n",
    "            processed_edge_weight_j.append(torch.mean(torch.stack((\n",
    "                processed_edge_weight_directed_j[prev_start:prev_start+(50-edge_length)],\n",
    "                processed_edge_weight_directed_j[prev_start+(50-edge_length):prev_start+(50-edge_length)*2])),\n",
    "                dim=0))\n",
    "            undirected_edges_j.append(test_explanations[0].edge_index[:,prev_start:prev_start+(50-edge_length)])\n",
    "            prev_start += (50-edge_length)*2\n",
    "            i+=1\n",
    "    processed_edge_weight_j = torch.cat(processed_edge_weight_j)\n",
    "    undirected_edges_j = torch.cat(undirected_edges_j, dim=1)\n",
    "\n",
    "    processed_weighted_edges_j = [(undirected_edges_j[0, i].item(), \n",
    "                                undirected_edges_j[1, i].item(), \n",
    "                                processed_edge_weight_j[i].item()\n",
    "                                ) for i in range(len(processed_edge_weight_j))]\n",
    "    \n",
    "    # plt.figure(figsize=(12,10))\n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "    g = nx.Graph()\n",
    "\n",
    "    g.add_weighted_edges_from(processed_weighted_edges_j)\n",
    "    plotting_pos = nx.circular_layout(g)\n",
    "\n",
    "    edges,weights = zip(*nx.get_edge_attributes(g,'weight').items())\n",
    "    vmin = min(weights)\n",
    "    vmax = max(weights)\n",
    "    cmap=plt.cm.RdYlBu_r\n",
    "\n",
    "    widths = [weight*5 for weight in weights]\n",
    "    alphas = [weight for weight in weights]\n",
    "    code = {0:\"A\", 1:\"C\", 2:\"G\", 3:\"T\"}\n",
    "    plotting_label_values = []\n",
    "    for i, intval in enumerate((outlier_cluster_data[j].x[:,:4] == 1).nonzero(as_tuple=True)[1]):\n",
    "        plotting_label_values.append(code[intval.item()])\n",
    "    plotting_labels = {node: plotting_label_values[i] for i,node in enumerate(g.nodes())}\n",
    "\n",
    "    node_color_dict = {\"A\": \"red\", \"C\": \"green\", \"G\": \"dodgerblue\", \"T\": \"orange\"}\n",
    "    node_colors = []\n",
    "    for label in plotting_label_values:\n",
    "        node_colors.append(node_color_dict[label])\n",
    "\n",
    "    nx.draw_networkx_labels(g, plotting_pos, labels=plotting_labels)\n",
    "    nx.draw_networkx_nodes(g,plotting_pos,label=plotting_labels, node_color=node_colors)\n",
    "    # edges = nx.draw_networkx_edges(g,plotting_pos,edge_color=weights,width=widths,\n",
    "    #                             edge_cmap=cmap, alpha=alphas)\n",
    "    # cbar = plt.colorbar(edges)\n",
    "    # cbar.set_label(\"edge importance\")\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Cluster {outlier_cluster_id}, Sequence number {j}\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Cluster {outlier_cluster_id}, Sequence number {j} true C0: {round(true_y_by_cluster[outlier_cluster_id][j].item(),2)}\")\n",
    "    print(f\"Cluster {outlier_cluster_id}, Sequence number {j} predicted C0: {round(pred_y_by_cluster[outlier_cluster_id][j], 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data = pd.DataFrame({\n",
    "    'cluster_assignment': cluster_ids_x,\n",
    "    'C0': data.y[which_graph[range(len(test_explanations))]].flatten()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(cluster_data.C0, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ANOVA\n",
    "anova_result = f_oneway(\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 0]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 1]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 2]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 3]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 4]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 5]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 6]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 7]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 8]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 9]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 10]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 11]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 12]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 13]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 14]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 15]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 16]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 17]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 18]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 19]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 20]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 21]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 22]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 23]['C0'],\n",
    "    cluster_data[cluster_data['cluster_assignment'] == 24]['C0'],\n",
    ")\n",
    "\n",
    "# Display ANOVA results\n",
    "print(\"ANOVA p-value:\", anova_result.pvalue)\n",
    "\n",
    "# Check the significance\n",
    "alpha = 0.05\n",
    "if anova_result.pvalue < alpha:\n",
    "    print(\"Reject the null hypothesis. There is a significant difference between clusters.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant difference between clusters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_between = len(cluster_data['cluster_assignment'].unique()) - 1\n",
    "df_total = len(cluster_data) - 1\n",
    "df_resid = df_total - df_between\n",
    "\n",
    "eta_squared = anova_result.statistic / (anova_result.statistic + df_resid)\n",
    "print(\"Eta-squared:\", eta_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values = cluster_data.groupby('cluster_assignment')['C0'].mean().sort_values(ascending=False).index\n",
    "cluster_data['cluster_assignment'] = cluster_data['cluster_assignment'].astype(pd.CategoricalDtype(categories=mean_values, ordered=True))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='cluster_assignment', y='C0', data=cluster_data)\n",
    "plt.title('Side-by-Side Boxplots of C0 by Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Output Variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-Space Sub-Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_cluster_ids_x_list = []\n",
    "for cluster_id in range(num_clusters):\n",
    "    cur_cluster_kmeans_input = data[which_graph[0:len(test_explanations)][cluster_ids_x == cluster_id]].x[:,:4].reshape(sum(cluster_ids_x == cluster_id), -1)\n",
    "    # cur_cluster_ids_x, cur_cluster_centers = kmeans(X=cur_cluster_kmeans_input, num_clusters=2, distance='euclidean', tol=0.000000001)\n",
    "    cur_cluster_ids_x, cur_cluster_centers = kmeans(X=cur_cluster_kmeans_input, num_clusters=2, distance='euclidean')\n",
    "    sub_cluster_ids_x_list.append(cur_cluster_ids_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_id in range(num_clusters):\n",
    "    cur_cluster_data = data[which_graph[0:len(test_explanations)][cluster_ids_x == cluster_id]]\n",
    "    cur_cluster_sub_cluster0_data = cur_cluster_data[sub_cluster_ids_x_list[cluster_id] == 0]\n",
    "    cur_cluster_sub_cluster1_data = cur_cluster_data[sub_cluster_ids_x_list[cluster_id] == 1]\n",
    "\n",
    "    print(f\"Cluster number {cluster_id} overall mean C0: {round(torch.mean(cur_cluster_data.y).item(), 2)}\")\n",
    "    print(f\"\\t Sub-cluster 0 mean C0: {round(torch.mean(cur_cluster_sub_cluster0_data.y).item(), 2)}\")\n",
    "    print(f\"\\t Sub-cluster 1 mean C0: {round(torch.mean(cur_cluster_sub_cluster1_data.y).item(), 2)}\")\n",
    "    print(f\"\\t Absolute difference: {round(abs(torch.mean(cur_cluster_sub_cluster0_data.y) - torch.mean(cur_cluster_sub_cluster1_data.y)).item(), 2)}\")\n",
    "\n",
    "    plt.xlim(x)\n",
    "    plt.ylim(y)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Observed\")\n",
    "    plt.plot(x, y, color=\"black\")\n",
    "    plt.scatter(torch.Tensor(pred_y_by_cluster[cluster_id])[sub_cluster_ids_x_list[cluster_id] == 0], \n",
    "                true_y_by_cluster[cluster_id][sub_cluster_ids_x_list[cluster_id] == 0], \n",
    "                marker=\"o\", alpha=0.1, label=\"Subcluster 0\", color=\"r\")\n",
    "    plt.scatter(torch.Tensor(pred_y_by_cluster[cluster_id])[sub_cluster_ids_x_list[cluster_id] == 1], \n",
    "                true_y_by_cluster[cluster_id][sub_cluster_ids_x_list[cluster_id] == 1], \n",
    "                marker=\"o\", alpha=0.1, label=\"Subcluster 1\", color=\"b\")\n",
    "    plt.title(f\"Cluster number {cluster_id} Observed vs. Predicted C0 By Subcluster\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # plt.xlim(x)\n",
    "    # plt.ylim(y)\n",
    "    # plt.xlabel(\"Predicted\")\n",
    "    # plt.ylabel(\"Observed\")\n",
    "    # plt.plot(x, y, color=\"black\")\n",
    "    # plt.title(f\"Cluster number {cluster_id} Subcluster 1 Observed vs. Predicted C0\")\n",
    "    # plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start at smallest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_edge_weight = []\n",
    "undirected_edges = []\n",
    "prev_start = 0\n",
    "# for i, edge_length in enumerate(hyperparameters[\"edge_lengths\"]):\n",
    "#     processed_edge_weight.append(torch.mean(torch.stack((\n",
    "#         processed_edge_weight_directed[prev_start:prev_start+(50-edge_length)],\n",
    "#         processed_edge_weight_directed[prev_start+(50-edge_length):prev_start+(50-edge_length)*2])),\n",
    "#         dim=0))\n",
    "#     undirected_edges.append(test_explanations[0].edge_index[:,prev_start:prev_start+(50-edge_length)])\n",
    "#     prev_start += (50-edge_length)*2\n",
    "i = 0\n",
    "for edge_length_list in hyperparameters[\"edge_lengths\"]:\n",
    "    for edge_length in edge_length_list:\n",
    "        processed_edge_weight.append(torch.mean(torch.stack((\n",
    "            processed_edge_weight_directed[prev_start:prev_start+(50-edge_length)],\n",
    "            processed_edge_weight_directed[prev_start+(50-edge_length):prev_start+(50-edge_length)*2])),\n",
    "            dim=0))\n",
    "        undirected_edges.append(test_explanations[0].edge_index[:,prev_start:prev_start+(50-edge_length)])\n",
    "        prev_start += (50-edge_length)*2\n",
    "        i+=1\n",
    "processed_edge_weight = torch.cat(processed_edge_weight)\n",
    "undirected_edges = torch.cat(undirected_edges, dim=1)\n",
    "\n",
    "processed_weighted_edges = [(undirected_edges[0, i].item(), \n",
    "                             undirected_edges[1, i].item(), \n",
    "                             processed_edge_weight[i].item()\n",
    "                             ) for i in range(len(processed_edge_weight))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ids_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which_graph = np.random.random_integers(0, len(data), num_obs)\n",
    "_, which_graph_least = torch.topk(data.y.flatten(), k=num_obs, largest=False)\n",
    "which_graph_least = np.array(which_graph_least)\n",
    "explanation_data_least = DataLoader(data[which_graph_least], batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predicted_values_least = []\n",
    "true_values_least = []\n",
    "for explanation_batch in explanation_data_least:\n",
    "    predicted_values_least.append(model(explanation_batch))\n",
    "    true_values_least.append(explanation_batch.y)\n",
    "# np.corrcoef((predicted_values[0].reshape(-1).detach().numpy(), true_values[0].reshape(-1).numpy()))[0,1]\n",
    "\n",
    "for i in range(len(predicted_values_least)):\n",
    "    for j in range(len(predicted_values_least[i])):\n",
    "        # print(f\"i: {i}, j: {j}, i+j: {i+j}\")\n",
    "        print(f\"Index {i*len(predicted_values_least[i]) + j}:\")\n",
    "        print(f\"\\t Predicted: {predicted_values_least[i][j].item()}, Actual: {true_values_least[i][j].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelConfig(mode=\"regression\", task_level=\"graph\", return_type=\"raw\")\n",
    "explainer_config = ExplainerConfig(explanation_type=\"phenomenon\", node_mask_type=\"attributes\")\n",
    "separate_network_explainer_least = CustomExplainer2(lr = .1)\n",
    "separate_network_explainer_least.connect(explainer_config=explainer_config, model_config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_network_explainer_least.epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_network_explanations_least=[]\n",
    "for explanation_batch in explanation_data_least:\n",
    "    until_mlps_out = forward_until_mlps(model, explanation_batch)\n",
    "\n",
    "    explanation_batch.x = until_mlps_out\n",
    "    model_2.eval()\n",
    "    separate_network_explanations_least.append(separate_network_explainer_least(model=model_2, data=explanation_batch, \n",
    "                                                                    edge_index=torch.Tensor().reshape(2,-1), \n",
    "                                                                    target=explanation_batch.y,\n",
    "                                                                    batch=explanation_batch.batch, \n",
    "                                                                    cluster_list=model.cluster_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_channels = hyperparameters[\"hidden_channels\"]\n",
    "all_network_contributions_least = []\n",
    "for network_explanations in separate_network_explanations_least:\n",
    "    # temp_explain = separate_network_explanations[0]\n",
    "    total_contribution = torch.sum(network_explanations.node_mask)\n",
    "    cur_network_contributions = []\n",
    "    for i in range(network_explanations.node_mask.shape[1]//hyperparameters[\"hidden_channels\"]):\n",
    "        cur_network_contribution = torch.sum(network_explanations.node_mask[:,i*hidden_channels:(i+1)*hidden_channels])\n",
    "        cur_network_contributions.append(cur_network_contribution)\n",
    "        # print(f\"Contribution from network corresponding to edge length {hyperparameters['edge_lengths'][i]}: {cur_network_contribution/total_contribution}\")\n",
    "    all_network_contributions_least.append(cur_network_contributions)\n",
    "avg_network_contributions_least = []\n",
    "for i in range(len(all_network_contributions_least[0])):\n",
    "    avg_network_contributions_least.append(np.mean([all_network_contributions_least[j][i]/sum(all_network_contributions_least[j]) for j in range(len(all_network_contributions_least))]))\n",
    "    print(f\"Average contribution from network corresponding to edge length {hyperparameters['edge_lengths'][i]}: {avg_network_contributions_least[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelConfig(mode=\"regression\", task_level=\"graph\", return_type=\"raw\")\n",
    "explainer_config = ExplainerConfig(explanation_type=\"model\", edge_mask_type=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_explainer_least = CustomExplainer(lr = 0.01)\n",
    "test_explainer_least.connect(explainer_config=explainer_config, model_config=model_config)\n",
    "test_explainer_least.epochs=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_explanations_least=[]\n",
    "for explanation_batch in explanation_data_least:\n",
    "    test_explanations_least.append(test_explainer_least(model=model, data=explanation_batch, target=explanation_batch.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First find the average contribution for each batch:\n",
    "batch_wise_processed_edge_weight_directed_least = []\n",
    "for test_explanation in test_explanations_least:\n",
    "    # Average of all weights in test_explanation\n",
    "    batch_wise_processed_edge_weight_directed_least.append(torch.mean(\n",
    "        torch.stack([test_explanation.edge_mask[i*338:(i+1)*338] for i in range(test_explanation.edge_mask.shape[0]//338)]),\n",
    "        dim=0))\n",
    "\n",
    "batch_wise_processed_edge_weight_directed_least = torch.cat(batch_wise_processed_edge_weight_directed_least)\n",
    "\n",
    "processed_edge_weight_directed_least = torch.mean(\n",
    "    torch.stack([batch_wise_processed_edge_weight_directed_least[i*338:(i+1)*338] for i in range(batch_wise_processed_edge_weight_directed_least.shape[0]//338)]),\n",
    "    dim=0)\n",
    "\n",
    "processed_weighted_edges_directed_least = [(test_explanations_least[0].edge_index[0, i].item(), \n",
    "                                            test_explanations_least[0].edge_index[1, i].item(), \n",
    "                                            processed_edge_weight_directed_least[i].item()\n",
    "                                            ) for i in range(len(processed_edge_weight_directed))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_edge_weight_least = []\n",
    "undirected_edges_least = []\n",
    "prev_start = 0\n",
    "for i, edge_length in enumerate(hyperparameters[\"edge_lengths\"]):\n",
    "    processed_edge_weight_least.append(torch.mean(torch.stack((\n",
    "        processed_edge_weight_directed_least[prev_start:prev_start+(50-edge_length)],\n",
    "        processed_edge_weight_directed_least[prev_start+(50-edge_length):prev_start+(50-edge_length)*2])),\n",
    "        dim=0))\n",
    "    undirected_edges_least.append(test_explanations_least[0].edge_index[:,prev_start:prev_start+(50-edge_length)])\n",
    "    prev_start += (50-edge_length)*2\n",
    "processed_edge_weight_least = torch.cat(processed_edge_weight_least)\n",
    "undirected_edges_least = torch.cat(undirected_edges_least, dim=1)\n",
    "\n",
    "processed_weighted_edges_least = [(undirected_edges_least[0, i].item(), \n",
    "                                   undirected_edges_least[1, i].item(), \n",
    "                                   processed_edge_weight_least[i].item()\n",
    "                                   ) for i in range(len(processed_edge_weight_least))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,12))\n",
    "\n",
    "g = nx.Graph()\n",
    "\n",
    "g.add_weighted_edges_from(processed_weighted_edges_least)\n",
    "plotting_pos = nx.circular_layout(g)\n",
    "\n",
    "edges,weights = zip(*nx.get_edge_attributes(g,'weight').items())\n",
    "vmin = min(weights)\n",
    "vmax = max(weights)\n",
    "cmap=plt.cm.coolwarm\n",
    "plotting_labels = {node: str(node) for node in g.nodes()}\n",
    "\n",
    "widths = [weight*5 for weight in weights]\n",
    "\n",
    "nx.draw_networkx_labels(g, plotting_pos, labels=plotting_labels)\n",
    "nx.draw_networkx_nodes(g,plotting_pos,label=plotting_labels)\n",
    "edges = nx.draw_networkx_edges(g,plotting_pos,edge_color=weights,width=widths,\n",
    "                               edge_cmap=cmap)\n",
    "cbar = plt.colorbar(edges)\n",
    "cbar.set_label(\"edge importance\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_weighted_edges_least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=57\n",
    "\n",
    "# If all in one batch:\n",
    "# processed_edge_weight_directed_j = test_explanations[0].edge_mask[j*338:(j+1)*338]\n",
    "\n",
    "# If all in separate batches:\n",
    "processed_edge_weight_directed_j_least = test_explanations_least[j].edge_mask\n",
    "\n",
    "processed_weighted_edges_directed_j_least = [(test_explanations_least[0].edge_index[0, i].item(), \n",
    "                                              test_explanations_least[0].edge_index[1, i].item(), \n",
    "                                              processed_edge_weight_directed_j_least[i].item()\n",
    "                                              ) for i in range(len(processed_edge_weight_directed_j_least))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_edge_weight_j_least = []\n",
    "undirected_edges_j_least = []\n",
    "prev_start = 0\n",
    "for i, edge_length in enumerate(hyperparameters[\"edge_lengths\"]):\n",
    "    processed_edge_weight_j_least.append(torch.mean(torch.stack((\n",
    "        processed_edge_weight_directed_j_least[prev_start:prev_start+(50-edge_length)],\n",
    "        processed_edge_weight_directed_j_least[prev_start+(50-edge_length):prev_start+(50-edge_length)*2])),\n",
    "        dim=0))\n",
    "    undirected_edges_j_least.append(test_explanations_least[0].edge_index[:,prev_start:prev_start+(50-edge_length)])\n",
    "    prev_start += (50-edge_length)*2\n",
    "processed_edge_weight_j_least = torch.cat(processed_edge_weight_j_least)\n",
    "undirected_edges_j_least = torch.cat(undirected_edges_j_least, dim=1)\n",
    "\n",
    "processed_weighted_edges_j_least = [(undirected_edges_j_least[0, i].item(), \n",
    "                                     undirected_edges_j_least[1, i].item(), \n",
    "                                     processed_edge_weight_j_least[i].item()\n",
    "                                     ) for i in range(len(processed_edge_weight_j_least))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_weighted_edges_j_least = [edge if edge[2] > 0.7 else (edge[0], edge[1], 0) for edge in processed_weighted_edges_j_least]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,12))\n",
    "\n",
    "g = nx.Graph()\n",
    "\n",
    "g.add_weighted_edges_from(processed_weighted_edges_j_least)\n",
    "plotting_pos = nx.circular_layout(g)\n",
    "\n",
    "edges,weights = zip(*nx.get_edge_attributes(g,'weight').items())\n",
    "vmin = min(weights)\n",
    "vmax = max(weights)\n",
    "cmap=plt.cm.coolwarm\n",
    "\n",
    "widths = [weight*5 for weight in weights]\n",
    "code = {0:\"A\", 1:\"C\", 2:\"G\", 3:\"T\"}\n",
    "plotting_label_values = []\n",
    "for i, intval in enumerate((data[which_graph_least][j].x[:,:4] == 1).nonzero(as_tuple=True)[1]):\n",
    "    plotting_label_values.append(code[intval.item()])\n",
    "plotting_labels = {node: plotting_label_values[i] for i,node in enumerate(g.nodes())}\n",
    "\n",
    "node_color_dict = {\"A\": \"red\", \"C\": \"green\", \"G\": \"blue\", \"T\": \"orange\"}\n",
    "node_colors = []\n",
    "for label in plotting_label_values:\n",
    "    node_colors.append(node_color_dict[label])\n",
    "\n",
    "nx.draw_networkx_labels(g, plotting_pos, labels=plotting_labels)\n",
    "nx.draw_networkx_nodes(g,plotting_pos,label=plotting_labels, node_color=node_colors)\n",
    "edges = nx.draw_networkx_edges(g,plotting_pos,edge_color=weights,width=widths,\n",
    "                               edge_cmap=cmap)\n",
    "cbar = plt.colorbar(edges)\n",
    "cbar.set_label(\"edge importance\")\n",
    "plt.axis('off')\n",
    "plt.title(f\"Sequence number {j} (Least)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
